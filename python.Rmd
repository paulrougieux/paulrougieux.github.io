---
title: "Python Commands"
author: "Paul Rougieux"
date: "17 August 2017"
output: 
  html_document: 
    toc: yes
---


# Packages

## Install with Pip

pypi.org [pip](https://pypi.org/project/pip/):

> "pip is the package installer for Python. You can use pip to install packages
> from the Python Package Index and other indexes."

Run at the command line or from an ipython prompt:

    pip install packagename

[What is the difference between a python module and a python
package](https://stackoverflow.com/questions/7948494/whats-the-difference-between-a-python-module-and-a-python-package)

> "A package is a collection of modules in directories that give a package
> hierarchy."

[What is
setup.py?](https://stackoverflow.com/questions/1471994/what-is-setup-py)


### Install an old version

For example to install pandas 0.24.2

    python3 -m pip install --user pandas==0.24.2 

or

    pip3 install --user pandas==0.24.2

Sometimes you need to overwrite the existing version with I.

    pip install -I  package==version


### Install a local version

To [install the local version of a package with
pip](https://stackoverflow.com/a/18021540/2641825)

    pip install -e /develop/MyPackage

According to `man pip`, the `-e` option "installs a project in editable mode
(i.e. setuptools "develop mode") from a local project path or a VCS url".


#### Uninstall a local version

When uninstalling a package installed locally, you might get this error message: 

    uninstall localpackage
    # Found existing installation: localpackage 0.0.1
    # Can't uninstall 'localpackage'. No files were found to uninstall.

You can show the location of the package with

    pip show localpackage

Then remove it manually with

    rm -rf ~/.local/lib/python3.9/site-packages/localpackage*

And maybe this as well

    rm -rf ~/.local/lib/python3.9/site-packages/build/lib/localpackage*


### Install from a git repository

Install from the dev branch of a private repo on gitlab using ssh

    pip install git+ssh://git@gitlab.com/bioeconomy/forobs/biotrade.git@dev

Install from the dev branch of a private repo on gitlab using an
[authentication
token](https://docs.gitlab.com/ee/user/project/deploy_tokens/index.html)

    pip install git+https://gitlab+deploy-token-833444:ByW1T2bJZRtYhWuGrauY@gitlab.com/bioeconomy/forobs/biotrade.git@dev


## Install with Anaconda

Use conda update command to check to see if a new update is available. If conda
tells you an update is available, you can then choose whether or not to install
it.


### Install

Documentation conda.io  [installing
packages](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-pkgs.html#installing-packages)

> To install a specific package such as SciPy into an existing environment "myenv":

    conda install --name myenv scipy

> If you do not specify the environment name, which in this example is done by --name myenv, the package installs into the current environment:

    conda install scipy

> To install a specific version of a package such as SciPy:

    conda install scipy=0.15.0

> To install multiple packages at once, such as SciPy and cURL:

    conda install scipy curl

> Note: It is best to install all packages at once, so that all of the dependencies are installed at the same time.


### Update

Documentation conda.io [updating
packages](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-pkgs.html#updating-packages)

> Use the terminal or an Anaconda Prompt for the following steps.

> To update a specific package:

    conda update biopython

> To update Python:

    conda update python

> To update conda itself:

    conda update conda


### Environment file

Creating an environment file manually

You can create an environment file (environment.yml) manually to share with others.

EXAMPLE: A simple environment file:

name: stats
dependencies:
  - numpy
  - pandas

EXAMPLE: A more complex environment file:

name: stats2
channels:
  - javascript
dependencies:
  - python=3.6   # or 2.7
  - bokeh=0.9.2
  - numpy=1.9.*
  - nodejs=0.10.*
  - flask
  - pip:
    - Flask-Testing

Note

Note the use of the wildcard * when defining the patch version number. Defining the version number by fixing the major and minor version numbers while allowing the patch version number to vary allows us to use our environment file to update our environment to get any bug fixes whilst still maintaining consistency of software environment.




## Create a package

### Upload a package to pypi

To upload a package to pypi, you need a pypi account. The [instructions on
uploading distribution archives explain how to upload the package to pypi](
https://packaging.python.org/tutorials/packaging-projects/#uploading-the-distribution-archives):
    
      python3 -m twine upload --repository testpypi dist/*


### Authorship

It's only possible to specify one author field in setup.py.
The recommendation is to use a mailing list when there are multiple authors and
to set separate files for attribution.

[How to specify multiple authords in setup.py?](https://stackoverflow.com/questions/9999829/how-to-specify-multiple-authors-emails-in-setup-py)


### Add non code files

The Python packaging documentation on [adding non code
files](https://python-packaging.readthedocs.io/en/latest/non-code-files.html)

> "The mechanism that provides this is the MANIFEST.in file. This is
> relatively quite simple: MANIFEST.in is really just a list of relative file
> paths specifying files or globs to include.:

    include README.rst
    include docs/*.txt
    include funniest/data.json

> "In order for these files to be copied at install time to the package’s
> folder inside site-packages, you’ll need to supply include_package_data=True
> to the setup() function."

> "Files which are to be used by your installed library (e.g. data files to
> support a particular computation method) should usually be placed inside of
> the Python module directory itself. E.g. in our case, a data file might be at
> `funniest/funniest/data.json`. That way, code which loads those files can
> easily specify a relative path from the consuming module’s `__file__`
> variable."

The Python packaging [documentation on the Manifest commands](https://packaging.python.org/en/latest/guides/using-manifest-in/) 
The syntax of recursive-include  graft commands.

> Add all files under directories matching dir-pattern that match any of the
> listed patterns

    recursive-include dir-pattern pat1 pat2

> Add all files under directories matching dir-pattern

    graft dir-pattern

The Python packaging documentation on [source
dist](https://docs.python.org/3/distutils/sourcedist.html) gives an example of
the patterns

    include *.txt
    recursive-include examples *.txt *.py
    prune examples/sample?/build

> " The meanings should be fairly clear: include all files in the distribution
> root matching `*.txt`, all files anywhere under the examples directory matching
> `*.txt` or `*.py`, and exclude all directories matching examples/sample?/build.


### Version

[SO What is the correct way to share package version with setup.py and the
package?](https://stackoverflow.com/questions/17583443/what-is-the-correct-way-to-share-package-version-with-setup-py-and-the-package)

The version of a package has to be set both in `setup.py` and `__init__py` it's
crazy the number of options that people have thought about. [This
answers](https://stackoverflow.com/a/61960231/2641825) summarizes the state of
the art in 7 options, including a link to the
[python packaging user guide](https://packaging.python.org/en/latest/guides/single-sourcing-package-version/) 


#### Bump version

Install bumpversion

    pip install bumpversion

Increment the version number both in setup.py and __init__.py with the command
line tool bumpversion. First create a configuration file `.bumpversion.cfg`
where the `current_version` matches the versions in `setup.py` and
`packagename/__init__.py`

    [bumpversion]
    current_version = 0.0.5
    commit = True
    tag = True

    [bumpversion:file:setup.py]
    [bumpversion:file:biotrade/__init__.py]

Increment the version number in all files and the git tag with:

    bumpversion patch
    # Or to increment minor or major versions
    bumpversion minor
    bumpversion major

Push the corresponding tags to the remote repository

    git push origin --tags

Check the updated version in setup.py

    python setup.py --version 

Start an ipython prompt to test the package version

    ipython
    import packagename
    packagename.__version__


## Location of the python

[Get the location of the python
executable](https://stackoverflow.com/questions/749711/how-to-get-the-python-exe-location-programmatically)
with

    >>> import sys
    >>> print(sys.executable)

In virtual env, it can return the symlink to another folder. In that case, the
path can be deduced from 

    >>> import os
    >>> os.__file__



## Virtual environments

[Pipenv](https://pipenv.pypa.io/en/latest/)
makes pip and virtual env work together. 


# Compilers

## Numba just-in-time compiler

[Numba User Manual](https://numba.readthedocs.io/en/stable/user/5minguide.html)

> "When a call is made to a Numba-decorated function it is compiled to machine
> code “just-in-time” for execution and all or part of your code can
> subsequently run at native machine code speed!"


## Pythran Ahead of time compiler

https://github.com/serge-sans-paille/pythran

> "Pythran is an ahead of time compiler for a subset of the Python language,
> with a focus on scientific computing. It takes a Python module annotated with
> a few interface descriptions and turns it into a native Python module with
> the same interface, but (hopefully) faster."


# Control flow

dotnetperls: [not python](https://www.dotnetperls.com/not-python)

Sample code with a function and if conditions:

	def function(condition):
		if condition:
			print("Hi")
		if not condition:
			print("Bye")
	function(True)
	function(False)
	function('')
	function('lalala')

# Command line

## Parsing arguments with argparse

[Argparse Tutorial](https://docs.python.org/3/howto/argparse.html#) explain how
to create a python program that processes command line arguments. Save the
following in `prog.py`

    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("square", type=int,
                        help="display a square of a given number")
    parser.add_argument("-v", "--verbosity", type=int,
                        help="increase output verbosity")
    args = parser.parse_args()
    answer = args.square**2
    if args.verbosity == 2:
        print(f"the square of {args.square} equals {answer}")
    elif args.verbosity == 1:
        print(f"{args.square}^2 == {answer}")
    else:
        print(answer)

Usage

    $ python3 prog.py 4
    16
    $ python3 prog.py 4 -v
    usage: prog.py [-h] [-v VERBOSITY] square
    prog.py: error: argument -v/--verbosity: expected one argument
    $ python3 prog.py 4 -v 1
    4^2 == 16
    $ python3 prog.py 4 -v 2
    the square of 4 equals 16
    $ python3 prog.py 4 -v 3
    16

[SO answer](https://stackoverflow.com/a/42829692/2641825) explains that when
using **ipython**, you need to separate ipython arguments from your script
arguments using `--`.


# Databases

## SQL Alchemy

SQL Alchemy is a database abstraction layer. Interaction with the database is
built upon [metadata
objects](https://docs.sqlalchemy.org/en/14/core/schema.html):

> The core of SQLAlchemy’s query and object mapping operations are supported by
> **database metadata**, which is comprised of Python objects that describe tables
> and other schema-level objects. These objects are at the core of three major
> types of operations - issuing CREATE and DROP statements (known as DDL),
> constructing SQL queries, and expressing information about structures that
> already exist within the database. Database metadata can be expressed by
> explicitly naming the various components and their properties, using
> constructs such as Table, Column, ForeignKey and Sequence, all of which are
> imported from the sqlalchemy.schema package. It can also be generated by
> SQLAlchemy using a process called reflection, which means you start with a
> single object such as Table, assign it a name, and then instruct SQLAlchemy
> to load all the additional information related to that name from a particular
> engine source.

[Reflecting database
objects](https://docs.sqlalchemy.org/en/14/core/reflection.html)

    from sqlalchemy import MetaData
    from sqlalchemy import Table
    meta = MetaData(schema = "raw_comtrade")
    meta.bind = comtrade.database.engine
    yearly_hs2 = Table('yearly_hs2', meta, autoload_with=comtrade.database.engine)

SQL Alchemy has an
[automap](https://docs.sqlalchemy.org/en/14/orm/extensions/automap.html)
feature which generates mapped classes and relationships from a database
schema.

I used [sqlacodegen](https://pypi.org/project/sqlacodegen/) to automatically
generate python code from an existing PostGreSQl database table as follows

    sqlacodegen --schema raw_comtrade --tables yearly_hs2 postgresql://rdb@localhost/biotrade


### Check for table existence

Paul's [SO Answer](https://stackoverflow.com/a/69224576/2641825). SQL Alchemy's
recommended way to check for the presence of a table is to create an inspector
object and use its `has_table()` method. The following example was copied from
[sqlalchemy.engine.reflection.Inspector.has_table](https://docs.sqlalchemy.org/en/14/core/reflection.html#sqlalchemy.engine.reflection.Inspector.has_table),
with the addition of an SQLite engine to make it reproducible:

    from sqlalchemy import create_engine, inspect
    from sqlalchemy import MetaData, Table, Column, Text
    engine = create_engine('sqlite://')
    meta = MetaData()
    meta.bind = engine
    user_table = Table('user', meta, 
                       Column("name", Text),
                       Column("full_name", Text))
    user_table.create()
    inspector = inspect(engine)
    inspector.has_table('user')

You can also use the `user_table` metadata element `name` to check if it exists as such:

    inspector.has_table(user_table.name)


### ORM querying guide

#### Select where

SQL Alchemy [Object Relational Model Querying
Guide](https://docs.sqlalchemy.org/en/14/orm/queryguide.html)

    from sqlalchemy import select
    stmt = select(user_table).where(user_table.c.name == 'spongebob')
    print(stmt)

Since version 1.4 `.where()` is a synonym of `.filter()` as explained in [sqlalchemy.orm.Query.where](https://docs.sqlalchemy.org/en/14/orm/query.html#sqlalchemy.orm.Query.where).

To select only one column you can use [Select.with_only_columns](https://docs.sqlalchemy.org/en/14/core/selectable.html#sqlalchemy.sql.expression.Select.with_only_columns):

    from sqlalchemy import MetaData, Table, Column, Text
    meta = MetaData()
    table = Table('user', meta, 
                  Column("name", Text),
                  Column("full_name", Text))
    stmt = (table.select()
            .with_only_columns([table.c.name])
           )
    print(stmt)

Entering columns in the `select` method returns an error. Although it should be
valid according to the documentation.

    print(table.select([table.c.name]))
    # ArgumentError: SQL expression for WHERE/HAVING role expected, 
    # got [Column('name', Text(), table=<user>)].


#### Insert

[Insert](https://docs.sqlalchemy.org/en/14/core/dml.html#sqlalchemy.sql.expression.insert) some data into the `user` table

    from sqlalchemy import insert
    from sqlalchemy.orm import Session
    stmt = (
        insert(user_table).
        values(name='Bob', full_name='Sponge Bob')
    )
    with Session(engine) as session:
        result = session.execute(stmt)
        session.commit()


### ORM query to pandas 

The
[pandas.to_sql](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-sql-method)
method uses sqlalchemy to write pandas data frame to a PostgreSQL database.

> "The pandas.io.sql module provides a collection of query wrappers to both
> facilitate data retrieval and to reduce dependency on DB-specific API.
> Database abstraction is provided by SQLAlchemy if installed. In addition you
> will need a driver library for your database. Examples of such drivers are
> psycopg2 for PostgreSQL or pymysql for MySQL. For SQLite this is included in
> Python’s standard library by default." 


#### table.select(), select() or a session

Repeat the example table defined above, read the result of a
[select](https://docs.sqlalchemy.org/en/14/tutorial/data_select.html#the-select-sql-expression-construct)
statement into a pandas data frame:

    import pandas
    from sqlalchemy import create_engine
    from sqlalchemy import MetaData, Table, Column, Text
    from sqlalchemy.orm import Session
    # Define metadata and create the table
    engine = create_engine('sqlite://')
    meta = MetaData()
    meta.bind = engine
    user_table = Table('user', meta,
                       Column("name", Text),
                       Column("full_name", Text))
    user_table.create()
    # Insert data into the user table
    stmt = user_table.insert().values(name='Bob', full_name='Sponge Bob')
    with Session(engine) as session:
        result = session.execute(stmt)
        session.commit()
    # Select data into a pandas data frame
    stmt = user_table.select().where(user_table.c.name == 'Bob')
    df = pandas.read_sql_query(stmt, engine)

Another way importing the select statement:

    from sqlalchemy import select
    stmt = select(user_table).where(user_table.c.name == 'Bob')
    df = pandas.read_sql_query(stmt, engine)

Another way using a session

    with Session(engine) as session:
        df2 = pandas.read_sql(session.query(user_table).filter(user_table.name=="Bob").statement, session.bind)

Read the whole table into pandas

    df3 = pandas.read_sql_table("user", engine)

[Stack Overflow Answer](https://stackoverflow.com/a/69812326/2641825)


#### Define and insert the iris dataset

Define an ORM structure for the iris dataset, then use pandas to insert the
data into an SQLite database. Pandas inserts with `if_exists="append"` argument
so that it keeps the structure defined in SQL Alchemy.

    import seaborn
    import pandas
    from sqlalchemy import create_engine
    from sqlalchemy import MetaData, Table, Column, Text, Float
    from sqlalchemy.orm import Session

Define metadata and create the table

    engine = create_engine('sqlite://')
    meta = MetaData()
    meta.bind = engine
    iris_table = Table('iris',
                       meta,
                       Column("sepal_length", Float),
                       Column("sepal_width", Float),
                       Column("petal_length", Float),
                       Column("petal_width", Float),
                       Column("species", Text))
    iris_table.create()

Load data into the table

    iris = seaborn.load_dataset("iris")
    iris.to_sql(name="iris",
                con=engine,
                if_exists="append",
                index=False,
                chunksize=10 ** 6,
                )


#### Unique values

The SQL ALchemy `iris_table` from above can be used to build a select statement
that extracts unique values:

    from sqlalchemy import distinct, select
    stmt = select(distinct(iris_table.c.species))
    df = pandas.read_sql_query(stmt, engine)


## PostgreSQL

Create a database engin with SQLalchemy

    from sqlalchemy import create_engine
    engine = create_engine('postgresql://myusername:mypassword@myhost:5432/mydatabase')

Blogs and Stackoverflow

- [Load data into postgreSQL using python](https://hakibenita.com/fast-load-data-python-postgresql) (without pandas)

- [Load CSVs into PostgreSQL using python and
  pandas](https://medium.com/@apoor/quickly-load-csvs-into-postgresql-using-python-and-pandas-9101c274a92f)

- [SO question pygresql-vs-psycopg2](https://stackoverflow.com/questions/413228/pygresql-vs-psycopg2)

- [5 ways to backup your postgreSQl database using
  python](https://medium.com/poka-techblog/5-different-ways-to-backup-your-postgresql-database-using-python-3f06cea4f51)
  Mentions the [sh package](https://pypi.org/project/sh/), a subprocess replacement.


## SQLite

Create an SQLITE in memory database and add a table to it. 

    In [17]: from sqlalchemy import create_engine, inspect
        ...: from sqlalchemy import MetaData, Table, Column, Text
        ...: engine = create_engine('sqlite://')
        ...: meta = MetaData()
        ...: meta.bind = engine
        ...: user_table = Table('user', meta, Column("name", Text))
        ...: user_table.create()
        ...: inspector = inspect(engine)
        ...: inspector.has_table('user')
    Out[17]: True

Create a file based database at a specific path:

    # absolute path
    e = create_engine('sqlite:////path/to/database.db')


# Editors

## Spyder

I have set the following shortcuts to be similar to RStudio:

* Ctrl+H find and replace dialog
* Ctrl+R run selection or current line
* Ctrl+Shift+C comment/uncomment code block
* F1 inspect current object (i.e. display function and classes documentation)
* F2 go to function definition

## Vim 

See my page on [vim.html](vim.html).

# Neural Networks

## Pytorch

Print the size of the output layer

    import torch
    import torch.nn as nn
    x = torch.randn(28,28).view(-1,1,28,28)
    model = nn.Sequential(
          nn.Conv2d(1, 32, (3, 3)),
          nn.ReLU(),
          nn.MaxPool2d((2, 2)),
          nn.Conv2d(32, 64, (3, 3)),
    )
    print(model(x).shape)



# Objects


## Object types

`type()` displays the type of an object.

    i = 1
    print(type(i))
    # <type 'int'>
    x = 1.2
    print(type(x))
    # <type 'float'>
    t = (1,2)
    print(type(t))
    # <type 'tuple'>
    l = [1,2]
    print(type(l))
    # <type 'list'>


### Check object types

Check if a variable is a string, int or float

    isinstance("a", str)
    isinstance(1,  int)
    isinstance(1.2, float)


### Convert between object types

Character to numeric

    int("3")
    float("3.33")
    int("3.33")

Numeric to character

    str(2)

Convert a list to a comma separated string

    ",".join(["a","b","c"])

Another example with the list of the last 5 years

    import datetime
    year = datetime.datetime.today().year
    # Create a numeric list of years
    YEARS = [year - i for i in range(1,6)]
    # Convert each element of the list to a string
    YEARS = [str(x) for x in YEARS]
    ",".join(YEARS)


### Dictionary

Create a dictionary with curly braces

    ceci = {'x':1, 'y':2, 'z':3}

Converts 2 lists into a dictionary with the `dict` built in function

    dict(zip(['x', 'y', 'z'], [1, 2, 3]))

Dictionary comprehension

    d = {n: True for n in range(5)}

Loop over the key and values of a dictionary

    for key, value in ceci.items():
        print(key, "has the value", value)


### Iterator

The `map` function makes an iterator object of type `map`

    iter = map(lambda x:x+1,range(3))
    type(iter)
    [i for i in iter]


### List

[Remove an element from a list of strings ](https://stackoverflow.com/a/31077838/2641825)

    myList = ['a', 'b', 'c', 'd']
    myList.remove('c')
    myList
    ['a', 'b', 'd']

Create a list of strings using split (seen in [this
answer](https://stackoverflow.com/a/52110266/2641825))

    "slope, intercept, r_value, p_value, std_err".split(", ")


#### List of tuples

[How to flatten a list of tuples](https://stackoverflow.com/questions/10632839/transform-list-of-tuples-into-a-flat-list-or-a-matrix)

    nested_list = [(1, 2, 4), (0, 9)]

Using `reduce`:

    reduce(lambda x,y:x+y, map(list, nested_list))                                                                                                                              
    [1, 2, 4, 0, 9]

Using itertools.chain:

    import itertools
    list(itertools.chain.from_iterable(nested_list))

Using `extend`:
    
    flat_list = []
    for a_tuple in nested_list:
        flat_list.extend(list(a_tuple))                                                                                                                                     
    flat_list
    [1, 2, 4, 0, 9]


### Set

Instances of [set](https://docs.python.org/3/library/stdtypes.html#set)
provide the following operations:

	issubset(other)
	set <= other

	    Test whether every element in the set is in other.
For example  [SO answer using
issubset](https://stackoverflow.com/a/3931655/2641825)

    l = [1,2,3]
    m = [1,2]
    set(m).issubset(l)
    # True

	set < other

	    Test whether the set is a proper subset of other, that is, set <= other and set != other.

	issuperset(other)
	set >= other

	    Test whether every element in other is in the set.

	set > other

	    Test whether the set is a proper superset of other, that is, set >= other and set != other.

	union(*others)
	set | other | ...

	    Return a new set with elements from the set and all others.

Note the following perform a union:

    set(range(3,10)).union(set(range(5)))
    set(range(3,10)) | set(range(5))

But this is not a union:

    set(range(3,10)) or set(range(5))

	intersection(*others)
	set & other & ...

	    Return a new set with elements common to the set and all others.

	difference(*others)
	set - other - ...

	    Return a new set with elements in the set that are not in the others.

	symmetric_difference(other)
	set ^ other

	    Return a new set with elements in either the set or other but not both.
    

## Programming objects

### Inheritance and composition

Below is an example of object inheritance where a Car and a Boat classes
inherit from a Vehicle class.

    class Vehicle(object):

        def __init__(self, color, speed_max, garage=None):
            self.color = color
            self.speed_max = speed_max
            self.garage = garage

        def paint(self, new_color):
            self.color = new_color

        def go_back_home(self, new_color):
            self.position = self.go_to(self.parent.location)

    class Car(Vehicle):

        def open_door(self):
            pass

    class Boat(Vehicle):

        def open_balast(self):
            pass

    honda = Car('bleu', 60)
    gorgeoote = Boat('rouge', 30)
    honda.paint('purple')

Note that the object should be able to access it's parent properties through
the super() method.

Below an example of object composition where the Garage class is parent to many
Vehicle objects.

    class Garage(object):

        def __init__(self, all_vehicles):
            self.all_vehicles = all_vehicles

        def mass_paint(self, new_color):
            for v in self.all_vehicles: v.paint(new_color)

        def build_car(self, color):
            new_car = Car(color, 90, self)
            self.all_vehicles.append(new_car)
            return new_car

        @property
        def location(self):
            return '10, 18'


    mike = Garage([honda, gorgeoote])

    mike.mass_paint()

    sport_car = mike.build_car('rouge')


### Why do Python classes inherit object?

[Why do Python classes inherit
object?](https://stackoverflow.com/questions/4015417/why-do-python-classes-inherit-object)

> So, what should you do?

> In Python 2: always inherit from object explicitly. Get the perks.

> In Python 3: inherit from object if you are writing code that tries to be
> Python agnostic, that is, it needs to work both in Python 2 and in Python 3.
> Otherwise don't, it really makes no difference since Python inserts it for
> you behind the scenes. 


# HTTP

## File download

### Zipped csv files

The following example uses urllib.request.urlopen to download a zip file
containing Oceania's crop production data from the FAO statistical database. In
that example, it is necessary to define a minimal header, otherwise FAOSTAT
throws an `Error 403: Forbidden`. It was posted as a [StackOverflow
Answer](https://stackoverflow.com/a/68804963/2641825).

    import shutil
    import urllib.request
    import tempfile

    # Create a request object with URL and headers    
    url = "http://fenixservices.fao.org/faostat/static/bulkdownloads/Production_Crops_Livestock_E_Oceania.zip"
    header = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) '}
    req = urllib.request.Request(url=url, headers=header)

    # Define the destination file
    dest_file = tempfile.gettempdir() + '/' + 'crop.zip'
    print(f"File located at:{dest_file}")

    # Create an http response object
    with urllib.request.urlopen(req) as response:
        # Create a file object
        with open(dest_file, "wb") as f:
            # Copy the binary content of the response to the file
            shutil.copyfileobj(response, f)

Based on https://stackoverflow.com/a/48691447/2641825 and
https://stackoverflow.com/a/66591873/2641825, see also the documentation at
https://docs.python.org/3/howto/urllib2.html 


### JSON files

The following loads a JSON file into a pandas data frame from the Comtrade API.

    import urllib.request
    import json
    import pandas

    url_reporter = "https://comtrade.un.org/Data/cache/reporterAreas.json"
    url_partner = "https://comtrade.un.org/Data/cache/partnerAreas.json"

    # attempt with pandas.io, with an issue related to nested json
    pandas.io.json.read_json(url_reporter, encoding='utf-8-sig')
    pandas.io.json.read_json(url_partner)
    # `results` is a character column containing {'id': '4', 'text': 'Afghanistan'}.
    # Is there a way to tell read_json to load the id and text columns directly instead?

[SO answer](https://stackoverflow.com/a/68988284/2641825)

> "Since the whole processing is done in the pd.io.json.read_json method, we
> cannot select the keys to direct to the actual data that we are after. So you
> need to run this additional code to get your desired results:"

    df = pandas.io.json.read_json(url_reporter, encoding='utf-8-sig')
    df2 = pandas.json_normalize(df.results.to_list())

Other attempt using lower level packages

    req = urllib.request.Request(url=url_reporter)
    with urllib.request.urlopen(req) as response:
        json_content = json.load(response)
        df = pandas.json_normalize(json_content['results'])

    In [17]: df
    Out[17]:
          id                    text
    0    all                     All
    1      4             Afghanistan
    2      8                 Albania
    3     12                 Algeria
    4     20                 Andorra
    ..   ...                     ...
    252  876  Wallis and Futuna Isds
    253  887                   Yemen
    254  894                  Zambia
    255  716                Zimbabwe
    256  975                   ASEAN


- Related question I asked on SO.: [How to load a nested data frame with
  pandas.io.json.read_json?](https://stackoverflow.com/questions/68985729/how-to-load-a-nested-data-frame-with-pandas-io-json-read-json)

- Enconding issue [What is the difference between utf-8 and
  utf-8-sig?](https://stackoverflow.com/questions/57152985/what-is-the-difference-between-utf-8-and-utf-8-sig)

- [What's the difference between UTF-8 and UTF-8 without
  BOM?](https://stackoverflow.com/questions/2223882/whats-the-difference-between-utf-8-and-utf-8-without-bom) 


# ipython

Add these options at the ipyhton command line to reload objects automatically while you are coding

    %load_ext autoreload   
    %autoreload 2         


## Debugging in ipython


Once an error occurs at the ipython command line. 
Press `debug` then you can move up the stack trace with:

    `u`

Move down the stack trace with:

    `d` 

Show code context of the error:

    `l` 

Show available variable in the current context:

    `a` 


## Breakpoint

To break at every step in a loop, use the `breakpoint()` function in any part
of the code as explained in [setp by step debuging with
ipython](https://stackoverflow.com/questions/16867347/step-by-step-debugging-with-ipython).

    continue

## Running from ipython

Run a file from the ipython console

    %run -i test.py.

https://github.com/ipython/ipython/issues/1001


# Jupyter notebooks and lab


## Call bash from a notebook

Prefix the bash call with an exclamation mark, for example:

    !df -h

In fact the question mark also works from an ipython shell. 
See also 
[Difference between ! and % in Jupyter Notebooks](https://stackoverflow.com/questions/45784499/difference-between-and-in-jupyter-notebooks)


## Convert and execute notebooks programmatically

### At the shell

To work from the ipython command line it's useful to load execute the whole
notebook inside the ipython shell with

    ipython -c "%run notebook.ipynb"

It's also possible to convert the long notebooks to a python script with:

    jupyter nbconvert --to script notebook.ipynb

Then run the whole notebook and start an interactive shell with:

    ipython -i notebook.py

Otherwise I also sometimes open the synchronized markdown version of the
notebook and execute a few cells using Vim slime to sent them to a tmux pane
where ipython is running.


### Convert notebooks to html

Notebooks can be converted from the File / Save and Export Notebook As / HTML
menu. Or [at the command line with nbconvert](https://nbconvert.readthedocs.io/en/latest/usage.html#convert-html)

    jupyter nbconvert --to html notebook.ipynb


### From python

Run an ipython notebook from python using
[nbconver](https://nbconvert.readthedocs.io/en/latest/execute_api.html#example)'s
execute API:

    import nbformat
    from nbconvert.preprocessors import ExecutePreprocessor
    import jupytext

    ####################
    # Run one notebook #
    ####################
    filename = 'notebook.ipynb'
    with open(filename) as ff:
        nb_in = nbformat.read(ff, nbformat.NO_CONVERT)

    # Read a notebook from the markdown file synchronized by jupytext
    nb_md = jupytext.read('notebook.md')

    # Run the notebook
    ep = ExecutePreprocessor(timeout=600, kernel_name='python3')
    nb_out = ep.preprocess(nb_in)

    # Save the output notebook
    with open(filename, 'w', encoding='utf-8') as f:
        nbformat.write(nb_out, f)

Saving fails in my case.


## Dashboards and widgets

- [Voila dashboard](https://github.com/voila-dashboards/voila)

    - Examples dasboards and applications in the [Voilà
      gallery](https://voila-gallery.org/)

- [ipywidgets](https://github.com/jupyter-widgets/ipywidgets)
  
  > "interactive HTML widgets for Jupyter notebooks and the IPython kernel."


### Interactive widgets

[Documentation of interactive widgets](https://ipywidgets.readthedocs.io/en/latest/examples/Using%20Interact.html)

Create a text box

    def print_name(name):
        return("Name: " + name)
    interact(print_name, name="Paul")

Create a drop down list for an interactive plot

    import matplotlib.pyplot as plt
    import seaborn
    from ipywidgets import interact
    iris = seaborn.load_dataset("iris").set_index("species")

    def plot_iris(species):
        """Plot the given species"""
        df = iris.loc[species]
        ax = df.plot.scatter(x='petal_length', y='petal_width', title=species)
        ax.set_xlim(0,8)
        ax.set_ylim(0,4)

    interact(plot_iris, species=list(iris.index.unique()))

Use the `@interact` decorator

    @interact(species=list(iris.index.unique()))
    def plot_iris(species):
        """Plot the given species"""
        df = iris.loc[species]
        ax = df.plot.scatter(x='petal_length', y='petal_width', title=species)
        ax.set_xlim(0,8)
        ax.set_ylim(0,4)


## Display all rows and columns of a data frame

Display all columns

    pandas.options.display.max_columns = None

Display max rows

    pandas.set_option('display.max_rows', 500)

With a  context manager [as in this answer](https://stackoverflow.com/a/47113685/2641825)

    with pd.option_context('display.max_rows', 100, 'display.max_columns', 10):
    some pandas stuff

## Docker stacks

[Docker stacks for Jupyter notebooks](https://jupyter-docker-stacks.readthedocs.io/en/latest/)

[Selecting an
image](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html#core-stacks)


## Download data from a jupyter notebook    

I wrote this csv download function in an 
[SO answer](https://stackoverflow.com/a/57613621/2641825)

    def csv_download_link(df, csv_file_name, delete_prompt=True):
        """Display a download link to load a data frame as csv from within a Jupyter notebook"""
        df.to_csv(csv_file_name, index=False)
        from IPython.display import FileLink
        display(FileLink(csv_file_name))
        if delete_prompt:
            a = input('Press enter to delete the file after you have downloaded it.')
            import os
            os.remove(csv_file_name)

To get a link to a csv file, enter the above function and the code below in a jupyter notebook cell :

    csv_download_link(df, 'df.csv')

## Documentation with Jupyter

[Using jupyter to write documentation](https://hub.packtpub.com/using-jupyter-write-documentation/)


## Help in a jupyter notebook 

To get help on a function, enter `function_name?` in a cell. 
Quick hep can also be obtained by pressing SHIFT + TAB.

## Install Jupyter

To [install Jupyter](https://jupyter.org/install) notebooks on python3:

    pip3 install jupyter notebook

Then start the notebook server as such:

    jupyter notebook


## Plots in notebooks


It is [sometimes necessary to add the
following](https://stackoverflow.com/a/24884342/2641825) at the beginning of a
jupyter notebook so that plots are displayed inline

    %matplotlib inline

Change the size of a plot displayed in a notebook

    import seaborn
    p = seaborn.lineplot(x="year", y="value", hue="source", data=df1)
    p.figure.set_figwidth(15)


## Security and authentication on a public server

[Jupyter notebook with
authentication](https://stackoverflow.com/questions/37808410/ipython-jupyter-notebook-with-authentication)


## TOC Table of content in your notebooks

Install [jupyter_contrib_nbextensions](https://github.com/ipython-contrib/jupyter_contrib_nbextensions)

    python3 -m pip install --user jupyter_contrib_nbextensions
    python3 -m jupyter contrib nbextension install --user

Activate the table of content extension:

    python3 -m jupyter nbextension enable toc2/main

There are many other extensions available in this package.
**Optionally** you can install the jupyter notebook extension configurator (not needed)

    python3 -m pip install --user jupyter_nbextensions_configurator
    jupyter nbextensions_configurator enable --user

This will make a configuration interface available at:

    http://localhost:8888/nbextensions

Using the old Table of Content extension
[jupyter table of content extension](https://github.com/minrk/ipython_extensions#table-of-contents)

    jupyter nbconvert --to markdown mynotebook.ipynb
    jupyter nbconvert --to html mynotebook.ipynb

For a colleague using Anaconda [Installing
jupyter_contrib_nbextensions](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html)
specifies that 

> "There are conda packages for the notebook extensions and the
> jupyter_nbextensions_configurator available from conda-forge. You can install
> both using" 

    conda install -c conda-forge jupyter_contrib_nbextensions


## Jupyter and git version control 

### Jupytext with markdown and git

Convert notebooks to markdown so they are easier to track in git. 

Install https://github.com/mwouts/jupytext

    python3 -m pip install --user jupytext

More commands:

    python3 -m jupyter notebook --generate-config
    vim ~/.jupyter/jupyter_notebook_config.py

Add this line:

    c.NotebookApp.contents_manager_class = "jupytext.TextFileContentsManager"

And also this line if you always want to pair notebooks with their markdown counterparts:

    c.ContentsManager.default_jupytext_formats = "ipynb,md"

More commands:

    python3 -m jupyter nbextension install jupytext --py --user
    python3 -m jupyter nbextension enable  jupytext --py --user

Add syncing to a given notebook:

    # Markdown sync
    jupytext --set-formats ipynb,md --sync ~/repos/example_repos/notebooks/test.ipynb
    # Python sync
    jupytext --set-formats ipynb,py --sync ~/repos/example_repos/notebooks/test.ipynb

### Remove jupyter notebook output when using git

Alternative to the above, this post explains [how to remove  Jupyter notebook
output from terminal and when using git](https://janakiev.com/blog/jupyter-git-remove-output/).


# Errors, exceptions and logging

## Handling Exceptions with try and except statements

Python documentation on [Handling
Exceptions](https://docs.python.org/3/tutorial/errors.html#handling-exceptions).

    while True:
        try:
            x = int(input("Please enter a number: "))
            break
        except ValueError:
            print("Oops!  That was no valid number.  Try again...")

[Exception message capturing](https://stackoverflow.com/a/4690655/2641825)

    for i in [1,0]:
        try:
            print(1/i)
        except Exception as e:
            print("Failed to compute:", str(e))

Handle empty data in pandas

    from pandas.errors import EmptyDataError
    try:
        df = gfpmx_data[s]
        columns = df.columns
    except EmptyDataError:
        print(f"no data in file {s}")
        columns = []


## Raising Exceptions

Python documentation on [Raising
Exceptions](https://docs.python.org/3/tutorial/errors.html#raising-exceptions)

    raise Exception('spam')
    raise ValueError('Not an acceptable value')
    raise NameError("Wrong name: %s" % "quack quack quack")

Display variables in the error message:

    raise ValueError("This is wrong: %s" % "wrong_value")
    msg = "Value %s and value %s have problems."
    raise ValueError(msg % (1, 2))


## Warnings

Send a warning to the user

    import warnings
    warnings.warn("there is no data")


## Logging

docs.python.org [logging
cookbook](https://docs.python.org/3/howto/logging-cookbook.html)


Pylint error: Use lazy % formatting in logging functions

[Answer to Lazy evaluation of strings in python logging: comparing `%` with
`.format`](https://stackoverflow.com/a/52012660/2641825)

The documentation https://docs.python.org/2/library/logging.html suggest the
following for lazy evaluation of string interpolation: 

    logging.getLogger().debug('test: %i', 42)


# Functions

Functions in python can be defined with 

    def add_one(x):
        return x + 1
    add_one(1)

    # 2


## Annotations

[PEP 3107](https://peps.python.org/pep-3107/)

> Annotations for parameters take the form of optional expressions that follow the parameter name:

    def foo(a: expression, b: expression = 5):
        ...
> to annotate the type of a function’s return value. This is done like so:

    def sum() -> expression:
        ...

[SO example](https://stackoverflow.com/a/15073109/2641825)

    def kinetic_energy(m:'in KG', v:'in M/S')->'Joules':
    return 1/2_m_v**2

    kinetic_energy.__annotations__
    {'m': 'in KG', 'v': 'in M/S', 'return': 'Joules'}

The pandas code base doesn't use it everywhere, there are functions that
use the standard sphinx type of documentation
[timedeltas.py#L1094](https://github.com/pandas-dev/pandas/blob/a2029ce7bdd640931cb2c19e8a2c2c5a258fa5f9/pandas/core/arrays/timedeltas.py#L1094).
I have the impression that the annotation are used for the package internal
functions, while the sphinx documentation is used for the functions that are
exposed to the outside users. And in the same script, they use both
sphinx documentation and type annotations
[timedeltas.py#L952](https://github.com/pandas-dev/pandas/blob/a2029ce7bdd640931cb2c19e8a2c2c5a258fa5f9/pandas/core/arrays/timedeltas.py#L952).


[Type checking in python](https://www.blog.pythonlibrary.org/2020/04/15/type-checking-in-python/)

> There are several things to know about up front when it comes to type hinting
> in Python. Let's look at the pros of type hinting first:
>  
>  - Type hints are nice way to document your code in addition to docstrings
>  - Type hints can make IDEs and linters give better feedback and better autocomplete
>  - Adding type hints forces you to think about types, which may help you make good decisions during the design of your applications.
>  
> Adding type hinting isn't all rainbows and roses though. There are some downsides:
>  
>  - The code is more verbose and arguably harder to write
>  - Type hinting adds development time
>  - Type hints only work in Python 3.5+. Before that, you had to use type comments
>  - Type hinting can have a minor start up time penalty in code that uses it, especially if you import the typing module.
  

## Call by reference or call by value

When using numpy arrays, python displays a behaviour of call by reference 

    a = np.array([1,2])

    def changeinput(x, scalar):
        x[0] = scalar

    changeinput(a,3)

    a
    # array([3, 2])

This is really weird coming from R, which has a copy-on-modify principle.

The R Language Definition says this (in section 4.3.3 Argument Evaluation)

> "The semantics of invoking a function in R argument are call-by-value. In
> general, supplied arguments behave as if they are local variables initialized
> with the value supplied and the name of the corresponding formal argument.
> Changing the value of a supplied argument within a function will not affect
> the value of the variable in the calling frame. [Emphasis added]"


## Decorators

Decorators are a way to wrap a function around another function. 
It is useful to repeat a pattern of behaviour around a function.

* Data camp [course on decorators](https://www.datacamp.com/community/tutorials/decorators-python)
* Examples [5 use cases for decorators](https://www.oreilly.com/ideas/5-reasons-you-need-to-learn-to-write-python-decorators)

I have used decorators to cache the function output
along a data processing pipeline. 


### Property and cached property

Since python 3.8 there is also a `@cached_property` decorator
[functools.cached_property](https://docs.python.org/dev/library/functools.html#functools.cached_property)

> "Transform a method of a class into a property whose value is computed once
> and then cached as a normal attribute for the life of the instance. Similar
> to property(), with the addition of caching. Useful for expensive computed
> properties of instances that are otherwise effectively immutable."

There is also a `@cache` decorator
[functools.cache](https://docs.python.org/dev/library/functools.html#functools.cache)
that creates:

> "a thin wrapper around a dictionary lookup for the function arguments. Because it never needs to evict old values, this is smaller and faster than lru_cache() with a size limit.


## Deprecate arguments

Deprecate the old name of a function argument

    def agg_trade_eu_row(df, grouping_side="partner", index_side=None):
        if index_side is not None:
            warnings.warn("index_side is deprecated; use grouping_side", DeprecationWarning, 2)
            grouping_side = index_side

This [SO Questions](https://stackoverflow.com/q/49802412/2641825) asks how to
create an argument alias, without changing the number of arguments to the
function.


## Docstring Documentation 

Document python functions with the sphinx convention
[SO Answer](https://stackoverflow.com/a/40596167)

    def send_message(sender, recipient, message_body, priority=1) -> int:
       """
       Send a message to a recipient

       :param str sender: The person sending the message
       :param str recipient: The recipient of the message
       :param str message_body: The body of the message
       :param priority: The priority of the message, can be a number 1-5
       :type priority: integer or None
       :return: the message id
       :rtype: int
       :raises ValueError: if the message_body exceeds 160 characters
       :raises TypeError: if the message_body is not a basestring
       """


# Geopandas

geopandas help topic

- [Input output](https://geopandas.org/en/stable/docs/user_guide/io.html)
  reading and writing files

All geopandas examples below require the following imports:

    from matplotlib import pyplot as plt
    import geopandas


## Map of the World

Plot a world map of GDP per capita

    world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))
    world = world[(world.pop_est>0) & (world.name!="Antarctica")]
    world['gdp_per_cap'] = world.gdp_md_est / world.pop_est
    world.plot(column='gdp_per_cap')
    plt.show()


## Map of Europe

Load the JSON data from the Eurostat [NUTS
page](https://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts#nuts21)
(Nomenclature of territorial units for statistics) using the EPSG 3035 projection.

    nuts = geopandas.read_file("~/downloads/NUTS_RG_20M_2021_3035.geojson")
    ax = nuts.plot()
    ax.set_xlim(2000000, 8000000)
    ax.set_ylim(1000000, 5500000)
    nuts.boundary.plot(color="darkgreen", ax=ax)
    plt.show()

NUTS ID have either 2, 3, 4 or 5 characters

    nuts.NUTS_ID.str.len().unique()
    # array([2, 3, 4, 5])
    nuts["id_n"] = nuts.NUTS_ID.str.len()
    def map_borders(df):
        ax = df.plot()
        ax.set_xlim(2000000, 8000000)
        ax.set_ylim(1000000, 5500000)
        df.boundary.plot(color="grey", ax=ax)
        return(ax)

Map NUTS 0 which has 2 character codes

    nuts0 = nuts.query("id_n == 2")
    map_borders(nuts0)
    plt.show()

Map NUTS 1 which has 3 character codes

    nuts1 = nuts.query("id_n == 3")
    map_borders(nuts1)
    plt.show()

Map NUTS 2 which has 4 character codes

    nuts2 = nuts.query("id_n == 4")
    map_borders(nuts2)
    plt.show()
    # Drop the geometry and write the description columns to a csv file
    nuts2.drop(columns="geometry").to_csv("/tmp/nuts2.csv")

Blogs and help pages:

- Jan's blog, a quick visual comparison of the EPSG 3035, EPSG 4326 and EPSG 3857
  projections on [plotting maps with european data in python part
  i](https://jan-46106.medium.com/plotting-maps-with-european-data-in-python-part-i-decd83837de4)

    - I chose EPSG 3035 because it represents areas in a comparable way. 
    - Use `.boundary.plot()`to plot boundaries.

- towardsdatascience.com [using eurostat statistical data on Europe with python](https://towardsdatascience.com/using-eurostat-statistical-data-on-europe-with-python-2d77c9b7b02b) Setting x an y limits to the map of continental Europe only


# Math

[How to do maths in python 3 with operators](https://www.digitalocean.com/community/tutorials/how-to-do-math-in-python-3-with-operators) 

2 to the power of 3

    2**3
    # 8

## Floor division

Floor division

    5//3
    # 1
    # Use it to extract the year of a Comtrade period
    202105 // 100


## Modulo

    5%3
    # 2
    12
    # Use it to extract the last 2 digits of an integer
    202105 % 100


## Sympy

https://www.sympy.org/en/index.html

> "SymPy is a Python library for symbolic mathematics. It aims to become a
> full-featured computer algebra system (CAS) while keeping the code as simple
> as possible in order to be comprehensible and easily extensible. SymPy is
> written entirely in Python."


# Modelling


## Pyomo

[Pyomo
Cookbook](https://jckantor.github.io/ND-Pyomo-Cookbook/01.00-Getting-Started-with-Pyomo.html)

> "Pyomo is well suited to modeling simple and complex systems that can be
> described by linear or nonlinear algebraic, differential, and partial
> differential equations and constraints."


## Panel data regressions

- The package
  [linearmodels](https://bashtage.github.io/linearmodels/panel/panel/linearmodels.panel.model.PanelOLS.html#linearmodels.panel.model.PanelOLS)
  provides fixed effects estimator for panel data.

- blog [Panel data with
  python](https://towardsdatascience.com/a-guide-to-panel-data-regression-theoretics-and-implementation-with-python-4c84c5055cf8)


# Numpy vectors and matrices (arrays)

All examples below are based on the numpy package being imported as np :

    import numpy as np


## Logical operators and binary operations

I mostly use [binary
operators](<https://numpy.org/doc/stable/reference/routines.bitwise.html) on
boolean arrays for index selections in pandas data frames.

Bitwise and

    np.array([True, True]) & np.array([False, True])

Bitwise not

    ~np.array([True, False])

They are equivalent to logical operators
[numpy.logical_and](https://numpy.org/doc/stable/reference/generated/numpy.logical_and.html),
[numpy.logical_not](https://numpy.org/doc/stable/reference/generated/numpy.logical_not.html)
for logical arrays.

A [SO answer](https://stackoverflow.com/a/54435820/2641825) quotes [the
NumPy v1.15
Manual](https://www.numpy.org/devdocs/user/numpy-for-matlab-users.html#numpy-for-matlab-users-notes)

    > If you know you have boolean arguments, you can get away with using
    > NumPy’s bitwise operators, but be careful with parentheses, like this:
    > `z = (x > 1) & (x < 2)`. The absence of NumPy operator forms of
    > `logical_and` and `logical_or` is an unfortunate consequence of Python’s
    > design.

> So one can also use `~` for `logical_not` and `|` for `logical_or`.

[Bitwise and](https://numpy.org/doc/stable/reference/generated/numpy.bitwise_and.html)

> "The number 13 is represented by 00001101. Likewise, 17 is represented by
> 00010001. The bit-wise AND of 13 and 17 is therefore 000000001, or 1"

    np.bitwise_and(13, 17)
    # 1

The `&` operator can be used as a shorthand for np.bitwise_and on ndarrays.

    x1 = np.array([2, 5, 255])
    x2 = np.array([3, 14, 16])
    x1 & x2


## Indexing Multi-dimensional arrays and masks

Numpy [array indexing](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)

> "Basic slicing extends Python’s basic concept of slicing to N dimensions. Basic slicing occurs when 
> obj is a slice object (constructed by start:stop:step notation inside of brackets), an integer, or a tuple of slice objects and integers."
> [...]
> The basic slice syntax is i:j:k where i is the starting index, j is the stopping index, and k is the step ($k\neq0$).
> "[...]
> "Advanced indexing always returns a copy of the data (contrast with basic slicing that returns a view)." 
> "Integer array indexing allows selection of arbitrary items in the array based on their N-dimensional index.
> Each integer array represents a number of indexes into that dimension."

    x[0:3,0:2]
    # array([[0.64174957, 0.18540429],
    #        [0.97558697, 0.69314058],
    #        [0.51646795, 0.71055115]])

In this case because every row is selected, it is the same as:

    x[:,0:2]

Examples modified from https://docs.scipy.org/doc/numpy/user/basics.indexing.html

    y = np.arange(35).reshape(5,7)
    print(y[np.array([0,2,4]), np.array([0,1,2])])

    print('With slice 1:3')
    print(y[np.array([0,2,4]),1:3])
    print('is equivalent to')
    print(y[np.array([[0],[2],[4]]),np.array([[1,2]])])
    # This one is the same but transposed, which is weird
    print(y[np.array([[0,2,4]]),np.array([[1],[2]])])
    # Notice the difference with the following
    print(y[np.array([0,2,4]),np.array([1,2,3])])

Masks [masked array](https://docs.scipy.org/doc/numpy/reference/maskedarray.generic.html)
We wish to mark the fourth entry as invalid. The easiest is to create a masked array:

    x = np.array([1, 2, 3, -1, 5])
    mx = np.ma.masked_array(x, mask=[0, 0, 0, 1, 0])
    print(x.sum(), mx.sum())
    # 10 11


## Matrix creation and shapes

Create a vector

    a = np.array([1,2,3])

Create a matrix

    b = np.array([[1,2,3],[5,6,6]])

Shape

    a.shape
    # (3,)
    b.shape
    # (2, 3)

Matrix of zeroes

    np.zeros([2,2])
    #array([[0., 0.],
    #       [0., 0.]])

Create a matrix with an additional dimension 

    np.zeros(b.shape + (2,))
    array([[[0., 0.],
            [0., 0.],
            [0., 0.]],

           [[0., 0.],
            [0., 0.],
            [0., 0.]]])

Transpose

    b.transpose()
    # array([[1, 5],
    #        [2, 6],
    #        [3, 6]])
    c = b.transpose()

Math functions in numpy:

    np.cos()
    np.sin()
    np.tan()
    np.exp()

min and max

    x = np.array([1,2,3,4,5,-7,10,-8])
    x.max()
    # 10
    x.min()
    # -8

## Matrix multiplication

Matrix multiplication [matmul](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html)

    np.matmul(a,c)
    # array([14, 35])

    # Can also be written as
    a @ c
    # array([14, 35])

Otherwise the multiplication symbol implements an element wise multiplication, also called the

[Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)).
It only works on 2 matrices of same dimensions. 
Element-wise multiplication is used for example in convolution kernels. 

    b * b
    # array([[ 1,  4,  9],
    #        [25, 36, 36]])

So here is again an example showing the difference between

    m = np.array([[0,1],[2,3]])

Element wise multiplication :

    m * m 
    # array([[0, 1],
    #        [4, 9]])

Matrix multiplication :

    m @ m
    # array([[ 2,  3],
    #        [ 6, 11]])


## Norm of a matrix

Linear algebra functionalities are provided by numpy.linalg
For example the norm of a matrix or vector:

    np.linalg.norm(x)
    # 16.3707055437449
    np.linalg.norm(np.array([3,4]))
    # 5.0
    np.linalg.norm(a)
    # 3.7416573867739413

Norm of the matrix for the regularization parameter in a machine learning model

    bli = np.array([[1, 1, 0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0, 0.0, 0.0],
                    [0.0, 1, 0.0, 0.0, 0.0]])
    sum(np.linalg.norm(bli, axis=0)**2) 3.0000000000000004
    sum(np.linalg.norm(bli, axis=1)**2) 3.0000000000000004
    np.linalg.norm(bli)**2 2.9999999999999996

Append vs concatenate

    x = np.array([1,2])
    print(np.append(x,x))
    # [1 2 1 2]
    print(np.concatenate((x,x),axis=None))
    a = np.array([[1, 2], [3, 4]])
    b = np.array([[5, 6]])
    print(np.concatenate((a, b), axis=0))
    print(np.concatenate((a, b.T), axis=1))
    print(np.concatenate((a, b), axis=None))


## Power

Power of an array

    import numpy as np
    a = np.arange(4).reshape(2, 2)
    print(a)
    print(a**2)
    print(a*a)
    np.power(a, 2)

Broadcast the power operator

    np.power(a, a)


## Random vector or matrices

    x = np.random.random([3,4])
    x
    # array([[0.64174957, 0.18540429, 0.7045183 , 0.44623567],
    #        [0.97558697, 0.69314058, 0.32469324, 0.82612627],
    #        [0.51646795, 0.71055115, 0.74864751, 0.2142459 ]])


Random choice, with a given probability
Choose zero with probability 0.1 and one with probability 0.9.

    for i in range(10):
        print(np.random.choice(2, p=[0.1, 0.9]))
        
    print(np.random.choice(2, 10, p=[0.1, 0.9]))
    print(np.random.choice(2, (10,10), p=[0.1, 0.9]))

    [[1 1 1 1 1 1 1 1 1 0]
     [1 1 1 1 1 1 1 1 1 1]
     [1 1 1 1 1 1 1 1 1 0]
     [1 1 1 1 1 1 1 1 1 1]
     [1 1 1 1 1 1 1 0 1 1]
     [1 1 1 0 1 1 1 1 1 1]
     [1 1 0 1 1 1 1 1 0 1]
     [1 1 1 1 1 1 1 1 1 1]
     [1 1 1 1 1 1 0 1 1 0]
     [1 1 1 1 1 1 1 1 1 1]]

Error if probabilities do not sum up to one

    print(np.random.choice(2, p=[0.1, 0.8]))

    # ---------------------------------------------------------------------------
    # ValueError                                Traceback (most recent call last)
    # <ipython-input-31-8a8665287968> in <module>
    # ----> 1 print(np.random.choice(2, p=[0.1, 0.8]))

    # mtrand.pyx in numpy.random.mtrand.RandomState.choice()

    # ValueError: probabilities do not sum to 1


# Pandas data frames

All code below assumes you have imported pandas
    
    import pandas 


## Assign values

### Create a data frame

You can create a data frame by passing a **dictionary of lists** with column names as keys

    df = pandas.DataFrame({'x':range(0,3), 
                      'y':['a','b','c']})

    #      a  b
    #   0  0  3
    #   1  1  4
    #   2  2  5

Or by passing a list of tuples and defining the `columns` argument

    pandas.DataFrame(
        list(zip(range(0,3), ['a','b','c'])), 
        columns=["x", "y"]
    )

Random numbers

    import numpy as np
    df = pandas.DataFrame({'x':np.random.random(100)})


### Use the assign method

Create a new column based on another one

    df = pandas.DataFrame({'a':range(0,3), 
                           'b':['p','q','r'], 
                           'c':['m','n','o']})
    df["d"] = df["a"] * 2

Use the `assign` method

    df.assign(e = lambda x: x["a"] * 3)


#### Sum columns together and compute a share

Sum all columns in an assign and use it to compute a share

    import seaborn
    iris = seaborn.load_dataset("iris").set_index("species")
    iris.assign(sp_sum = lambda x: x.sum(axis=1),
                sl_share = lambda x: x.sepal_length / x.sp_sum)


### Recursive computation x(t) depends on x(t-1)

A recursive function is difficult to vectorize because each input at time t
depends on the previous input at time t-1. When possible use a year index for
shorter selection with `.loc()`.

    import pandas
    df = pandas.DataFrame({'year':range(2020,2024),'a':range(3,7)})
    df1 = df.copy()
    # Set the initial value
    t0 = min(df1.year)
    df1.loc[df1.year==t0, "x"] = 0

    # Doesn't work when the right side of the equation is a pandas.core.series.Series
    for t in range (min(df1.year)+1, max(df1.year)+1):
        df1.loc[df1.year==t, "x"] = df1.loc[df1.year==t-1,"x"] + df1.loc[df1.year==t-1,"a"]
    print(df1)
    #    year  a    x
    # 0  2020  3  0.0
    # 1  2021  4  NaN
    # 2  2022  5  NaN
    # 3  2023  6  NaN
    print(type(df1.loc[df1.year==t-1,"x"] + df1.loc[df1.year==t-1,"a"]))
    # <class 'pandas.core.series.Series'>

    # Works when the right side of the equation is a numpy array
    for t in range (min(df1.year)+1, max(df1.year)+1):
        df1.loc[df1.year==t, "x"] = (df1.loc[df1.year==t-1,"x"] + df1.loc[df1.year==t-1,"a"]).unique()
        #break
    print(df1)
    #    year  a     x
    # 0  2020  3   0.0
    # 1  2021  4   3.0
    # 2  2022  5   7.0
    # 3  2023  6  12.0
    print(type((df1.loc[df1.year==t-1,"x"] + df1.loc[df1.year==t-1,"a"]).unique()))
    # <class 'numpy.ndarray'>

    # Assignement works directly when the .loc() selection is using a year index
    df2 = df.set_index("year").copy()
    # Set the initial value
    df2.loc[df2.index.min(), "x"] = 0
    for t in range (df2.index.min()+1, df2.index.max()+1):
        df2.loc[t, "x"] = df2.loc[t-1, "x"] + df2.loc[t-1,"a"]
        #break
    print(df2)
    #       a     x
    # year
    # 2020  3   0.0
    # 2021  4   3.0
    # 2022  5   7.0
    # 2023  6  12.0
    print(type(df2.loc[t-1, "x"] + df2.loc[t-1,"a"]))
    # <class 'numpy.float64'>

    #SO answer using cumsum

Our real problem is more complicated since there is a multiplicative and an
additive component

    import pandas
    df3 = pandas.DataFrame({'year':range(2020,2024),'a':range(3,7), 'b':range(8,12)})
    df3 = df3.set_index("year").copy()
    # Set the initial value
    initial_value = 1
    df3.loc[df3.index.min(), "x"] = initial_value
    # Use a loop
    for t in range (df3.index.min()+1, df3.index.max()+1):
        df3.loc[t, "x"] = df3.loc[t-1, "x"] * df3.loc[t-1, "a"] + df3.loc[t-1, "b"]
    # Use cumsum and cumprod
    df3["cumprod_a"] = df3.a.cumprod().shift(1).fillna(1)
    df3["cumsum_cumprod_a_b"] = df3.cumprod_a.cumsum().shift(1).fillna(0) * df3.b
    df3["x2"] = df3.cumprod_a * initial_value + df3.cumsum_cumprod_a_b
    print(df3)


- `type(df1.loc[df1.year==t-1,"x"] + df1.loc[df1.year==t-1,"a"])` is a pandas
  series while `type(df2.loc[t-1, "x"] + df2.loc[t-1,"a"])` is a numpy float.
  Why are types different? 

- Is there a better way to write a recursive `.loc()` assignment than to use
  `.unique()`? 

See also:

- related Question and Answer on [recursive
  assignment](https://stackoverflow.com/a/38008937/2641825)
- related documentation on [Mutating User Defined Function
  methods](https://pandas.pydata.org/pandas-docs/stable/user_guide/gotchas.html#mutating-with-user-defined-function-udf-methods)

> "It is a general rule in programming that one should not mutate a container
> while it is being iterated over. Mutation will invalidate the iterator,
> causing unexpected behavior."
> [...]
> "To resolve this issue, one can make a copy so that the mutation does not
> apply to the container being iterated over."


### Set values with .loc

Create an example data frame

    import pandas
    df = pandas.DataFrame([[1, 2], [4, 5], [7, 8]],
                          index=['cobra', 'viper', 'sidewinder'],
                          columns=['max_speed', 'shield'])

Set value for all items matching the list of labels

    df.loc[['viper', 'sidewinder'], ['shield']] = 50

    #             max_speed  shield
    # cobra               1       2
    # viper               4      50
    # sidewinder          7      50


### Map values with a dictionary

    df = pandas.DataFrame({'lettre':['p','q','r','r','s','v','p']})
    mapping = {'p':'pour','q':'quoi','r':'roi'}
    df["mot"] = df["lettre"].map(mapping)


## Convert data frames

Convert 2 columns to a dictionary

    df = pandas.DataFrame({'a':range(0,3), 
                           'b':['p','q','r'], 
                           'c':['m','n','o']})
    df.set_index('b').to_dict()['c']


### Convert a column to string, float or integer

Convert a string to a numeric type

    s = pandas.Series(["1", "2", "a"])
    pandas.to_numeric(s, errors="coerce")

    # The following would return an error
    s.astype(float)
    s.astype(int)
    # And errors="ignore" would not convert at all
    s.astype(float, errors="ignore")
    pandas.to_numeric(s, errors="ignore")


Convert an integer to a string type

    s = pandas.Series(range(3))
    s.astype(str)


### Convert one value to a scalar

SO question [Convert to
scalar](https://stackoverflow.com/questions/47910328/convert-pandas-dataframe-value-to-scalar)

- iat()

    > "Access a single value for a row/column pair by integer position. Similar to
    > iloc, in that both provide integer-based lookups. Use iat if you only need to
    > get or set a single value in a DataFrame or Series."

- squeeze()

    > "Squeeze 1 dimensional axis objects into scalars. Series or DataFrames with a
    > single element are squeezed to a scalar. DataFrames with a single column or a
    > single row are squeezed to a Series. Otherwise the object is unchanged. This
    > method is most useful when you don’t know if your object is a Series or
    > DataFrame, but you do know it has just a single column. In that case you can
    > safely call squeeze to ensure you have a Series."


## Compare data frames

Pure equality example from
[pandas.DataFrame.equals](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.equals.html)

    import pandas
    df = pandas.DataFrame({1: [10], 2: [20]})
    exactly_equal = pandas.DataFrame({1: [10], 2: [20]})
    df.equals(exactly_equal)
    different_column_type = pandas.DataFrame({1.0: [10], 2.0: [20]})
    df.equals(different_column_type)
    different_data_type = pandas.DataFrame({1: [10.0], 2: [20.0]})
    df.equals(different_data_type)

Testing closeness (for example with floating point results computed in another
software)

    import numpy as np
    df.equals(df+1e-6)
    np.testing.assert_allclose(df,df+1e-7)
    np.testing.assert_allclose(df,df+1e-3)


## Concatenate and merge

### Concatenate  2 data frames

### Concatenate 2 series

Concatenate two series
[SO](https://stackoverflow.com/questions/18062135/combining-two-series-into-a-dataframe-in-pandas)
Notice the difference between the default `axis=0` concatenate on the index,
and `axis=1` concatenate on the columns.

    import pandas
    s1 = pandas.Series([1, 2, 3], index=['A', 'B', 'c'], name='s1')
    s2 = pandas.Series([4, 5, 6], index=['A', 'B', 'D'], name='s2')
    pandas.concat([s1, s2], axis=0)
    pandas.concat([s1, s2], axis=1)


### Merge or join

Stackoverflow [Pandas merging](https://stackoverflow.com/questions/53645882/pandas-merging-101/53645883#53645883)


## Columns

### List Columns

List columns as an index object

    df = pandas.DataFrame({'a':range(0,3),'b':range(3,6)})
    df.columns

List columns as a list

    df.columns.tolist()

Select only certain columns in a list

    df['bla'] = 0
    cols = df.columns.tolist()
    [name for name in cols if 'a' in name]


### Remove empty columns

Remove empty columns

    import pandas
    import numpy as np
    df = pandas.DataFrame({'A' : ['bli', 'bla', 'bla', 'bla', 'bla'],
                           'B' : [np.nan, '2','2', '4', '1'],
                           'C' : np.nan})
    columns_to_keep = [x for x in df.columns if not all(df[x].isna())]
    df = df[columns_to_keep].copy()


### Rename columns

Rename the 'a' column to 'new'

    df.rename(columns={'a':'new'})

Rename columns to snake case using a regular expression

    import re
    df.rename(columns=lambda x: re.sub(r" ", "_", str(x)).lower(), inplace=True)
    # Another regexp that replaces all non alphanumeric characters by an
    # underscore
    df.rename(columns=lambda x: re.sub(r"\W+", "_", str(x)).lower(), inplace=True)

Remove parenthesis and dots in column names

    df.rename(columns=lambda x: re.sub(r"[()\.]", "", x), inplace=True)

### Reorder columns

Place the last column first

      cols = df.columns.to_list()
      cols = [cols[-1] + cols[:-1]
      df = df[cols]
   
This [SO Answer](https://stackoverflow.com/a/58776941/2641825) provide 6
different ways to reorder columns.


### Replace the content of columns

Replace Comtrade product code by the FAOSTAT product codes

    # Create a dictionary from 2 columns of a data frame
    product_dict = product_mapping.set_index('comtrade_code').to_dict()['faostat_code']
    df_comtrade["product_code"] = df_comtrade["product_code"].replace(product_dict)


### Variable Type

To change the type of a column use astype:

    s = pandas.Series(range(3))
    s.to_list()
    s.astype(str).to_list()
    s.astype(float).to_list()

Note using NA values is not possible with the base integer type, it requires a
special type Int64 as explained in this [SO
answer](https://stackoverflow.com/a/67270477/2641825)


### Memory usage

To display the memory usage of [each column in a pandas data frame](https://stackoverflow.com/questions/18089667/how-to-estimate-how-much-memory-a-pandas-dataframe-will-need)

    import pandas
    df = pandas.DataFrame({'x':range(0,3), 'y':['a','b','c']})
    print(df.memory_usage(deep=True))
    print(df.memory_usage(deep=True).sum())
    df.info()

Using `sys.getsizeof`:

    import sys
    print(sys.getsizeof(df))

Changing a repeated data series to a categorical can help reduce memory usage

    import seaborn
    iris = seaborn.load_dataset("iris")
    print(iris["species"].memory_usage(deep=True))
    print(iris["species"].astype('category').memory_usage(deep=True))
    iris2 = iris.copy()
    iris2["species"] = iris["species"].astype('category')
    print(sys.getsizeof(iris2))
    print(sys.getsizeof(iris))


## Datetime operations

Create date time columns from a character column

    import pandas
    pandas.to_datetime('2020-01-01', format='%Y-%m-%d')
    pandas.to_datetime('2020-01-02')
    pandas.to_datetime('20200103')

Extract the year

    s = pandas.Series(pandas.date_range("2000-01-01", periods=3, freq="Y"))
    print(s)
    print(s.dt.year)


## Group by operations

Compute the sum of sepal length grouped by species

    import seaborn
    iris = seaborn.load_dataset("iris")
    # Aggregate one value
    iris.groupby('species')["sepal_length"].agg(sum).reset_index()
    # Aggregate multiple values
    iris.groupby('species')[["sepal_length", "petal_length"]].agg(sum).reset_index()
    # Aggregate multiple values and give new names
    iris.groupby('species').agg(sepal_length_sum = ('sepal_length', sum),
                                petal_length_sum = ('petal_length', sum))

Compute the sum but repeated for every original row

    iris['sepal_sum'] = iris.groupby('species')['sepal_length'].transform('sum')
    iris

This is useful to compute the [share of total in each
group](https://stackoverflow.com/questions/23377108/pandas-percentage-of-total-with-groupby)
for example.

Compute the cumulative sum of the sepal length

    iris['cumsum'] = iris.groupby('species').sepal_length.cumsum()
    ris['cumsum'].plot()
    from matplotlib import pyplot
    pyplot.show()

Compute a lag 

    iris['cumsum_lag'] = iris.groupby('species')['cumsum'].transform('shift', fill_value=0)
    iris[['cumsum', 'cumsum_lag']].plot()
    pyplot.show()


### Compute with a lambda function

Beyond standard function such as `sum` and `mean`, it's possible to use a self
defined lambda function as follows

    import numpy as np
    (iris
     .groupby(["species"])
     .agg(pw_sum = ("petal_width", sum),
          pw_sum_div_by_10 = ("petal_width", lambda x: x.sum()/0),
          n = ("petal_width", len),
          mean1 = ("petal_width", np.mean))
     .assign(mean2 = lambda x: x.pw_sum / x.n)
    )


### Lag or shift a grouped variable

Load the flights dataset and for each month, display the passenger value in the
same month of the previous year. Compare the `passengers` and
`pass_year_minus_one` columns by displaying the tables for January and
December. 

    import seaborn
    flights = seaborn.load_dataset("flights")
    flights['pass_year_minus_one'] = flights.groupby(['month']).passengers.shift()
    flights.query("month=='January'")
    flights.query("month=='December'")


### Min or max in group

Min or max in each group

    df.loc[df.groupby('A')['B val'].idxmin()]


### Slice, get the first elements of each group

- [How do I select the first row in each group in
groupby](https://stackoverflow.com/questions/30486417/pandas-how-do-i-select-first-row-in-each-group-by-group)

    import pandas
    import numpy as np
    df = pandas.DataFrame({'A' : ['foo', 'foo', 'bar', 'bar', 'bar'],
                           'B' : ['1', '2','2', '4', '1'],
                           'C' : [np.nan, 'X', 'Y', 'Y', 'Y']})
    df.sort_values('B').groupby('A').nth(0)
    df.sort_values('B').groupby('A').nth(list(range(2)))
    df.sort_values('B').groupby('A').head(2)

### Transform and apply

[SO answer explaining the difference between transform and apply](https://stackoverflow.com/a/47143056/2641825)


## Index

Index can be converted back to a data frame
See also index selection in the ".loc" section.

### Drop index levels

Example from [pandas DataFrame
drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html)

    import pandas
    midx = pandas.MultiIndex(levels=[['lama', 'cow', 'falcon'],
                                 ['speed', 'weight', 'length']],
                         codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],
                                [0, 1, 2, 0, 1, 2, 0, 1, 2]])
    df = pandas.DataFrame(index=midx, columns=['big', 'small'],
                      data=[[45, 30], [200, 100], [1.5, 1], [30, 20],
                            [250, 150], [1.5, 0.8], [320, 250],
                            [1, 0.8], [0.3, 0.2]])
    df
    df.drop(index='cow', columns='small')
    df.drop(index='length', level=1)


### Recursive computation on a index in a loop

#### Simple index case

Compute in a loop based on the value of the previous year t-1.
If there is a single value by year, scalar computation

    df = pandas.DataFrame({'x':range(0,10)})
    df.loc[0, "y"] = 2
    for t in range(1, len(df)):
        df.loc[t, "y"] = pow(df.loc[t-1, "y"], df.loc[t, "x"]/2)
    df

If there are multiple values for each single year
vector computation.

    import itertools
    import pandas
    countries = ["a","b","c","d"]
    years = range(1990, 2020)
    expand_grid = list(itertools.product(countries, years))
    df = pandas.DataFrame(expand_grid, columns=('country', 'year'))
    df["x"] = 1
    df["x"] = df["x"].cumsum()
    df.set_index(["year"], inplace=True)
    df.loc[min(years), "y"] = 2
    for t in range(min(years)+1, max(years)+1):
        df.loc[t, "y"] = pow(df.loc[t-1, "y"], df.loc[t, "x"]/2)
    df


#### Multi index case

I would like to compute the consumption equation of a partial equilibrium
model.

    df = pandas.DataFrame({'x':range(0,3), 
                           'y':['a','b','c']})

    for t in range(gfpmx_data.base_year + 1, years.max()+1):
        # TODO: replace this loop by vectorized operations using only the index on years
        for c in countries:
            # Consumption
            swd.loc[(t,c), "cons2"] = (swd.loc[(t, c), "constant"]
                                       * pow(swd.loc[(t-1, c), "price"],
                                             swd.loc[(t, c), "price_elasticity"])
                                       * pow(swd.loc[(t, c), "gdp"],
                                             swd.loc[(t, c), "gdp_elasticity"])
                                      )
    swd['comp_prop'] = swd.cons2 / swd.cons -1
    print(swd["comp_prop"].abs().max())
    swd.query("year >= 2019")


### Unique values of a multi index

Display the unique values of the two columns with a count of occurrences

    import seaborn
    penguins = seaborn.load_dataset("penguins")
    penguins.value_counts(["species", "island"])
    penguins[["species", "island"]].value_counts()

Lower level method using `unique()` on a multi index and returning a data frame

    penguins.set_index(["species", "island"]).index.unique().to_frame(False)


### Query index greater or smaller than

See also the query section for other ways to query data frames.

[SO answer](https://stackoverflow.com/a/18103894/2641825)

    df = pandas.DataFrame({'i':range(0,3), 
                           'j':['a','b','c'],
                           'x':range(22,25)})
    df = df.set_index(["i","j"])
    df.loc[(df.index.get_level_values('i') > 1)]

Using query instead

    df.query("i>1")


## PyArrow

- [Apache Arrow overview](https://arrow.apache.org/overview/) explains the
advantage of using a in memory columnar format to store data:

> "The Apache Arrow format allows computational routines and execution engines
> to maximize their efficiency when scanning and iterating large chunks of
> data. In particular, the contiguous columnar layout enables vectorization
> using the latest SIMD (Single Instruction, Multiple Data) operations included
> in modern processors." "[...] a standardized memory format facilitates reuse
> of libraries of algorithms, even across languages." "Arrow libraries for C
> (Glib), MATLAB, Python, R, and Ruby are built on top of the C++ library."

- [PyArrow interface with
pandas](https://arrow.apache.org/docs/python/pandas.html)

- [Read and write csv files](https://arrow.apache.org/docs/python/csv.html)


### Dask data frame

https://docs.dask.org/en/latest/dataframe.html


### Vaex

https://github.com/vaexio/vaex



## IO Input Output

### CSV

Read and write csv

#### Compressed csv

Write a compressed csv file as a gzip archive

    import pandas
    df = pandas.DataFrame({'x':range(0,3), 'y':['a','b','c']})
    df.to_csv("/tmp/df.csv.gz", index=False, compression="gzip")

Write a compressed csv file as a zip archive, using a dict with the option
"archive_name" (works only for the zip format)

    compression_opts = dict(method='zip', archive_name='out.csv')
    df.to_csv('/tmp/df.csv.zip', index=False, compression=compression_opts)

Read compressed csv files

    df1 = pandas.read_csv("/tmp/df.csv.gz")
    df.equals(df1)
    df2 = pandas.read_csv("/tmp/df.csv.zip")
    df.equals(df2)


#### From an API

Pandas data frames can be used to read CSV files from the [Comtrade
data API](https://comtrade.un.org/Data/). For example, using the default API
URL for all countries: 

    import pandas
    df1 = pandas.read_csv('http://comtrade.un.org/api/get?max=500&type=C&freq=A&px=HS&ps=2020&r=all&p=0&rg=all&cc=TOTAL&fmt=csv')

    df2 = pandas.read_csv('http://comtrade.un.org/api/get?max=500&type=C&freq=A&px=HS&ps=2020&r=all&p=0&rg=all&cc=01&fmt=csv',
                           # Force the id column to remain a character column,
                           # otherwise str "01" becomes an int 1.
                           dtype={'Commodity Code': str, 'bli': str})

Then use df.to_csv to write the data frame to a csv file

     df1.to_csv("/tmp/comtrade.csv")


### Feather

Load a sample data frame and save it to a feather file

    import pandas
    import seaborn
    iris = seaborn.load_dataset("iris")
    iris.to_feather("/tmp/iris.feather")

Load the data from the feather file

    iris2 = pandas.read_feather("/tmp/iris.feather")
    iris2.equals(iris)


### Parquet

Write to one file defaults to [snappy compression](https://en.wikipedia.org/wiki/Snappy_(compression)

    import pandas
    import seaborn
    iris = seaborn.load_dataset("iris")
    iris.to_parquet("/tmp/iris.parquet")

Read back the file

    iris3 = pandas.read_parquet("/tmp/iris.parquet")
    iris3.equals(iris)

You can also use gzip compression for a smaller file size (but slower read and write times)

    iris.to_parquet("/tmp/iris.parquet.gzip", compression='gzip') 


#### Partition column

Write to multiple files along a column used as partition variable

    iris.to_parquet("/tmp/iris",partition_cols="species") 

The partitioned dataset is saved under a sub directory for each unique value of
the partition variable. For example there is a sub directory for each species
in the `/tmp/iris` directory

    iris
    ├── species=setosa
    │   └── 1609afe5535d4e2b94e65f1892210269.parquet
    ├── species=versicolor
    │   └── 18dd7ae6d0794fd48dad37bf8950d813.parquet
    └── species=virginica
        └── e0a9786251f54eed9f16380c8f5c3db3.parquet

On can read a single file in memory

    virginica = pandas.read_parquet("/tmp/iris/species=virginica")

Note it has lost the species column

Read all files in memory

    iris4 = pandas.read_parquet("/tmp/iris")

Note the data frame is slightly different. Values are the same but the species
columns has become a categorical variable.

    iris4.equals(iris)
    # False
    iris4.species
    # ...
    # Name: species, Length: 150, dtype: category
    # Categories (3, object): ['setosa', 'versicolor', 'virginica']

Changing it back to a strings makes the 2 data frames equals again.

    iris4["species"] = iris4["species"].astype("str")
    iris4.equals(iris)
    # True

Read files with a filter. See `help(pyarrow.parquet.read_pandas)` for arguments
concerning the pyarrow engine.
    
    selection = [("species", "in", ["versicolor","virginica"])]
    iris5 = pandas.read_parquet("/tmp/iris", filters=selection)

In fact, the filter variable doesn't have to be a partition variable.

    selection = [("species", "in", ["versicolor","virginica"]), 
                 ("petal_width", ">", 2.4)]
    iris6 = pandas.read_parquet("/tmp/iris", filters=selection)

This works as well on the single file version

    iris7 = pandas.read_parquet("/tmp/iris.parquet", filters=selection)
    # Change column type for the comparison
    iris6["species"] = iris6["species"].astype("str")
    iris7.equals(iris6)

But if the query is only on the partition variable, read time can be increased
by a lot.


#### Experiment with the parquet format using filters and partition columns.

Note the detaset to perform these comparisons is
not made available here. I keep these for information purposes.

Compare a read of 2 countries with the read of the whole dataset

    # start_time = timeit.default_timer()
    # selection = [("reporter", "in", ["France","Germany"])]
    # ft_frde = pandas.read_parquet(la_fo_data_dir / "comtrade_forest_footprint.parquet",
    #                                             filters=selection)
    # print("Reading 2 countries took:",timeit.default_timer() - start_time)
    #
    # start_time = timeit.default_timer()
    # ft2 = pandas.read_parquet(la_fo_data_dir / "comtrade_forest_footprint.parquet")
    # print("Reading the whole dataset took:",timeit.default_timer() - start_time)
    #

Time comparison when the reporter is used as a partition column
It's about 10 times faster!

    # ft.to_parquet("/tmp/ft", partition_cols="reporter")
    # start_time = timeit.default_timer()
    # selection = [("reporter", "in", ["France","Germany"])]
    # ft_frde2 = pandas.read_parquet("/tmp/ft", filters=selection)
    # print("Reading 2 countries took:",timeit.default_timer() - start_time)
    #
    # # Save to a compressed csv file in biotrade_data
    # # file_path = la_fo_data_dir / "comtrade_forest_footprint.csv.gz"
    # # ft.to_csv(file_path, index=False, compression="gzip")

Also try the feather format. 

    # # Save to a feather file
    # ft.to_feather(la_fo_data_dir / "comtrade_forest_footprint.feather")
    #
    # # Read time of a feather file
    # start_time = timeit.default_timer()
    # ft_frde2 = pandas.read_feather(la_fo_data_dir / "comtrade_forest_footprint.feather")
    # print("Reading a feather file took:",timeit.default_timer() - start_time)


#### What is the difference between Apache Arrow and Apache Parquet?

[Apache Arrow FAQ](https://arrow.apache.org/faq/#what-about-the-feather-file-format)

> Parquet is a storage format designed for maximum space efficiency, using
> advanced compression and encoding techniques. It is ideal when wanting to
> minimize disk usage while storing gigabytes of data, or perhaps more. This
> efficiency comes at the cost of relatively expensive reading into memory, as
> Parquet data cannot be directly operated on but must be decoded in large
> chunks.

> Conversely, Arrow is an in-memory format meant for direct and efficient use
> for computational purposes. Arrow data is not compressed (or only lightly so,
> when using dictionary encoding) but laid out in natural format for the CPU,
> so that data can be accessed at arbitrary places at full speed.

> Therefore, Arrow and Parquet complement each other and are commonly used
> together in applications. Storing your data on disk using Parquet and reading
> it into memory in the Arrow format will allow you to make the most of your
> computing hardware."

> What about “Arrow files” then?

> Apache Arrow defines an inter-process communication (IPC) mechanism to
> transfer a collection of Arrow columnar arrays (called a “record batch”). It
> can be used synchronously between processes using the Arrow “stream format”,
> or asynchronously by first persisting data on storage using the Arrow “file
> format”.

> The Arrow IPC mechanism is based on the Arrow in-memory format, such that
> there is no translation necessary between the on-disk representation and the
> in-memory representation. Therefore, performing analytics on an Arrow IPC
> file can use memory-mapping, avoiding any deserialization cost and extra
> copies.

> Some things to keep in mind when comparing the Arrow IPC file format and the
> Parquet format:

>     Parquet is designed for long-term storage and archival purposes, meaning
>     if you write a file today, you can expect that any system that says they
>     can “read Parquet” will be able to read the file in 5 years or 10 years.
>     While the Arrow on-disk format is stable and will be readable by future
>     versions of the libraries, it does not prioritize the requirements of
>     long-term archival storage.

>     Reading Parquet files generally requires efficient yet relatively complex
>     decoding, while reading Arrow IPC files does not involve any decoding
>     because the on-disk representation is the same as the in-memory
>     representation.

>     Parquet files are often much smaller than Arrow IPC files because of the
>     columnar data compression strategies that Parquet uses. If your disk
>     storage or network is slow, Parquet may be a better choice even for
>     short-term storage or caching.


#### One large parquet file or many smaller files?

[Is it better to have one large parquet file or lots of smaller parquet
files?](https://stackoverflow.com/a/59535659/2641825)

> "Notice that Parquet files are internally split into row groups
> https://parquet.apache.org/documentation/latest/ So by making parquet files
> larger, row groups can still be the same if your baseline parquet files were
> not small/tiny. There is no huge direct penalty on processing, but opposite,
> there are more opportunities for readers to take advantage of perhaps larger/
> more optimal row groups if your parquet files were smaller/tiny for example
> as row groups can't span multiple parquet files."

> "Also larger parquet files don't limit parallelism of readers, as each
> parquet file can be broken up logically into multiple splits (consisting of
> one or more row groups)."

> "The only downside of larger parquet files is it takes more memory to create
> them. So you can watch out if you need to bump up Spark executors' memory."


#### See also

- It's not possible to [store multiple data frames of different widths with parquet](https://stackoverflow.com/questions/50456673/storing-multiple-dataframes-of-different-widths-with-parquet)

- [Control row groups size with pandas
  df.to_parquet](https://stackoverflow.com/questions/59972588/control-row-groups-with-pandas-dataframe-to-parquet)

- [Benchmark of feather, hdf, msgpack, parquet and
  pickle](https://towardsdatascience.com/the-best-format-to-save-pandas-data-414dca023e0d)


## Reshape

### From wide to long

[The Pandas user guide on reshaping](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html#reshaping-by-melt) 
gives several example using `melt` (easier to rename the  “variable” and “value” columns) 
or `stack` (designed to work together with MultiIndex objects).

Reshape using `melt`

    cheese = pandas.DataFrame(
          {
              "first": ["John", "Mary"],
              "last": ["Doe", "Bo"],
              "height": [5.5, 6.0],
              "weight": [130, 150],
          }
    )
    cheese
    cheese.melt(id_vars=["first", "last"], var_name="quantity")

Another example

    grading_matrix = pandas.DataFrame({"dbh":["d1", "d2", "d3"],
                                       "abies":["p","q","r"],
                                       "picea":["m","n","o"],
                                       "larix":["m","n","o"]})
    grading_long = grading_matrix.melt(id_vars="dbh", 
                                       var_name="species", 
                                       value_name="grading")

Reshape using the `wide_to_long` convenience function

    import numpy as np
    dft = pandas.DataFrame(
        {
            "A1970": {0: "a", 1: "b", 2: "c"},
            "A1980": {0: "d", 1: "e", 2: "f"},
            "B1970": {0: 2.5, 1: 1.2, 2: 0.7},
            "B1980": {0: 3.2, 1: 1.3, 2: 0.1},
            "X": dict(zip(range(3), np.random.randn(3))),
            "id":  {0: 0, 1: 1, 2: 2},
        }
    )
    dft
    pandas.wide_to_long(dft, stubnames=["A", "B"], i="id", j="year")


### From long to wide

Pivot from long to wide format using `pivot`:

    df = pandas.DataFrame({
        "lev1": [1, 1, 1, 2, 2, 2],
        "lev2": [1, 1, 2, 1, 1, 2],
        "lev3": [1, 2, 1, 2, 1, 2],
        "lev4": [1, 2, 3, 4, 5, 6],
        "values": [0, 1, 2, 3, 4, 5]})
    df_wide = df.pivot(index="lev1", columns=["lev2", "lev3"], values="values")
    df_wide

    # lev2    1         2
    # lev3    1    2    1    2
    # lev1
    # 1     0.0  1.0  2.0  NaN
    # 2     4.0  3.0  NaN  5.0

Rename the (sometimes confusing) axis names

    df_wide.rename_axis(columns=[None, None])

    #         1         2
    #         1    2    1    2
    # lev1
    # 1     0.0  1.0  2.0  NaN
    # 2     4.0  3.0  NaN  5.0


## Replace

[Python pandas equivalent for replace](https://stackoverflow.com/questions/12152716/python-pandas-equivalent-for-replace)

    import pandas
    s = pandas.Series(["ape", "monkey", "seagull"])
    s.replace(["ape", "monkey"], ["lion", "panda"])
    s.replace("a", "x", regex=True)
    `s.replace({"ape": "lion", "monkey": "panda"})`
    pandas.Series(["bla", "bla"]).replace("a","i",regex=True)


### Replace values where a condition is false

Replace values where the condition is false see `help(df.where)`

> "Where `cond` is True, keep the original value. Where False, replace with
> corresponding value from `other`."

    df = pandas.DataFrame({'a':range(0,3), 
                           'b':['p','q','r'], 
                           'c':['m','n','o']})
    df["b"].where(df["c"].isin(["n","o"]),"no")
    df.where(df["c"].isin(["n","o"]),"no")


### Fill Na values

Replace NA values by another value

    import pandas
    import numpy as np
    df = pandas.DataFrame([[np.nan, 2, np.nan, 0],
                      [3, 4, np.nan, 1],
                      [np.nan, np.nan, np.nan, 5],
                      [np.nan, 3, np.nan, 4]],
                     columns=list("ABCD"))
    # Replace all NaN elements with 0s.
    df.fillna(0)
    # Replace by 0 and column 2 and by 1 in column B
    df.fillna({"A":0, "B":1}, inplace=True)
    df


## Select with loc, iloc, query, isin and xs

There are many ways to select data in pandas (squarebraquets, loc, iloc, query, isin). In a first stage, during data preparation it's better to keep data out of the index. But in a second stage, when you are doing modelling, multi indexes become useful. And especially slicers to computes on part of the dataset - only some years, only some products, only some countries . For this, tools such as df.xs or https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.IndexSlice.html are needed.

 * blog [Select pandas data frame rows and columns using iloc and loc](https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/)


### loc

`.loc` is primarily label based, but may also be used with a boolean array.

I copied the examples below from the pandas **loc** documentation at:
[pandas.DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html)

Create an example data frame

    import pandas
    df = pandas.DataFrame([[1, 2], [4, 5], [7, 8]],
                          index=['cobra', 'viper', 'sidewinder'],
                          columns=['max_speed', 'shield'])

List of index labels

    In :  df.loc[['viper', 'sidewinder']]
    Out:
                        max_speed  shield
            viper               4       5
            sidewinder          7       8

Selecting a cell with 2 lists returns a data frame 

    df.loc[["viper"], ["shield"]]

Selecting cell with tuples (for multi indexes) or strings returns its value

    df.loc[("viper"), ("shield")]
    df.loc["viper", "shield"]

Note: in the case of a multi index, use tuples for index selection, see section below on multi index selection with loc.

Conditional that returns a boolean Series

    In :  df.loc[df['shield'] > 6]
    Out:
                         max_speed  shield
             sidewinder          7       8

Slice with labels for row and labels for columns.

    In :  df.loc['cobra':'viper', 'max_speed':'shield']
    Out:
                   max_speed  shield
            cobra          1       2
            viper          4       5

Set value for all items matching the list of labels

    In : df.loc[['viper', 'sidewinder'], ['shield']] = 50

    In : df
    Out:
                        max_speed  shield
            cobra               1       2
            viper               4      50
            sidewinder          7      50

Another example using integers for the index

    df2 = pandas.DataFrame([[1, 2], [4, 5], [7, 8]],
                          index=[7, 8, 9],
                          columns=['max_speed', 'shield'])

Slice with integer labels for rows. Note that **both the start and stop of the
slice** are included. Python slices behave differently.

    In :  df2.loc[8:9]
    Out:
          max_speed  shield
       8          4       5
       9          7       8


#### index.isin()

Using the same example as above, select rows that are not in ['cobra','viper'].
Based on a [SO answer use isin on the
index](https://stackoverflow.com/a/29140194/2641825):

    In : df.index.isin(['cobra','viper'])
    Out: array([ True,  True, False])

    In : df.loc[~df.index.isin(['cobra','viper'])]
    Out: 
                max_speed  shield
    sidewinder          7       8

Or assign the selector to reuse it:

    selector = df.index.isin(['cobra','viper'])
    df.loc[selector]
    df.loc[~selector]


#### Multiple conditions

    import pandas
    df = pandas.DataFrame([[1, 2], [4, 5], [7, 8]],
                          index=['cobra', 'viper', 'sidewinder'],
                          columns=['max_speed', 'shield'])
    df.loc[(df["max_speed"] > 1) & (df["shield"] < 7)]
    df.query("max_speed > 1 & shield < 7")


#### Multi-index selection with loc

Create a panel data set with a multi index in years and countries

    import pandas
    import numpy as np
    df = pandas.DataFrame(
        {"country": ['Algeria', 'Angola', 'Benin', 'Botswana', 'Burkina Faso'] * 2,
         "year": np.repeat(np.array([2020,2021]), 5),
         "value":  np.random.randint(0,1e3,10)
         })
    df = df.set_index(["year", "country"])
          
See also the course material [Pandas for panel
data](https://python.quantecon.org/pandas_panel.html).

Sample data copied from `help(df.loc)`:

    tuples = [
       ('cobra', 'mark i'), ('cobra', 'mark ii'),
       ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),
       ('viper', 'mark ii'), ('viper', 'mark iii')
    ]
    index = pandas.MultiIndex.from_tuples(tuples)
    values = [[12, 2], [0, 4], [10, 20],
            [1, 4], [7, 1], [16, 36]]
    df = pandas.DataFrame(values, columns=['max_speed', 'shield'], index=index)
  
Single label. Note this returns a DataFrame with a single index.
  
    df.loc['cobra']
  
Single index tuple. Note this returns a Series.
  
    df.loc[('cobra', 'mark ii')]
    df.loc[(:,'mark ii')]

Single tuple. Note using ``[[]]`` returns a DataFrame.
  
    df.loc[[('cobra', 'mark ii')]]

Single label for row and column. Similar to passing in a tuple, this
returns a Series.
  
    df.loc['cobra', 'mark i']

Slice from index tuple to single label

    df.loc[('cobra', 'mark i'):'viper']

Slice from index tuple to index tuple

    df.loc[('cobra', 'mark i'):('viper', 'mark ii')]

Invert a selection on the second index

    df.loc[~df.index.isin(["mark i"], level=1)]


#### Multi-index slicers to select the second index element

[Using
slicers](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#using-slicers)

> "You can use pandas.IndexSlice to facilitate a more natural syntax using `:`,
> rather than using `slice(None)`."

Other example from [a SO
question](https://stackoverflow.com/questions/56340906/ilocing-one-level-of-a-multiindex)

    import pandas
    df = pandas.DataFrame(index = pandas.MultiIndex.from_product([range(2010,2020),
                          ['mike', 'matt', 'dave', 'frank', 'larry'], ]))
    df['x']=0
    df.index.names=['year', 'people']
    df.loc[2010]
    df.loc[(2010,"mike")]

These two `df.loc[2010]`,  `df.loc[(2010,"mike")]` work, but 

    df.loc["mike"]

Returns a `KeyError: 'mike'`. To select on the second index level only, you
need a multi index slicer. 

    idx = pandas.IndexSlice
    df.loc[idx[:, "mike"],:]

You can also use `df.xs`

    df.xs("mike", level=1)
    df.xs("mike", level="people")

[Using loc on just the second index in multi
index](https://stackoverflow.com/a/50414126/2641825)
Other example using the same data frame as the previous section. 

    idx = pandas.IndexSlice
    df.loc[idx[:, "mark i"],:]
    df.xs("mark i", level=1)


### iloc 

[.iloc](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-integer)
is primarily integer position based (from 0 to length -1 of the axis), but may also be used with a boolean array.

Create a sample data frame:

    In : example = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},
                    {'a': 100, 'b': 200, 'c': 300, 'd': 400},
                    {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]
          df = pandas.DataFrame(example)

    In : df
    Out: 
                a     b     c     d
          0     1     2     3     4
          1   100   200   300   400
          2  1000  2000  3000  4000

Index with a slice object. Note that it doesn't include the upper bound.

    In :  df.iloc[0:2]
    Out: 
              a    b    c    d
         0    1    2    3    4
         1  100  200  300  400

With lists of integers.

    In : df.iloc[[0, 2], [1, 3]]
    Out: 
                b     d
          0     2     4
          2  2000  4000

With slice objects.

    In : df.iloc[1:3, 0:3]
    Out: 
                a     b     c
          1   100   200   300
          2  1000  2000  3000

With a boolean array whose length matches the columns.

    In : df.iloc[:, [True, False, True, False]]
    Out: 
                a     c
          0     1     3
          1   100   300
          2  1000  3000


### Query

Query the columns of a Data Frame with a boolean expression.

    df = pandas.DataFrame({'A': range(1, 6),
                           'B': range(10, 0, -2),
                           'C': range(10, 5, -1)})
    df.query("A > B")

    A  B  C
    5  2  6

Two queries

    df.query("A < B and B < C")
    df.query("A < B or B < C")

Query using a variable

    limit = 3
    df.query("A > @limit")

    A  B  C
    4  4  7
    5  2  6

Query for a variable in a list

    df.query("A in [3,6]")

Query for a variable not in a list

    df.query("A not in [3,6]")


#### str.contains and str.startswith

str.contains and str.startswith do not work with the default numexpre engine,
you need to set `engine="python"` as explained in [this
answer](https://stackoverflow.com/a/51375487/2641825).

Example use on a table of product codes, query products description that
contain "oak" but not "cloak" and query sawnwood products starting with "4407 ":

    comtrade.products.hs.query("product_description.str.contains('oak') and not product_description.str.contains('cloak')", engine="python")
    comtrade.products.hs.query("product_code.str.startswith('4407')", engine="python")


### isin

[Use alist of values to select rows](https://stackoverflow.com/questions/12096252/use-a-list-of-values-to-select-rows-from-a-pandas-dataframe)

    df = pandas.DataFrame({'A': [5,6,3,4], 'B': [1,2,3,5]})
    df[df['A'].isin([3, 6])]
    df.loc[df['A'].isin([3, 6])]
    df.query("A in [3,6]")


### xs cross sections

The `key` and `level` arguments specify which part of the multilevel index
should be used. Create a sample data frame, copied from ` help(df.xs)`:

    d = {'num_legs': [4, 4, 2, 2],
         'num_wings': [0, 0, 2, 2],
         'class': ['mammal', 'mammal', 'mammal', 'bird'],
         'animal': ['cat', 'dog', 'bat', 'penguin'],
         'locomotion': ['walks', 'walks', 'flies', 'walks']}
    df = pandas.DataFrame(data=d)
    df = df.set_index(['class', 'animal', 'locomotion'])
    print(df)

Select with a key following the order in which levels appear in the index:

    df.xs('mammal')
    df.xs(('mammal', 'dog'))

Select with a key and specify the levels:

    df.xs(key='cat', level=1)
    df.xs(key=('bird', 'walks'),
          level=[0, 'locomotion'])

Pandas
[DataFrame.xs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.xs.html)
"cannot be used to set values."


### Determine whether a column contains a particular value

[How to determine whether a pandas column contains a particular
varlue](https://stackoverflow.com/questions/21319929/how-to-determine-whether-a-pandas-column-contains-a-particular-value) 

> In of a Series checks whether the value is in the index:

    In : s = pd.Series(list('abc'))
    In : 1 in s
    Out: True
    In : 'a' in s
    Out: False

> One option is to see if it's in unique values:

    In : 'a' in s.unique()
    Out: True


## Sort or arrange values

Sort iris by descending order of species and ascending order of petal width

    iris.sort_values(by=["species", "petal_width"], ascending=[False,True])


## String operations in pandas

See also string operations in python in another section.

String operations in pandas use vectorized string methods of the 
class StringMethods(pandas.core.base.NoNewAttributesMixin).

    df = pandas.DataFrame({'a':['a','b','c']})
    help(df.a.str)

Concatenate all values in a character vector:

    df['a'].str.cat()


Extract the first 2 or last 2 characters

    df = pandas.DataFrame({'a':['bla','bli','quoi?']})
    df["a"].str[:2]
    df["a"].str[-2:]


### Search and replace

Search one element or another in a character vector:

    df = pandas.DataFrame({'a':['bla','ble','bli']})
    df[df['a'].str.contains('a|i')]

Replace elements in a character vector:

    df['a'].replace('a|i','b',regex=True)


### Separate a columns in two based on a split pattern

The "too many values to unpack" error can also be returned by the `str.split` method of pandas data frames.

For example splitting a character vector on the "`,`" pattern:

    import pandas
    df = pandas.DataFrame({"x": ["a", "a, b", "a,b,c"]})
    df.x.str.split(",")

    # 0          [a]
    # 1      [a,  b]
    # 2    [a, b, c]

    df.x.str.split(",", n=1)

    # 0         [a]
    # 1     [a,  b]
    # 2    [a, b,c]

    df.x.str.split(",", expand=True)

    #    0     1     2
    # 0  a  None  None
    # 1  a     b  None
    # 2  a     b     c

    df.x.str.split(",", n=1, expand=True)

    #    0     1
    # 0  a  None
    # 1  a     b
    # 2  a   b,c

The following version works only if each row has exactly 2 splits. It **fails**
with the **error** “too many values to unpack (expected 2)” in this example:

    df["y"], df["z"] = df.x.str.split(",", n=1)

The last version with both `n=1` and `expand=True` is the one to use for
multiple vector assignment. It is equivalent to
[tidyr::separate](https://tidyr.tidyverse.org/reference/separate.html) in R.

    df[["y", "z"]] = df.x.str.split(",", n=1, expand=True)
    df

    #        x  y     z
    # 0      a  a  None
    # 1   a, b  a     b
    # 2  a,b,c  a   b,c

According to the [documentation of
pandas.Series.str.split](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html)
If n > 0 and

> "If for a certain row the number of found splits < n, append None for padding
> up to n if expand=True."


### Extract columns based on a pattern

Place product patterns in a capture group for extraction

    df = pandas.DataFrame({"x": ["am", "an", "o", "bm", "bn", "cm"]})
    product_pattern = "a|b|c"
    df[["product", "element"]] = df.x.str.extract(f"({product_pattern})?(.*)")
    df


## Difference between 2 data frames

* [Find difference between two data frames](https://stackoverflow.com/questions/48647534/python-pandas-find-difference-between-two-data-frames)
* [Diff of 2 data frames](https://stackoverflow.com/questions/36891977/pandas-diff-of-two-dataframes/36893773)

Two methods
Using `merge`:

    merged = df1.merge(df2, indicator=True, how='outer')
    merged[merged['_merge'] == 'right_only']

Using `drop_duplicates`

    newdf=pd.concat[df1,df2].drop_duplicates(keep=False)


## Duplicated values

Warn in case the variable x is duplicated

    import pandas
    df = pandas.DataFrame({"x": ["a", "b", "c", "a"], "y": range(4)})
    dup_x = df["x"].duplicated(keep=False)
    if any(dup_x):
        msg = "x values are not unique. "
        msg += "The following duplicates are present:\n"
        msg += f"{df.loc[dup_x]}"
        raise ValueError(msg)

Drop duplicates

    df["x"].drop_duplicates()
    df["x"].drop_duplicates(keep=False)
    df["x"].drop_duplicates(keep="last")


## Where and mask

`where` replaces values that do not fit the condition and `mask` replaces
values that fit the condition.

    s = pandas.Series(range(5))
    s.where(s > 1, 10)
    s.mask(s > 1, 10)

On a data frame

    import pandas
    import numpy as np
    df1 = pandas.DataFrame({'x':[0,np.nan, np.nan], 
                            'y':['a',np.nan,'c']})
    df2 = pandas.DataFrame({'x':[10, 11, 12], 
                            'y':['x','y', np.nan]})
    df1.mask(df1.isna(), df2)
    df1.where(df1.isna(), df2)


## Xarray

[DataArray](https://docs.xarray.dev/en/stable/user-guide/data-structures.html#dataarray)

    > "xarray.DataArray is xarray’s implementation of a labeled, multi-dimensional
    > array. It has several key properties:
    > 
    > - values: a numpy.ndarray holding the array’s values
    > 
    > - dims: dimension names for each axis (e.g., ('x', 'y', 'z'))
    > 
    > - coords: a dict-like container of arrays (coordinates) that label each point
    >   (e.g., 1-dimensional arrays of numbers, datetime objects or strings)
    > 
    > - attrs: dict to hold arbitrary metadata (attributes)
    > 
    > Xarray uses dims and coords to enable its core metadata aware operations.
    > Dimensions provide names that xarray uses instead of the axis argument found in
    > many numpy functions. Coordinates enable fast label based indexing and
    > alignment, building on the functionality of the index found on a pandas
    > DataFrame or Series."


[Dataset](https://docs.xarray.dev/en/stable/user-guide/data-structures.html#dataset)

    > "xarray.Dataset is xarray’s multi-dimensional equivalent of a DataFrame. It is a
    > dict-like container of labeled arrays (DataArray objects) with aligned
    > dimensions. It is designed as an in-memory representation of the data model
    > from the netCDF file format.
    > 
    > In addition to the dict-like interface of the dataset itself, which can be used
    > to access any variable in a dataset, datasets have four key properties:
    > 
    > dims: a dictionary mapping from dimension names to the fixed length of each
    > dimension (e.g., {'x': 6, 'y': 6, 'time': 8})
    > 
    > data_vars: a dict-like container of DataArrays corresponding to variables
    > 
    > coords: another dict-like container of DataArrays intended to label points
    > used in data_vars (e.g., arrays of numbers, datetime objects or strings)
    > attrs: dict to hold arbitrary metadata
    > 
    > The distinction between whether a variable falls in data or coordinates
    > (borrowed from CF conventions) is mostly semantic, and you can probably get
    > away with ignoring it if you like: dictionary like access on a dataset will
    > supply variables found in either category. However, xarray does make use of the
    > distinction for indexing and computations. Coordinates indicate
    > constant/fixed/independent quantities, unlike the varying/measured/dependent
    > quantities that belong in data."


[converting between datasets and
arrays](https://docs.xarray.dev/en/stable/user-guide/reshaping.html#converting-between-datasets-and-arrays)

    > "This method broadcasts all data variables in the dataset against each other,
    > then concatenates them along a new dimension into a new array while
    > preserving coordinates."


### Create xarray from pandas

Round trip from pandas to xarray and back from the [xarray user guide page on
pandas](https://docs.xarray.dev/en/stable/user-guide/pandas.html).

    import xarray
    import numpy as np
    ds = xarray.Dataset(
        {"foo": (("x", "y"), np.random.randn(2, 3))},
        coords={
            "x": [10, 20],
            "y": ["a", "b", "c"],
            "along_x": ("x", np.random.randn(2)),
            "scalar": 123,
        },
    )
    ds

Convert the xarray dataset to a pandas data frame

    df = ds.to_dataframe()
    df

Convert the data frame back to a dataset

    xarray.Dataset.from_dataframe(df)

> "Notice that that dimensions of variables in the Dataset have now expanded
> after the round-trip conversion to a DataFrame. This is because every object
> in a DataFrame must have the same indices, so we need to broadcast the data
> of each array to the full size of the new MultiIndex. Likewise, all the
> coordinates (other than indexes) ended up as variables, because pandas does
> not distinguish non-index coordinates."

You can also use 

    xarray.DataArray(df)


### Panel data

[transitioning from pandas panel to xarray](https://docs.xarray.dev/en/stable/user-guide/pandas.html#transitioning-from-pandas-panel-to-xarray)

> "As discussed elsewhere in the docs, there are two primary data structures
> in xarray: DataArray and Dataset. You can imagine a DataArray as a
> n-dimensional pandas Series (i.e. a single typed array), and a Dataset as the
> DataFrame equivalent (i.e. a dict of aligned DataArray objects).
> So you can represent a Panel, in two ways:
> 
>    As a 3-dimensional DataArray,
> 
>    Or as a Dataset containing a number of 2-dimensional DataArray objects.


### IO

- [How to read netcdf files in chunks with Xarray and Dask](https://stackoverflow.com/questions/35422862/speeding-up-reading-of-very-large-netcdf-file-in-python)


# Paths

## Copy or move files

Write to a text file using a [context
manager](https://book.pythontips.com/en/latest/context_managers.html), then
copy the file somewhere else. 

    with open("/tmp/bli.md", "w") as f:
        f.write('Hola!')

Copy a file

    import shutil
    shutil.copy("/tmp/bli.md", "/tmp/bla.md")

Move a file

    shutil.move("/tmp/bli.md", "/tmp/bla.md")


## Pathlib

Pathlib is an object oriented path API for python as explained in [PEP
428](https://www.python.org/dev/peps/pep-0428/#why-an-object-oriented-api)

Instead of 

    import os
    os.path.join('~','downloads')

You can use:

    from pathlib import Path
    Path('~') / 'downloads'

Data located in the home folder

     data_dir = Path.home() / "repos/data/"


### Dir name or parent directory

[SO question](https://stackoverflow.com/a/35490226/2641825) that illustrate different levels of parents 

    import os
    import pathlib
    p = pathlib.Path('/path/to/my/file')
    p.parents[0]
    p.parents[1]
    p.parent

> "Note that os.path.dirname and pathlib treat paths with a trailing slash
> differently. The pathlib parent of some/path/ is some: While os.path.dirname
> on some/path/ returns some/path":

    pathlib.Path('some/path/').parent
    os.path.dirname('some/path/')


### List all files in a directory

If `p` is a pathlib object you can list file names as such: 

    [x.name for x in p.glob('**/*.csv')]


### Check if a directory is empty

Check if a directory is empty using pathlib

    import pathlib
    p1 = pathlib.Path("/tmp/")
    p2 = pathlib.Path("/tmp/thisisempty/")
    p2.mkdir()
    any(p1.iterdir()) # returns True
    any(p2.iterdir()) # returns False


## Temporary directories and files

Docs.python.org [tempfile
examples](https://docs.python.org/3/library/tempfile.html#examples) using a
context manager

    import tempfile
    # create a temporary directory using the context manager
    with tempfile.TemporaryDirectory() as tmpdirname:
        print('created temporary directory', tmpdirname)
    # directory and contents have been removed

Using `pathlib` to facilitate path manipulation on top of `tempfile`
makes it possible to create new paths using the `/` path operator of pathlib:

    import tempfile
    from pathlib import Path
    with tempfile.TemporaryDirectory() as tmpdirname:
        temp_dir = Path(tmpdirname)
        print(temp_dir, temp_dir.exists())
        file_name = temp_dir / "test.txt"
        file_name.write_text("bla bla bla")
        print(file_name, "contains", file_name.open().read())

Outside the context manager, files have been destroyed

    print(temp_dir, temp_dir.exists())
    # /tmp/tmp81iox6s2 False
    print(file_name, file_name.exists())
    # /tmp/tmp81iox6s2/test.txt False


# Plot

[Python plotting for exploratory analysis](https://pythonplot.com/) is a great
gallery of plot examples, each example is written in 5 different plotting
libraries: pandas, plotnine, plotly, altair and R ggplot2. There is also one
seaborn example.


## Matplotlib

All matplotlib examples require the following imports:

    from matplotlib import pyplot as plt
    plt.style.use('seaborn-whitegrid')
    import numpy as np

Simple line plot changing the figure size and the axes limit with pyplot

    plt.rcParams['figure.figsize'] = [10, 10]
    fig = plt.figure()
    ax = plt.axes()
    x = np.linspace(-1.5, 1.5, 1000)
    ax.plot(x, 1-3*x)
    ax.set_xlim(-6, 6)
    ax.set_ylim(-6, 6)

Scatter plot, using a colour variable and the 'jet' colour map.

    Y = np.array([1,-1,-1, 1])
    X = np.array([
            [-1, -1],
            [ 1, -1],
            [-1,  1],
            [ 1,  1]])
    fig = plt.figure()
    ax = plt.axes()
    ax.scatter(X[:,0], X[:,1],c=Y, cmap='jet')

Use another [colour map](https://matplotlib.org/examples/color/colormaps_reference.html)

    ax.scatter(X[:,0], X[:,1],c=Y, cmap='Spectral')

Plot the probability density function of the 
[normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). 

$$f(x)=\frac{1}{\sigma{\sqrt {2\pi }}}e^{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}}$$

With various sigma and mu values displayed in the legend.

    fig = plt.figure()
    ax = plt.axes()
    x = np.linspace(-5, 5, 1000)
    def pdensitynormal(x,sigma_squared,mu):
        sigma = np.sqrt(sigma_squared)
        return 1/(sigma*np.sqrt(2*np.math.pi))*np.exp(-1/2*((x-mu)/sigma)**2)
    ax.plot(x, pdensitynormal(x,0.2,0), label="$\sigma^2=0.2, \mu=0$")
    ax.plot(x, pdensitynormal(x,1,0), label="$\sigma^2=1, \mu=0$")
    ax.plot(x, pdensitynormal(x,5,0), label="$\sigma^2=5, \mu=0$")
    ax.plot(x, pdensitynormal(x,0.5,-2), label="$\sigma^2=0.5, \mu=-2$")
    ax.legend(loc="upper right")
    plt.show()

3D line, contour plot and scatter plot

    from mpl_toolkits import mplot3d # Required for 3d plots
    fig = plt.figure()
    ax = plt.axes(projection='3d')
    # Data for a three-dimensional line
    xline = np.linspace(-10, 10, 1000)
    yline = np.linspace(-10, 10, 1000)
    # Just a line
    zline = xline**2 + yline**2
    ax.plot3D(xline, yline, zline, 'gray')
    # A mesh grid
    X, Y = np.meshgrid(xline, yline)
    Z = X**2 + Y**2
    ax.contour3D(X, Y, Z, 50, cmap='binary')
    # Scatter points
    ax.scatter(1,2,3)

See how the `np.meshgridi` objects interact with each other.
Note this nested loop is not the optimal way to compute.
Better to use X**2 + Y**2 directly as above.

    for i in range(Z.shape[0]):
        for j in range(Z.shape[1]):
            vector = np.array([X[i,j],Y[i,j]])
                Z[i,j] = np.linalg.norm(vector)**2
    fig = plt.figure()
    ax = plt.axes(projection='3d')
    ax.contour3D(X, Y, Z, 50, cmap='binary')


### Pandas plots are matplotlib AxesSubplot objects

#### Show pandas plots in ipython

Create some data and change the xticks labels

    import pandas
    import matplotlib.pyplot as plt
    df = pandas.DataFrame({'x':range(0,30), 'y':range(10,40)})
    df.set_index('x', inplace=True)
    plot = df.plot(title='Two ranges')
    type(plot)
    #help(plot)
    plot.set_xticks(range(0,31,10), minor=False)
    plt.show()

Save the figure to a pdf file:

    plot.get_figure().savefig('/tmp/output.pdf', format='pdf')


#### Pandas plots side by side

Using the same df as above show 2 plost side by side based on this [SO answer](https://stackoverflow.com/a/68008119/2641825)

    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))
    df.plot(title='Two ranges', ax=ax1)
    df.plot(title='Two ranges', ax=ax2)
    plt.show()


## Plotly 

The advantage of plotly is that it provides dynamic visualisation inside web
pages, such as the possibility to zoom in a graph. It's the open source
component of a commercial project called Dash entreprise.

For example [this
notebook](https://colab.research.google.com/github/bytehub-ai/blog-examples/blob/master/temperature_forecast_example.ipynb#scrollTo=7yJZA1CZSSx4)
on machine learning used to enhance the localisation of weather forecasts. Seen
on this blog post [What does machine learning have to do with
weather](https://medium.com/bytehub-ai/what-does-machine-learning-have-to-do-with-weather-94f3ac625ad3).


## Plotnine

Grammar of graphics for python
https://github.com/has2k1/plotnine


### Figure size

Change the [plotnine figure
size](https://plotnine.readthedocs.io/en/stable/generated/plotnine.options.figure_size.html)

    import plotnine
    plotnine.options.figure_size = (12, 8)


### Facet grid

Create a facet grid plot

    from plotnine import ggplot, aes, geom_line, facet_grid, labs


## Seaborn

All Seaborn examples below require the following imports and datasets:

    import seaborn
    iris = seaborn.load_dataset("iris")
    tips = seaborn.load_dataset("tips") 
    from matplotlib import pyplot as plt


### Scatter plot

Create a scatter plot with a title and axis labels

    p = seaborn.scatterplot('petal_length','petal_width','species',data=iris)
    p.set(xlabel = "Petal Length", ylabel = "Petal Width", title = "Flower sizes")
    plt.show()


### Line plot

Create a line plot with a title and axis labels.

    import numpy as np
    df = pandas.DataFrame({'value':np.random.random(100), 
                           'year':range(1901,2001)})
    p = seaborn.lineplot(x="year", y="value", data=df)
    p.set(ylabel = "Random variation", title = "Title here")
    plt.show()
    

### Axes limit

Set limits on the one axis in a Seaborn plot:

    p = seaborn.scatterplot('petal_length','petal_width','species',data=iris)
    p.set(ylim=(-2,None))

In Seaborn facet grid.
[How to set xlim and ylim in seaborn facet grid](https://stackoverflow.com/a/25213614/2641825)

    g = seaborn.FacetGrid(tips, col="time", row="smoker")
    g = g.map(plt.hist, "total_bill")
    g.set(ylim=(0, None)) 


### Grid plot


#### Grid scatter plot

Draw a scatter plot for each iris species

    g = seaborn.FacetGrid(iris, col="species", height=6, width=2)
    iris['species'] = iris['species'].astype('category')
    g.map(seaborn.scatterplot,'petal_length','petal_width','species')
    plt.show()

Notice that if you don't change the color to a categorical variable, it will not vary across the species. 
I reported [this issue](https://github.com/mwaskom/seaborn/issues/2028).


#### Grid Bar plot 

Draw a facet bar plot [from SO](https://stackoverflow.com/a/62225095/2641825)
for each combination of size and smoker/non smoker 

    import seaborn as sns
    import matplotlib.pyplot as plt
    sns.set()
    tips=sns.load_dataset("tips")
    g = sns.FacetGrid(tips, col = 'size',  row = 'smoker', hue = 'day')
    g = (g.map(sns.barplot, 'time', 'total_bill', ci = None).add_legend())
    plt.show()


#### Grid line plot


>   This function provides access to several different axes-level functions
>   that show the relationship between two variables with semantic mappings
>   of subsets. The ``kind`` parameter selects the underlying axes-level
>   function to use:

>   - :func:`scatterplot` (with ``kind="scatter"``; the default)
>   - :func:`lineplot` (with ``kind="line"``)


https://seaborn.pydata.org/examples/faceted_lineplot.html

    import seaborn as sns
    sns.set_theme(style="ticks")

    dots = sns.load_dataset("dots")

    # Define the palette as a list to specify exact values
    palette = sns.color_palette("rocket_r")

    # Plot the lines on two facets
    g = sns.relplot(
        data=dots,
        x="time", y="firing_rate",
        hue="coherence", size="choice", col="align",
        kind="line", size_order=["T1", "T2"], palette=palette,
        height=5, aspect=.75, facet_kws=dict(sharex=False),
    )
    g.fig.suptitle("Dots example")
    # Add a title and adjust the position so the title doesn't overwrite facets
    g.set_ylabels("Y label")
    plt.subplots_adjust(top=0.9)


### Title

Use `set_title` to add a title:

    (seaborn
     .scatterplot(x="total_bill", y="tip", data=tips)
     .set_title('Progression of tips along the bill amount')
    )

Set a common title for grid plots

    g = seaborn.FacetGrid(tips, col="time", row="smoker")
    g = g.map(plt.hist, "total_bill")
    g.fig.suptitle('I don't smoke and I don't tip.')

In case the title is overwritten on the subplots, you might need to use 
[fig.subplot_adjust()](https://stackoverflow.com/a/28650623/2641825) 
as such:

    g.fig.subplots_adjust(top=.95)


### Figure size

Resize a scatter plot

    p = seaborn.scatterplot('petal_length','petal_width','species',data=iris)
    p.figure.set_figwidth(15)


#### Larger grid plots

`set_figwidth` and `set_figheight` work well to resize a grid object in its
entirety. 

    g = seaborn.FacetGrid(tips, col="time", row="smoker")
    g = g.map(plt.hist, "total_bill")
    g.fig.set_figwidth(10)
    g.fig.set_figheight(10) 

Mentioned as a comment under [this answer](https://stackoverflow.com/a/56970556/2641825)

To change the height and aspect ration of individual grid cells, you can use
the `height` and `aspect` arguments of the FacetGrid call as such:

    import seaborn 
    from matplotlib import pyplot
    seaborn.set()
    iris = seaborn.load_dataset("iris")
    # Change height and aspect ratio
    g = seaborn.FacetGrid(iris, col="species", height=8, aspect=0.3)
    iris['species'] = iris['species'].astype('category')
    g.map(seaborn.scatterplot,'petal_length','petal_width','species')
    pyplot.show()

`help(seaborn.FacetGrid)`

> ``aspect * height`` gives the width of each facet in inches.


### Scatter Plot

Create a scatter plot

    import seaborn
    import matplotlib.pyplot as plt
    tips = seaborn.load_dataset("tips")
    seaborn.scatterplot(x="total_bill", y="tip", data=tips)
    plt.show()

Group by another variable and show the groups with different colors:
    
    seaborn(x="total_bill", y="tip", hue="time", data=tips)


### Sample data

Show all Seaborn sample datasets

    for dataset in seaborn.get_dataset_names():
        print(dataset)
        print(seaborn.load_dataset(dataset).head())


## Squarify treemaps

Plot a [tree map from the python graph
gallery](https://www.python-graph-gallery.com/treemap/)

    import matplotlib.pyplot as plt
    import squarify    # pip install squarify (algorithm for treemap)
    import pandas
    df = pandas.DataFrame({'nb_people':[8,3,4,2], 'group':["group A", "group B", "group C", "group D"] })
    squarify.plot(sizes=df['nb_people'], label=df['group'], alpha=.8 )
    plt.axis('off')
    plt.show()


## Vega

The tool tip feature is nice in an interactive notebook.

- Ipython vega for Jupyter notebooks https://github.com/vega/ipyvega
- Vega gallery https://vega.github.io/vega/examples/

Vega Lite

- Vega lite gallery https://vega.github.io/vega-lite-v1/examples/
- Vega lite documentation on
  [tooltips](https://vega.github.io/vega-lite/docs/tooltip.html)

    from vega import VegaLite
    import pandas
    df = pandas.read_json('cars.json')
    VegaLite({
          "data": {"url": "data/cars.json"},
          "mark": {"type": "point", "tooltip": true},
          "encoding": {
            "x": {"field": "Horsepower","type": "quantitative"},
            "y": {"field": "Miles_per_Gallon","type": "quantitative"}
          }
        }, df)


# Print

[How to print coloured text at the
terminal?](https://stackoverflow.com/a/21786287/2641825)

> "Print a string that starts a color/style, then the string, and then end the
> color/style change with '\x1b[0m'."

For example

    print(1000 * ("\x1b[1;32;44m" + "Winter" + "\x1b[0m" + ", " +
                  "\x1b[1;32;42m" + "Spring" + "\x1b[0m" + ", " +
                  "\x1b[1;35;41m" + "Summer" + "\x1b[0m" + ", " +
                  "\x1b[1;35;45m" + "Autumn" + "\x1b[0m" + ", "))


# Profiling and measuring time

[How can I time a code segment for testing performance with Pythons timeit?](https://stackoverflow.com/questions/2866380/how-can-i-time-a-code-segment-for-testing-performance-with-pythons-timeit)

Time a function:

    import timeit
    import time
    def wait():
        time.sleep(1)
    timeit.timeit(wait, number=3)

> "If you are profiling your code and can use IPython, it has the magic
> function `%timeit`. `%%timeit` operates on cells." 

    %timeit wait()

[Time a code block](https://stackoverflow.com/a/15707125/2641825):

    import timeit
    start_time = timeit.default_timer()
    # code you want to evaluate
    elapsed = timeit.default_timer() - start_time


# R and python

See also the [R page](R.html) for more details on R.

Reddit [python vs
R](https://old.reddit.com/r/datascience/comments/67p72w/python_vs_r/)

> "R is for analysis. Python is for production. If you want to do analysis
> only, use R. If you want to do production only, use Python. If you want to do
> analysis then production, use Python for both. If you aren't planning to do
> production then it's not worth doing, (unless you're an academic).
> Conclusion: Use python."


## History

The central objects in R are vectors, matrices and data frames, that is why I
mostly compare R to the python packages numpy and pandas. R was created almost
20 years before numpy and more than 40 years before pandas.

[R_(programming_language)](https://en.wikipedia.org/wiki/R_(programming_language))

> "R is an implementation of the S programming language combined with lexical
> scoping semantics, inspired by Scheme. S was created by John Chambers in
> 1976 while at Bell Labs. A commercial version of S was offered as S-PLUS
> starting in 1988. " 

[NumPy history](https://en.wikipedia.org/wiki/NumPy#History)

> "In 1995 the special interest group (SIG) matrix-sig was founded with the aim
> of defining an array computing package; among its members was Python designer
> and maintainer Guido van Rossum, who extended Python's syntax (in particular
> the indexing syntax) to make array computing easier. [...] An implementation
> of a matrix package was completed by Jim Fulton, then generalized by Jim
> Hugunin and called Numeric. [...] new package called Numarray was written as
> a more flexible replacement for Numeric. Like Numeric, it too is now
> deprecated. [...] In early 2005, NumPy developer Travis Oliphant wanted to
> unify the community around a single array package and ported Numarray's
> features to Numeric, releasing the result as NumPy 1.0 in 2006."

[Pandas_(software)](https://en.wikipedia.org/wiki/Pandas_(software)#History)

> "Developer Wes McKinney started working on pandas in 2008 while at AQR
> Capital Management out of the need for a high performance, flexible tool to
> perform quantitative analysis on financial data. Before leaving AQR he was
> able to convince management to allow him to open source the library." 

[Migrating from R to python](https://jrvcomputing.wordpress.com/2016/11/14/migrating-from-r-to-python/)

> "Python is a full fledge programming language but it is missing statistical
> and plotting libraries. Vectors are an after thought in python most
> functionality can be reproduced using operator overloading, but some
> functionality looks clumsy."


## Numpy and R

R session showing a division by zero returning an infinite value.

    > 1/0
    [1] Inf

Python session showing a division by zero error for normal integer division and
the same operation on a numpy array returning an infinite value with a warning.


    In [1]: 1/0
    ---------------------------------------------------------------------------
    ZeroDivisionError                         Traceback (most recent call last)
    <ipython-input-1-9e1622b385b6> in <module>
    ----> 1 1/0

    ZeroDivisionError: division by zero

    In [2]: import numpy as np

    In [3]: np.array([1]) / 0
    /home/paul/.local/bin/ipython:1: RuntimeWarning: divide by zero encountered in true_divide
      #!/usr/bin/python3
    Out[3]: array([inf])


## Pandas comparison with R

R data frame to be used for examples:

    df = data.frame(x = 1:3, y = c('a','b','c'), stringsAsFactors = FALSE)

Pandas data frame to be used for examples:

    import pandas
    df = pandas.DataFrame({'x' : [1,2,3], 'y' : ['a','b','c']})

| Base R                      | python or pandas              | SO questions                                                   |
| --------------------------- | ----------------------------- | --------------------------------------------------             |
| `df[df$y %in% c('a','b'),]` | `df[df['y'].isin(['a','b'])]` | [list of values to select a row]                               |
| `dput(df)`                  | `df.to_dict(orient="list")`   | [Print pandas data frame for reproducible example]             |
| `expand.grid(df$x,df$y)`    | `itertools.product`           | see section below                                              |
| `ifelse`                    | `df.where()`                  | [ifelse in pandas]                                             |
| `gsub`                      | `df.x.replace(regex=True)`    | [gsub in pandas]                                               |
|                             | or df.x.str.replace()         |                                                                |
| `length(df)` and `dim(df)`  | `df.shape`                    | [row count of a data frame]                                    |
| `rbind`                     | `pandas.concat`               | [Pandas version of rbind]                                      |
| `rep(1,3)`                  | `[1]*3`                       |                                                                |
| `seq(1:5)`                  | `np.array(range(0,5))`        | [numpy function to generate sequences]                         |
| `summary`                   | `describe`                    |                                                                |
| `str`                       | `df.info()`                   | [pandas equivalents for R functions like str summary and head] |


[gsub in pandas]: https://stackoverflow.com/questions/21834293/equivalent-of-gsub-for-pandas-series-dataframe/56547104?noredirect=1#comment99677847_56547104
[list of values to select a row]: https://stackoverflow.com/questions/12096252/use-a-list-of-values-to-select-rows-from-a-pandas-dataframe
[row count of a data frame]: https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe
[Pandas version of rbind]: https://stackoverflow.com/questions/14988480/pandas-version-of-rbind
[Print pandas data frame for reproducible example]: https://stackoverflow.com/questions/47450931/print-pandas-data-frame-for-reproducible-example-equivalent-to-dput-in-r
[numpy function to generate sequences]: https://stackoverflow.com/a/60753578/2641825
[pandas equivalents for R functions like str summary and head]: https://stackoverflow.com/questions/27637281/what-are-python-pandas-equivalents-for-r-functions-like-str-summary-and-he
[ifelse in pandas](https://stackoverflow.com/questions/43391591/if-else-function-in-pandas-dataframe)

The mapping of tidyverse to pandas is:

| tidyverse                 | pandas                                           | Help or SO questions                 |
| ------------------------- | ------------------------------------------------ | -----------------------------        |
| arrange                   | df.sort_values(by="y", ascending=False)          |                                      |
| df %>% select(-a,-b)      | df.drop(columns=['x', 'y'])                      |                                      |
| select(a)                 | df.filter(items=['x'])                           |                                      |
| select(contains("a"))     | df.filter(regex='x')                             |                                      |
| filter                    | df.query("y=='b'")                               |                                      |
| group_by                  | groupby                                          |                                      |
| lag                       | [shift]                                          | [pandas lag function][so_lag_pandas] |
| mutate                    | df.assign(e = lambda x: x["a"] * 3)              | [assign]                             |
| pivot_longer              | [melt] or [wide_to_long]                         |                                      |
| pivot_wider               | [pivot]                                          |                                      |
| rename                    | df.rename(columns={'a':'new'})                   |                                      |
| separate                  | df['b','c'] = df.a.str.split(',',1,expand=True)  | [pandas separate] str section        |
| summarize                 | agg                                              |                                      |
| unite                     | df["z"] = df.y + df.y                            | [pandas unite]                       |
| unnest                    | [explode]                                        | [unnest in pandas][so_unnest]        |

[assign]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html
[explode]: https://pandas.pydata.org/pandas-docs/version/0.25/reference/api/pandas.DataFrame.explode.html
[melt]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html
[pivot]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html
[shift]: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html
[so_lag_pandas]: https://stackoverflow.com/questions/23664877/pandas-equivalent-of-oracle-lead-lag-function
[so_unnest]: https://stackoverflow.com/a/53218939/2641825
[wide_to_long]: https://pandas.pydata.org/docs/reference/api/pandas.wide_to_long.html
[pandas separate]: https://stackoverflow.com/a/57823566
[pandas unite]: https://stackoverflow.com/questions/19377969/combine-two-columns-of-text-in-pandas-dataframe

Methods to use inside the .groupby().agg() method:

* `sum`
* `count`
* `mean`
* `', '.join` [to get a union of strings](https://stackoverflow.com/questions/17841149/pandas-groupby-how-to-get-a-union-of-strings)


### Expand grid in pandas

This [SO answer](https://stackoverflow.com/a/60371544/2641825) provides an
implementation of expand grid using itertools:

    import itertools
    import pandas
    countries = ["a","b","c","d"]
    years = range(1990, 2020)
    expand_grid = list(itertools.product(countries, years))
    df = pandas.DataFrame(expand_grid, columns=('country', 'year'))

Another [SO answer on the same topic](https://stackoverflow.com/a/64449660/2641825)


### Blogs and quotes on Pandas and R

 - [Moving from R to
   python](https://www.kdnuggets.com/2017/02/moving-r-python-libraries.html)

> "One thing that is a blessing and a curse in R is that the machine learning
> algorithms are generally segmented by package. [...]  it can be a pain for
> day-to-day use where you might be switching between algorithms. [...]
> scikit-learn provides a common set of ML algorithms all under the same API. 

> "one thing that R still does better than Python is plotting. Hands down, R is
> better in just about every facet. Even so, Python plotting has matured though
> it's a fractured community."

 * pandas.pydata.org [Comparison with
   R](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html)

 * [Tydiverse style pandas](https://stmorse.github.io/journal/tidyverse-style-pandas.html) 

> "Tidyverse allows a mix of quoted and unquoted references to variable names.
> In my (in)experience, the convenience this brings is accompanied by equal
> consternation. It seems to me a lot of the problems solved by tidyeval would
> not exist if all variables were quoted all the time, as in pandas, but there
> are likely deeper truths I’m missing here…" 

Help of the R function unite from the tidyr package:

> "col: The name of the new column, as a string or symbol. This argument is
> passed by expression and supports quasiquotation (you can unquote strings and
> symbols). The name is captured from the expression with ‘rlang::ensym()’
> (note that **this kind of interface where symbols do not represent actual
> objects is now discouraged** in the tidyverse; we support it here for backward
> compatibility)."

The use of symbols which do not represent actual objects was was frustrating at first when using pandas,
because we hat to use df["x"] to assign vectors to new column names whereas we
could use df.x to display them.


## Xarray, pandas and R

The [xarray user guide page on pandas](https://docs.xarray.dev/en/stable/user-guide/pandas.html) cites Hadley Whickham's paper on tidy data:

> "Tabular data is easiest to work with when it meets the criteria for tidy
> data".


## Personal reflection

R is great for statistical analysis and plotting. You can also use it to
elaborate a pipeline to load data, prepare it and analyse it. But when things
start to get complicated, such as loading json data from APIs, or dealing with
http requests issues, or understanding lazy evaluation, or the consequences of
non standard evaluation, moving down the rabbit whole can get really
complicated with R. The rabbit hole slide is smoother with python. I have the
feeling that I keep a certain level of understanding at all steps. It's just a
matter of taste anyway.

The Python language can be more verbose on some aspects, but it allows for
greater programmability, it is also more predictable because non standard
evaluation doesn't create scoping problems and it enables to dive deeper into
input/output issues such as URL request headers for example. R remains very
good for data exploration, statistical analysis and plotting because non
standard evaluation makes it possible to call variables without quotes and to
pass formulas to plotting and estimation functions. 

I see R more like the bash command line. It's great for scripts, but you
wouldn't want to write large applications in bash.

Non standard evaluation doesn't exist in python. 
- An email thread
discussing [the idea of non standard evaluation in
python](https://www.mail-archive.com/python-ideas@python.org/msg15694.html).
- A [comparison of a python implementation and an R implementation using non
  standard evaluation](https://third-bit.com/2018/11/16/non-standard-evaluation/).


# String 

See also string operations in pandas character vectors.

[SO answer ](https://stackoverflow.com/questions/25675943/how-can-i-concatenate-str-and-int-objects) 
providing various ways to concatenate python strings.


## Number formatting in strings

[How to print number with commas as thousands separators?](https://stackoverflow.com/a/10742904/2641825)

    f'{1e6:,}' 

See also string operations in pandas with df["x"].str methods.


## Regex substitution

[Documentation of the re package](https://docs.python.org/3/library/re.html).

Replace one or more consecutive non alphanumeric characters by an underscore.

    re.sub(r'\W+', '_', 'bla: bla**(bla)')

Insert a suffix in a file name before the extension
[SO anwser](https://stackoverflow.com/a/2763772/2641825)

    import re
    re.sub(r'(?:_a)?(\.[^\.]*)$' , r'_suff\1',"long.file.name.jpg")


## Regex search

Search for patterns

    import re
    re.findall(r'\bf[a-z]*', 'which foot or hand fell fastest')
    ['foot', 'fell', 'fastest']

    re.findall(r'(\w+)=(\d+)', 'set width=20 and height=10')
    [('width', '20'), ('height', '10')]

Search for ab in baba:

    re.search("ab", "baba")

Search for the numeric after "value_"

    re.findall("value_(\d+)", "value_2022")

Search for the group values occurrence of the numeric after "value_"

    re.search("(value)_(\d+)", "value_2022").group(0)
    re.search("(value)_(\d+)", "value_2022").group(1)
    re.search("(value)_(\d+)", "value_2022").group(2)

Search charcters that are not value

    l = ["value123", "a", "b"]
    [x for x in l if not re.search("value", x)]


## Split

[Split lines in a string](https://stackoverflow.com/a/172454/2641825)

    input = """bla
    bla
    bla"""
    for line in input.splitlines():
        print(line, "\n")


# Statistics

## Linear programming solvers

Real Python [What is linear
programing](https://realpython.com/linear-programming-python/#what-is-linear-programming) 

> Several free Python libraries are specialized to interact with linear or
> mixed-integer linear programming solvers:

[SciPy Optimization and Root
Finding](https://docs.scipy.org/doc/scipy/reference/optimize.html)

[PuLP](https://www.coin-or.org/PuLP/solvers.html)

[Pyomo](https://pyomo.readthedocs.io/en/stable/solving_pyomo_models.html#supported-solvers)

[CVXOPT](https://cvxopt.org/userguide/coneprog.html#optional-solvers)


## Scaling

[Feature scaling with scikit learn](http://benalexkeen.com/feature-scaling-with-scikit-learn/)

 * StandardScaler
 * MinMaxScaler
 * RobustScaler
 * Normalizer


# Style guide and linter

## Black

[Black](https://github.com/psf/black)
is "the uncompromising Python code formater"

Run `black` as a pre commit hook with `pre-commit` as explained below.

In vim, you can run black on the current file with:

    :!black %


## Flake 8

Flake 8 looks at more than just formatting.

[List of FLake8 warnings and error
codes](https://pypi.org/project/flake8/1.6.1/#warning-error-codes)


## PEP Python Enhancement Proposals

[PEP 8 Style Guide for Python Code](https://legacy.python.org/dev/peps/pep-0008/#version-bookkeeping)

> "A style guide is about consistency. Consistency with this style guide is
> important. Consistency within a project is more important. Consistency within
> one module or function is the most important."

> "However, **know when to be inconsistent** -- sometimes style guide
> recommendations just aren't applicable. When in doubt, use your best
> judgment. Look at other examples and decide what looks best. And don't
> hesitate to ask!"

> "In particular: do not break backwards compatibility just to comply with this
> PEP!"


## Pre commit hooks

Blog:

- [pre commits using black and
  flake8](https://ljvmiranda921.github.io/notebook/2018/06/21/precommits-using-black-and-flake8/)

### Install pre commit hooks

Install `pre-commit`

    pip install pre-commit

Set up `pre-commit` in a repository

    cd path_to_repository
    # Add the "pre-commit" python module to a requirements file
    vim requirements.txt 
    # Create a configuration file
    vim .pre-commit-config.yaml 

Configuration options such as

```
repos:
-   repo: https://github.com/ambv/black
    rev: 21.6b0
    hooks:
    - id: black
      language_version: python3.7
-   repo: https://gitlab.com/pycqa/flake8
    rev: 3.7.9
    hooks:
    - id: flake8
```

Update [hook repositories to the latest
version](https://pre-commit.com/#using-the-latest-version-for-a-repository)

    pre-commit autoupdate

Install git hooks in your .git/ directory.

    pre-commit install


### Usage in Continuous integration

[Usage in Continuous integration](https://pre-commit.com/#usage-in-continuous-integration) has a gitlab example:

```
    my_job:
      variables:
        PRE_COMMIT_HOME: ${CI_PROJECT_DIR}/.cache/pre-commit
      cache:
        paths:
          - ${PRE_COMMIT_HOME}
```


### Un install pre-commit hooks

Uninstall

    pre-commit uninstall


## Pylint

Edit the configuration file

    vim ~/.pylintrc

List of good names that shouldn't give a "short name" warning

    [pylint]
    good-names=df

Generate a configuration file

    pylint --generate-rcfile

Blog

- Reddit [Any advantages of Flake8 over
  PyLint](https://www.reddit.com/r/Python/comments/82hgzm/any_advantages_of_flake8_over_pylint/)
  some answers suggest keeping both.



###  Dangerous default argument

- [SO
answer][https://stackoverflow.com/questions/101268/hidden-features-of-python#113198)
- [pylint issues](https://github.com/PyCQA/pylint/issues/3642)

> I understand the dangerous of using a mutable default value and I suggest
> switching the warning message for something like "Dangerous mutable default
> value as argument". However, this is dangerous for all sorts of scenarios? (I
> know that pylint isn't supposed to check the functionality of my code, just
> trying to clarify this anti-pattern)

    >>> def find(_filter={'_id': 0}):
    ...     print({**find.__defaults__[0], **_filter})
    ...
    >>> find()
    {'_id': 0}
    >>> find({'a': 1})
    {'_id': 0, 'a': 1}
    >>> find()
    {'_id': 0}
    >>> find({'a': 1, 'b': 2})
    {'_id': 0, 'a': 1, 'b': 2}

> One might argue that the following should be used and I tend to agree:

    >>> def find(_filter=None):
    ...     if _filter is None:
    ...             _filter = {'_id': 0}
    ...     else:
    ...             _filter['_id'] = 0
    ...     print(_filter)
    ...
    >>> find()
    {'_id': 0}
    >>> find({'a': 1})
    {'a': 1, '_id': 0}
    >>> find()
    {'_id': 0}
    >>> find({'a': 1, 'b': 2})
    {'a': 1, 'b': 2, '_id': 0}



### Using with for resource allocation

Pylint message

> Consider using 'with' for resource-allocating operations

Explained in a [SO answer](https://stackoverflow.com/a/67419279/2641825)

> suppose you are opening a file:

    file_handle = open("some_file.txt", "r")
    ...
    ...
    file_handle.close()

> You need to close that file manually after required task is done. If it's not
> closed, then resource (memory/buffer in this case) is wasted. If you use `with`
> in the above example:

    with open("some_file.txt", "r") as file_handle:
        ...
        ...

> there is no need to close that file. Resource de-allocation automatically
> happens when you use with. 


# System information

Platform type

    import sys
    sys.platform

or

    import os
    os.name

Sys and os return different results 'linux' or 'posix'.

More details are given by

    os.uname()


## Environment variables

Get an environment variable

    import os
    os.environ["XYZ"]

Set an environment variable

    os.environ["XYZ"]  = "/tmp"


## Memory

### Memory usage of a python object

To display the memory usage of a python object

    import sys
    a = 1
    print(sys.getsizeof(a))

See also the section on memory usage of pandas data frames under columns /
memory usage.


### Out of memory error

Sometimes when a python process runs out of memory, it can get killed by the
Linux Kernel. In that case the error message is short "killed" and there is no
python trace back printed. You can check that it is indeed a memory error by
calling

    sudo dmesg

Here is a typical message:

    [85962.510533] Out of memory: Kill process 16035 (ipython3) score 320 or sacrifice child
    [85962.510554] Killed process 16035 (ipython3) total-vm:7081812kB, anon-rss:4536336kB, file-rss:0kB, shmem-rss:8kB
    [85962.687468] oom_reaper: reaped process 16035 (ipython3), now anon-rss:0kB, file-rss:0kB, shmem-rss:8kB

Various related Stack Overflow questions [what does "kill"
mean](https://stackoverflow.com/questions/19189522/what-does-killed-mean-when-a-processing-of-a-huge-csv-with-python-which-sudde), 
[How can I find the reason that python script is
killed?](https://stackoverflow.com/questions/47408608/how-can-i-find-the-reason-that-python-script-is-killed),
[Why does python script randomly gets
killed?](https://stackoverflow.com/questions/1811173/why-does-my-python-script-randomly-get-killed).



# Test driven development

A good post about TDD 
[Unit testing your doing it wrong](https://medium.com/@Cyrdup/unit-testing-youre-doing-it-wrong-407a07692989)

> "TDD is actually about every form of tests. For example, I often write
> performance tests as part of my TDD routine; end-to-end tests as well.
> Furthermore, this is about behaviours, not implementation: you write a new
> test when you need to fulfil a requirement. You do not write a test when you
> need to code a new class or a new method. Subtle, but important nuance. For
> example, you should not write a new test just because you refactored the
> code. If you have to, it means you were not really doing TDD."
> [...]
> "Good tests must test a behavior in isolation to other tests. Calling them unit, system or integration has no relevance to this.
> Kent Beck says this much better than I would ever do.
> '''From this perspective, the integration/unit test frontier is a frontier of design, not of tools or frameworks or how long tests run or how many lines of code we wrote get executed while running the test.'''
> Kent Beck"


## Assertions


- [Use of Assertions](https://blog.regehr.org/archives/1091)


## Unit tests

See 

- [python markdown unit tests](https://github.com/okken/markdown.py/blob/master/test_markdown_unittest.py)

- [Unit testing with
  singleton](https://stackoverflow.com/questions/2085953/unit-testing-with-singletons)


## pytest

Numpy moved from nose to pytest as explained in their
[testing guidelines](https://docs.scipy.org/doc/numpy/reference/testing.html):

> "Until the 1.15 release, NumPy used the nose testing framework, it now uses the pytest framework. The older framework is still maintained in order to support downstream projects that use the old numpy framework, but all tests for NumPy should use pytest." 

Save this function in a file names test_numpy.py

    def test_numpy_closeness():
        assert [1,2] == [1,2]
        assert (np.array([1,2]) == np.array([1,2])).all()
        np.testing.assert_allclose(np.array([1,2]),np.array([1,3]))

Save the file to test_nn.py

    import neural_nets as nn
    import numpy as np

    def test_rectified_linear_unit():
        x = np.array([[1,0],
                      [-1,-3]])
        expected = np.array([[1,0],
                             [0,0]])
        provided = nn.rectified_linear_unit(x)
        assert np.allclose(expected, provided), "test failed"

Execute the test suite from bash with py.test as follows:

    cd ~/rp/course_machine_learning/projects/project_2_3_mnist/part2-nn
    py.test

Test pandas data frame with

    assert_frame_equal
    assert_series_equal # tricky to use


### Expected exceptions

pytest [assert](https://docs.pytest.org/en/6.2.x/assert.html)

> "In order to write assertions about raised exceptions, you can use
> pytest.raises() as a context manager like this:"

    import pytest
    def test_zero_division():
        with pytest.raises(ZeroDivisionError):
            1 / 0

> "and if you need to have access to the actual exception info you may use:"

    def test_recursion_depth():
        with pytest.raises(RuntimeError) as excinfo:

            def f():
                f()

            f()
        assert "maximum recursion" in str(excinfo.value)

> "excinfo is an ExceptionInfo instance, which is a wrapper around the actual
> exception raised. The main attributes of interest are .type, .value and
> .traceback."


## Test use in projects

- [tabulate](https://pypi.org/project/tabulate/)

    > "uses pytest testing framework and [tox](https://tox.wiki/en/latest/) to
    > automate testing in different environments."


# Web 

## Back-end API

- https://fastapi.tiangolo.com/ seems to be a recommended way to create python APIs.

- Django API

- Flask API


## Frameworks

[Flask vs. Django](asynchrosnus://www.codementor.io/garethdwyer/flask-vs-django-why-flask-might-be-better-4xs7mdf8v)

Note: [Flask Evolution into Quart](http://pgjones.gitlab.io/quart/flask_evolution.html#flask-evolution) 
to support 
[asyncio](http://pgjones.gitlab.io/quart/asyncio.html#asyncio) 
This last link contains a nice, simple example of 
how asyncio works with a simulated delay to fetch a web page.


# Media and organizations

## Blogs

* Julio Biason [Things I Learnt The Hard Way (in 30 Years of Software
  Development)](https://blog.juliobiason.me/thoughts/things-i-learnt-the-hard-way/) 

* Daniel Lemire [I do not use a debugger](https://lemire.me/blog/2016/06/21/i-do-not-use-a-debugger/)

    > “Debuggers don’t remove bugs. They only show them in slow motion.”

    [Linus Toarvald doesn't use a debugger](https://lwn.net/2000/0914/a/lt-debugger.php3)

- Wes McKinney 

  - 2017 [Apache Arrow and the 10 things I hate about
  pandas](https://wesmckinney.com/blog/apache-arrow-pandas-internals/)

    > "pandas rule of thumb: have 5 to 10 times as much RAM as the size of your
    > dataset "

  - 2018 [Announcing Ursalabs](https://wesmckinney.com/blog/announcing-ursalabs/)

    > "It has long been a frustration of mine that it isn’t easier to share code
    > and systems between R and Python. This is part of why working on Arrow has
    > been so important for me; it provides a path to sharing of systems code
    > outside of Python by enabling free interoperability at the data level."

    > "Critically, RStudio has avoided the “**startup trap**” and managed to build a
    > sustainable business while still investing the vast majority of its
    > engineering resources in **open source development**. Nearly 9 years have passed
    > since J.J. started building the RStudio IDE, but in many ways he and Hadley
    > and others feel like they are just getting started."

- Dotan Nahum [Functional Programming with Python for People Without Time](https://jondot.medium.com/functional-programming-with-python-for-people-without-time-1eebdbd9526c)

    > "Cracks in the Ice - We ended the previous part with stating that with a good
    > measure of abstraction, functional programming doesn’t offer a considerable
    > advantage over the “traditional” way of design, object oriented. It’s a lie.
    > [...] In our pipeline example above with our Executors — how do you feed in
    > the output of one executor as the input for the next one? well, you have to
    > build that infrastructure. With functional programming, those abstractions
    > are not abstractions that you have to custom build. They’re part of the
    > language, mindset, and ecosystem. Generically speaking — it’s all about
    > impedence mismatch and leaky abstractions and when it comes to data and
    > functions; there’s no mismatch because it’s built up from the core. The
    > thesis is — that to build a functional programming approach over an
    > object-oriented playground — is going to crash and burn at one point or
    > another: be it bad modeling of abstractions, performance problems, bad
    > developer ergonomics, and the worst — wrong mindset. Being able to model
    > problems and solutions in a functional way, transcends above traditional
    > abstraction; the object-oriented approach, in comparison, is crude,
    > inefficient and prone to maintenance problems."


- Christopher Rackauckas [Why numba and cython are no substitute for
  Julia](https://www.stochasticlifestyle.com/why-numba-and-cython-are-not-substitutes-for-julia/)
  discusses the advantages of the Julia language over Python for large code
  bases.

- Ethan Rosenthal [Everything Gets a Package: My Python Data Science
  Setup](https://www.ethanrosenthal.com/2022/02/01/everything-gets-a-package/)


- [Data Formats for Panel Data
Analysis](https://bashtage.github.io/linearmodels/panel/examples/data-formats.html)

> There are two primary methods to express data:
>
> - MultiIndex DataFrames where the outer index is the entity and the inner is the time
>   index. This requires using pandas.
>
> - 3D structures were dimension 0 (outer) is variable, dimension 1 is time index and
>   dimension 2 is the entity index. It is also possible to use a 2D data structure with
>   dimensions (t, n) which is treated as a 3D data structure having dimensions (1, t, n).
>   These 3D data structures can be pandas, NumPy or xarray.

- [Pandas for panel data](https://python.quantecon.org/pandas_panel.html)

Explains multi index with stacking and unstacking.

- [Sphinx reStructuredText vs markdown for project
  documentation](https://www.zverovich.net/2016/06/16/rst-vs-markdown.html)


## Foundations

- [COIN-OR project](https://www.coin-or.org/projects/) "open source for the
  operations research community" 

    > "Without open source implementations of existing algorithms, testing new
    > ideas built on existing ones typically requires the time-consuming and
    > error-prone process of re-implementing (and re-debugging and re-testing) the
    > original algorithm. If the original algorithm were publicly available in a
    > community repository, imagine the productivity gains from software reuse!
    > Science evolves when previous results can be easily replicated"

- [Python software foundation](https://www.python.org/psf-landing/)

    > "We support and maintain python.org, The Python Package Index, Python
    > Documentation, and many other services the Python Community relies on."


## Interviews

[Interview with Alex
Martelli](https://mappingthejourney.com/single-post/2017/06/21/episode-2-interview-with-alex-martelli-python-guru/)

> "Larry Page in his dormitory at Stanford had written or tried to write a web
> spider to get a copy of some subset of the web on his computers so he could
> try his famous Page algorithm. He was trying to use the brand-new language
> Java in 1.0 beta version, and it kept crashing. So he asked for help from his
> roommate and his roommate took a look at said ‘oh you’re using that Java
> disaster’. Of course, it crashed and did it in 100 lines of Python. It runs
> perfectly, and that’s how Google became possible through 100 lines of Python.
> But I had no idea until about five years ago that it had played so crucial
> role so early on."

> " Similarly, if I hadn’t heard it from the mouth of Guido himself, I would
> never have known that Python was at the heart of the web. The very first Web
> server and web browser were written by the inventor of the World Wide Web,
> HTTP, and HTML in Python. He wasn’t really a programmer; he was a physicist
> and Python was far easier to use than anything else."


## Podcasts

[talk python to me](https://talkpython.fm/)

* [Data science year in
  review](https://talkpython.fm/episodes/show/193/data-science-year-in-review-2018-edition)

- Interesting how Samuel Colvin talks about serialization [automate your data
  exchange with pydantic](https://talkpython.fm/episodes/show/313/automate
  your data exchange with pydantic)

## Teaching


