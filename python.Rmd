---
title: "Python Commands"
author: "Paul Rougieux"
date: "17 August 2017"
output: 
  html_document: 
    toc: yes
---


# Pip

pypi.org [pip](https://pypi.org/project/pip/):

> "pip is the package installer for Python. You can use pip to install packages
> from the Python Package Index and other indexes."

[What is the difference between a python module and a python
package](https://stackoverflow.com/questions/7948494/whats-the-difference-between-a-python-module-and-a-python-package)

> "A package is a collection of modules in directories that give a package
> hierarchy."

[What is
setup.py?](https://stackoverflow.com/questions/1471994/what-is-setup-py)


## Install an old version

For example to install pandas 0.24.2

    python3 -m pip install --user pandas==0.24.2 

or

    pip3 install --user pandas==0.24.2

Sometimes you need to overwrite the existing version with I.

    pip install -I  package==version


## Install a local version

To [install the local version of a package with
pip](https://stackoverflow.com/a/18021540/2641825)

    pip install -e /develop/MyPackage

According to `man pip`, the `-e` option "installs a project in editable mode
(i.e. setuptools "develop mode") from a local project path or a VCS url".


## Upload a package to pypi

To upload a package to pypi, you need a pypi account. The [instructions on
uploading distribution archives explain how to upload the package to pypi](
https://packaging.python.org/tutorials/packaging-projects/#uploading-the-distribution-archives):
    
      python3 -m twine upload --repository testpypi dist/*


### Authorship

It's only possible to specify one author field in setup.py.
The recommendation is to use a mailing list when there are multiple authors and
to set separate files for attribution. 

[How to specify multiple authords in setup.py?](https://stackoverflow.com/questions/9999829/how-to-specify-multiple-authors-emails-in-setup-py)

## Virtual environments

[Pipenv](https://pipenv.pypa.io/en/latest/)
makes pip and virtual env work together. 

# Control flow

dotnetperls: [not python](https://www.dotnetperls.com/not-python)

Sample code with a function and if conditions:

	def function(condition):
		if condition:
			print("Hi")
		if not condition:
			print("Bye")
	function(True)
	function(False)
	function('')
	function('lalala')

# Databases

## SQL Alchemy

The
[pandas.to_sql](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-sql-method)
method uses sqlalchemy to write pandas data frame to a PostgreSQL database.

> "The pandas.io.sql module provides a collection of query wrappers to both
> facilitate data retrieval and to reduce dependency on DB-specific API.
> Database abstraction is provided by SQLAlchemy if installed. In addition you
> will need a driver library for your database. Examples of such drivers are
> psycopg2 for PostgreSQL or pymysql for MySQL. For SQLite this is included in
> Python’s standard library by default." 

SQL Alchemy works with [metadata
objects](https://docs.sqlalchemy.org/en/14/core/schema.html):

> The core of SQLAlchemy’s query and object mapping operations are supported by
> **database metadata**, which is comprised of Python objects that describe tables
> and other schema-level objects. These objects are at the core of three major
> types of operations - issuing CREATE and DROP statements (known as DDL),
> constructing SQL queries, and expressing information about structures that
> already exist within the database. Database metadata can be expressed by
> explicitly naming the various components and their properties, using
> constructs such as Table, Column, ForeignKey and Sequence, all of which are
> imported from the sqlalchemy.schema package. It can also be generated by
> SQLAlchemy using a process called reflection, which means you start with a
> single object such as Table, assign it a name, and then instruct SQLAlchemy
> to load all the additional information related to that name from a particular
> engine source.

[Reflecting database
objects](https://docs.sqlalchemy.org/en/14/core/reflection.html)

    from sqlalchemy import MetaData
    from sqlalchemy import Table
    meta = MetaData(schema = "raw_comtrade")
    meta.bind = comtrade.database.engine
    yearly_hs2 = Table('yearly_hs2', meta, autoload_with=comtrade.database.engine)

SQL Alchemy has an
[automap](https://docs.sqlalchemy.org/en/14/orm/extensions/automap.html)
feature which generates mapped classes and relationships from a database
schema.

I used [sqlacodegen](https://pypi.org/project/sqlacodegen/) to automatically
generate python code from an existing PostGreSQl database table as follows

    sqlacodegen --schema raw_comtrade --tables yearly_hs2 postgresql://rdb@localhost/biotrade


### Check for table existence

Paul's [SO Answer](https://stackoverflow.com/a/69224576/2641825). SQL Alchemy's
recommended way to check for the presence of a table is to create an inspector
object and use its `has_table()` method. The following example was copied from
[sqlalchemy.engine.reflection.Inspector.has_table](https://docs.sqlalchemy.org/en/14/core/reflection.html#sqlalchemy.engine.reflection.Inspector.has_table),
with the addition of an SQLite engine to make it reproducible:

    In [17]: from sqlalchemy import create_engine, inspect
        ...: from sqlalchemy import MetaData, Table, Column, Text
        ...: engine = create_engine('sqlite://')
        ...: meta = MetaData()
        ...: meta.bind = engine
        ...: user_table = Table('user', meta, Column("name", Text))
        ...: user_table.create()
        ...: inspector = inspect(engine)
        ...: inspector.has_table('user')
    Out[17]: True

You can also use the `user_table` metadata element `name` to check if it exists as such:

    inspector.has_table(user_table.name)


## PostgreSQL

Create a database engin with SQLalchemy

    from sqlalchemy import create_engine
    engine = create_engine('postgresql://myusername:mypassword@myhost:5432/mydatabase')

Blogs and Stackoverflow

- [Load data into postgreSQL using python](https://hakibenita.com/fast-load-data-python-postgresql) (without pandas)

- [Load CSVs into PostgreSQL using python and
  pandas](https://medium.com/@apoor/quickly-load-csvs-into-postgresql-using-python-and-pandas-9101c274a92f)

- [SO question pygresql-vs-psycopg2](https://stackoverflow.com/questions/413228/pygresql-vs-psycopg2)

- [5 ways to backup your postgreSQl database using
  python](https://medium.com/poka-techblog/5-different-ways-to-backup-your-postgresql-database-using-python-3f06cea4f51)
  Mentions the [sh package](https://pypi.org/project/sh/), a subprocess replacement.


## SQLite

Create an SQLITE in memory database and add a table to it. 

    In [17]: from sqlalchemy import create_engine, inspect
        ...: from sqlalchemy import MetaData, Table, Column, Text
        ...: engine = create_engine('sqlite://')
        ...: meta = MetaData()
        ...: meta.bind = engine
        ...: user_table = Table('user', meta, Column("name", Text))
        ...: user_table.create()
        ...: inspector = inspect(engine)
        ...: inspector.has_table('user')
    Out[17]: True

Create a file based database at a specific path:

    # absolute path
    e = create_engine('sqlite:////path/to/database.db')


# Editors

## Spyder

I have set the following shortcuts to be similar to RStudio:

* Ctrl+H find and replace dialog
* Ctrl+R run selection or current line
* Ctrl+Shift+C comment/uncomment code block
* F1 inspect current object (i.e. display function and classes documentation)
* F2 go to function definition

## Vim 

See my page on [vim.html](vim.html).

# Neural Networks

## Pytorch

Print the size of the output layer

    import torch
    import torch.nn as nn
    x = torch.randn(28,28).view(-1,1,28,28)
    model = nn.Sequential(
          nn.Conv2d(1, 32, (3, 3)),
          nn.ReLU(),
          nn.MaxPool2d((2, 2)),
          nn.Conv2d(32, 64, (3, 3)),
    )
    print(model(x).shape)


# Object types

`type()` displays the type of an object.

    i = 1
    print(type(i))
    # <type 'int'>
    x = 1.2
    print(type(x))
    # <type 'float'>
    t = (1,2)
    print(type(t))
    # <type 'tuple'>
    l = [1,2]
    print(type(l))
    # <type 'list'>


## Convert between object types

Character to numeric

    int("3")
    float("3.33")
    int("3.33")

Numeric to character

    str(2)

Convert a list to a comma separated string

    ",".join(["a","b","c"])

Another example with the list of the last 5 years

    import datetime
    year = datetime.datetime.today().year
    # Create a numeric list of years
    YEARS = [year - i for i in range(1,6)]
    # Convert each element of the list to a string
    YEARS = [str(x) for x in YEARS]
    ",".join(YEARS)


## Dictionary

Create a dictionary

    {'x':1, 'y':2, 'z':3}

Converts 2 lists into a dictionary

    dict(zip(['x', 'y', 'z'], [1, 2, 3]))


## List

[Remove an element from a list of strings ](https://stackoverflow.com/a/31077838/2641825)

    myList = ['a', 'b', 'c', 'd']
    myList.remove('c')
    myList
    ['a', 'b', 'd']


### List of tuples

[How to flatten a list of tuples](https://stackoverflow.com/questions/10632839/transform-list-of-tuples-into-a-flat-list-or-a-matrix)

    nested_list = [(1, 2, 4), (0, 9)]

Using `reduce`:

    reduce(lambda x,y:x+y, map(list, nested_list))                                                                                                                              
    [1, 2, 4, 0, 9]

Using itertools.chain:

    import itertools
    list(itertools.chain.from_iterable(nested_list))

Using `extend`:
    
    flat_list = []
    for a_tuple in nested_list:
        flat_list.extend(list(a_tuple))                                                                                                                                     
    flat_list
    [1, 2, 4, 0, 9]


## Set

Instances of [set](https://docs.python.org/3/library/stdtypes.html#set)
provide the following operations:


	issubset(other)
	set <= other

	    Test whether every element in the set is in other.

	set < other

	    Test whether the set is a proper subset of other, that is, set <= other and set != other.

	issuperset(other)
	set >= other

	    Test whether every element in other is in the set.

	set > other

	    Test whether the set is a proper superset of other, that is, set >= other and set != other.

	union(*others)
	set | other | ...

	    Return a new set with elements from the set and all others.

	intersection(*others)
	set & other & ...

	    Return a new set with elements common to the set and all others.

	difference(*others)
	set - other - ...

	    Return a new set with elements in the set that are not in the others.

	symmetric_difference(other)
	set ^ other

	    Return a new set with elements in either the set or other but not both.
    

# HTTP

## File download

### Zipped csv files

The following example uses urllib.request.urlopen to download a zip file
containing Oceania's crop production data from the FAO statistical database. In
that example, it is necessary to define a minimal header, otherwise FAOSTAT
throws an `Error 403: Forbidden`. It was posted as a [StackOverflow
Answer](https://stackoverflow.com/a/68804963/2641825).

    import shutil
    import urllib.request
    import tempfile

    # Create a request object with URL and headers    
    url = “http://fenixservices.fao.org/faostat/static/bulkdownloads/Production_Crops_Livestock_E_Oceania.zip”
    header = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) '}
    req = urllib.request.Request(url=url, headers=header)

    # Define the destination file
    dest_file = tempfile.gettempdir() + '/' + 'crop.zip'
    print(f“File located at:{dest_file}”)

    # Create an http response object
    with urllib.request.urlopen(req) as response:
        # Create a file object
        with open(dest_file, "wb") as f:
            # Copy the binary content of the response to the file
            shutil.copyfileobj(response, f)

Based on https://stackoverflow.com/a/48691447/2641825 and
https://stackoverflow.com/a/66591873/2641825, see also the documentation at
https://docs.python.org/3/howto/urllib2.html 


### JSON files

The following loads a JSON file into a pandas data frame from the Comtrade API.

    import urllib.request
    import json
    import pandas

    url_reporter = "https://comtrade.un.org/Data/cache/reporterAreas.json"
    url_partner = "https://comtrade.un.org/Data/cache/partnerAreas.json"

    # attempt with pandas.io, with an issue related to nested json
    pandas.io.json.read_json(url_reporter, encoding='utf-8-sig')
    pandas.io.json.read_json(url_partner)
    # `results` is a character column containing {'id': '4', 'text': 'Afghanistan'}.
    # Is there a way to tell read_json to load the id and text columns directly instead?

[SO answer](https://stackoverflow.com/a/68988284/2641825)

> "Since the whole processing is done in the pd.io.json.read_json method, we
> cannot select the keys to direct to the actual data that we are after. So you
> need to run this additional code to get your desired results:"

    df = pandas.io.json.read_json(url_reporter, encoding='utf-8-sig')
    df2 = pandas.json_normalize(df.results.to_list())

Other attempt using lower level packages

    req = urllib.request.Request(url=url_reporter)
    with urllib.request.urlopen(req) as response:
        json_content = json.load(response)
        df = pandas.json_normalize(json_content['results'])

    In [17]: df
    Out[17]:
          id                    text
    0    all                     All
    1      4             Afghanistan
    2      8                 Albania
    3     12                 Algeria
    4     20                 Andorra
    ..   ...                     ...
    252  876  Wallis and Futuna Isds
    253  887                   Yemen
    254  894                  Zambia
    255  716                Zimbabwe
    256  975                   ASEAN


- Related question I asked on SO.: [How to load a nested data frame with
  pandas.io.json.read_json?](https://stackoverflow.com/questions/68985729/how-to-load-a-nested-data-frame-with-pandas-io-json-read-json)

- Enconding issue [What is the difference between utf-8 and
  utf-8-sig?](https://stackoverflow.com/questions/57152985/what-is-the-difference-between-utf-8-and-utf-8-sig)

- [What's the difference between UTF-8 and UTF-8 without
  BOM?](https://stackoverflow.com/questions/2223882/whats-the-difference-between-utf-8-and-utf-8-without-bom) 


# ipython

Add these options at the ipyhton command line to reload objects automatically while you are coding

    %load_ext autoreload   
    %autoreload 2         


## Debugging in ipython


Once an error occurs at the ipython command line. 
Press `debug` then you can move up the stack trace with:

    `u`

Move down the stack trace with:

    `d` 

Show code context of the error:

    `l` 

Show available variable in the current context:

    `a` 





# Jupyter notebooks


## Call bash from a notebook

Prefix the bash call with an exclamation mark, for example:

    !df -h

In fact the question mark also works from an ipython shell. 
See also 
[Difference between ! and % in Jupyter Notebooks](https://stackoverflow.com/questions/45784499/difference-between-and-in-jupyter-notebooks)


## Dashboards and widgets

- [Voila dashboard](https://github.com/voila-dashboards/voila)

    - Examples dasboards and applications in the [Voilà
      gallery](https://voila-gallery.org/)

- [ipywidgets](https://github.com/jupyter-widgets/ipywidgets)
  
  > "interactive HTML widgets for Jupyter notebooks and the IPython kernel."


## Display all rows and columns of a data frame

Display all columns

    pandas.options.display.max_columns = None

Display max rows

    pandas.set_option('display.max_rows', 500)

With a  context manager [as in this answer](https://stackoverflow.com/a/47113685/2641825)

    with pd.option_context('display.max_rows', 100, 'display.max_columns', 10):
    some pandas stuff

## Download data from a jupyter notebook    

I wrote this csv download function in an 
[SO answer](https://stackoverflow.com/a/57613621/2641825)

    def csv_download_link(df, csv_file_name, delete_prompt=True):
        """Display a download link to load a data frame as csv from within a Jupyter notebook"""
        df.to_csv(csv_file_name, index=False)
        from IPython.display import FileLink
        display(FileLink(csv_file_name))
        if delete_prompt:
            a = input('Press enter to delete the file after you have downloaded it.')
            import os
            os.remove(csv_file_name)

To get a link to a csv file, enter the above function and the code below in a jupyter notebook cell :

    csv_download_link(df, 'df.csv')

## Documentation with Jupyter

[Using jupyter to write documentation](https://hub.packtpub.com/using-jupyter-write-documentation/)


## Help in a jupyter notebook 

To get help on a function, enter `function_name?` in a cell. 
Quick hep can also be obtained by pressing SHIFT + TAB.

## Install Jupyter

To [install Jupyter](https://jupyter.org/install) notebooks on python3:

    pip3 install jupyter notebook

Then start the notebook server as such:

    jupyter notebook

## Plots




## Security and authentication on a public server

[Jupyter notebook with
authentication](https://stackoverflow.com/questions/37808410/ipython-jupyter-notebook-with-authentication)


## Table of content in your notebooks

Install [jupyter_contrib_nbextensions](https://github.com/ipython-contrib/jupyter_contrib_nbextensions)

    python3 -m pip install --user jupyter_contrib_nbextensions
    python3 -m jupyter contrib nbextension install --user

Activate the table of content extension:

    python3 -m jupyter nbextension enable toc2/main

There are many other extensions available in this package.
Optionally you can install the jupyter notebook extension configurator (not needed)

    python3 -m pip install --user jupyter_nbextensions_configurator
    jupyter nbextensions_configurator enable --user

This will make a configuration interface available at:

    http://localhost:8888/nbextensions

Using the old Table of Content extension
[jupyter table of content extension](https://github.com/minrk/ipython_extensions#table-of-contents)

    jupyter nbconvert --to markdown mynotebook.ipynb
    jupyter nbconvert --to html mynotebook.ipynb

For a colleague using Anaconda [Installing
jupyter_contrib_nbextensions](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html)
specifies that 

> "There are conda packages for the notebook extensions and the
> jupyter_nbextensions_configurator available from conda-forge. You can install
> both using" 

    conda install -c conda-forge jupyter_contrib_nbextensions


## Version control the markdown version with git

Convert notebooks to markdown so they are easier to track in git. 

Install https://github.com/mwouts/jupytext

    python3 -m pip install --user jupytext

More commands:

    python3 -m jupyter notebook --generate-config
    vim ~/.jupyter/jupyter_notebook_config.py

Add this line:

    c.NotebookApp.contents_manager_class = "jupytext.TextFileContentsManager"

And also this line if you always want to pair notebooks with their markdown counterparts:

    c.ContentsManager.default_jupytext_formats = "ipynb,md"

More commands:

    python3 -m jupyter nbextension install jupytext --py --user
    python3 -m jupyter nbextension enable  jupytext --py --user

Add syncing to a given notebook:

    # Markdown sync
    jupytext --set-formats ipynb,md --sync ~/repos/example_repos/notebooks/test.ipynb
    # Python sync
    jupytext --set-formats ipynb,py --sync ~/repos/example_repos/notebooks/test.ipynb


# Errors, exceptions and logging

## Handling Exceptions with try and except statements

Python documentation on [Handling
Exceptions](https://docs.python.org/3/tutorial/errors.html#handling-exceptions).

    while True:
        try:
            x = int(input("Please enter a number: "))
            break
        except ValueError:
            print("Oops!  That was no valid number.  Try again...")

[Exception message capturing](https://stackoverflow.com/a/4690655/2641825)

    for i in [1,0]:
        try:
            print(1/i)
        except Exception as e:
            print("Failed to compute:", str(e))


## Raising Exceptions

Python documentation on [Raising
Exceptions](https://docs.python.org/3/tutorial/errors.html#raising-exceptions)

    raise Exception('spam')
    raise ValueError('Not an acceptable value')
    raise NameError('Wrong name')


## Logging

docs.python.org [logging
cookbook](https://docs.python.org/3/howto/logging-cookbook.html)


Pylint error: Use lazy % formatting in logging functions

[Answer to Lazy evaluation of strings in python logging: comparing `%` with
`.format`](https://stackoverflow.com/a/52012660/2641825)

The documentation https://docs.python.org/2/library/logging.html suggest the
following for lazy evaluation of string interpolation: 

    logging.getLogger().debug('test: %i', 42)


# Functions

Functions in python can be defined with 

    def add_one(x):
        return x + 1
    add_one(1)

    # 2

## Call by reference or call by value

When using numpy arrays, python displays a behaviour of call by reference 

    a = np.array([1,2])

    def changeinput(x, scalar):
        x[0] = scalar

    changeinput(a,3)

    a
    # array([3, 2])

This is really weird coming from R, which has a copy-on-modify principle.

The R Language Definition says this (in section 4.3.3 Argument Evaluation)

> "The semantics of invoking a function in R argument are call-by-value. In
> general, supplied arguments behave as if they are local variables initialized
> with the value supplied and the name of the corresponding formal argument.
> Changing the value of a supplied argument within a function will not affect
> the value of the variable in the calling frame. [Emphasis added]"

## Decorators

Decorators are a way to wrap a function around another function. 
It is useful to repeat a pattern of behaviour around a function.

* Data camp [course on decorators](https://www.datacamp.com/community/tutorials/decorators-python)
* Examples [5 use cases for decorators](https://www.oreilly.com/ideas/5-reasons-you-need-to-learn-to-write-python-decorators)

I have used decorators to cache the function output
along a data processing pipeline. 

# Math

[How to do maths in python 3 with operators](https://www.digitalocean.com/community/tutorials/how-to-do-math-in-python-3-with-operators) 

2 to the power of 3

    2**3
    # 8

Floor division

    5//3
    # 1

Modulo

    5%3
    # 2

# Numpy vectors and matrices (arrays)

All examples below are based on the numpy package being imported as np :

    import numpy as np


## Indexing Multi-dimensional arrays and masks

Numpy [array indexing](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)

> "Basic slicing extends Python’s basic concept of slicing to N dimensions. Basic slicing occurs when 
> obj is a slice object (constructed by start:stop:step notation inside of brackets), an integer, or a tuple of slice objects and integers."
> [...]
> The basic slice syntax is i:j:k where i is the starting index, j is the stopping index, and k is the step ($k\neq0$).
> "[...]
> "Advanced indexing always returns a copy of the data (contrast with basic slicing that returns a view)." 
> "Integer array indexing allows selection of arbitrary items in the array based on their N-dimensional index.
> Each integer array represents a number of indexes into that dimension."

    x[0:3,0:2]
    # array([[0.64174957, 0.18540429],
    #        [0.97558697, 0.69314058],
    #        [0.51646795, 0.71055115]])

In this case because every row is selected, it is the same as:

    x[:,0:2]

Examples modified from https://docs.scipy.org/doc/numpy/user/basics.indexing.html

    y = np.arange(35).reshape(5,7)
    print(y[np.array([0,2,4]), np.array([0,1,2])])

    print('With slice 1:3')
    print(y[np.array([0,2,4]),1:3])
    print('is equivalent to')
    print(y[np.array([[0],[2],[4]]),np.array([[1,2]])])
    # This one is the same but transposed, which is weird
    print(y[np.array([[0,2,4]]),np.array([[1],[2]])])
    # Notice the difference with the following
    print(y[np.array([0,2,4]),np.array([1,2,3])])

Masks [masked array](https://docs.scipy.org/doc/numpy/reference/maskedarray.generic.html)
We wish to mark the fourth entry as invalid. The easiest is to create a masked array:

    x = np.array([1, 2, 3, -1, 5])
    mx = np.ma.masked_array(x, mask=[0, 0, 0, 1, 0])
    print(x.sum(), mx.sum())
    # 10 11


## Matrix creation and shapes

Create a vector

    a = np.array([1,2,3])

Create a matrix

    b = np.array([[1,2,3],[5,6,6]])

Shape

    a.shape
    # (3,)
    b.shape
    # (2, 3)

Matrix of zeroes

    np.zeros([2,2])
    #array([[0., 0.],
    #       [0., 0.]])

Create a matrix with an additional dimension 

    np.zeros(b.shape + (2,))
    array([[[0., 0.],
            [0., 0.],
            [0., 0.]],

           [[0., 0.],
            [0., 0.],
            [0., 0.]]])

Transpose

    b.transpose()
    # array([[1, 5],
    #        [2, 6],
    #        [3, 6]])
    c = b.transpose()

Math functions in numpy:

    np.cos()
    np.sin()
    np.tan()
    np.exp()

min and max

    x = np.array([1,2,3,4,5,-7,10,-8])
    x.max()
    # 10
    x.min()
    # -8

## Matrix multiplication

Matrix multiplication [matmul](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html)

    np.matmul(a,c)
    # array([14, 35])

    # Can also be written as
    a @ c
    # array([14, 35])

Otherwise the multiplication symbol implements an element wise multiplication, also called the

[Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)).
It only works on 2 matrices of same dimensions. 
Element-wise multiplication is used for example in convolution kernels. 

    b * b
    # array([[ 1,  4,  9],
    #        [25, 36, 36]])

So here is again an example showing the difference between

    m = np.array([[0,1],[2,3]])

Element wise multiplication :

    m * m 
    # array([[0, 1],
    #        [4, 9]])

Matrix multiplication :

    m @ m
    # array([[ 2,  3],
    #        [ 6, 11]])


## Norm of a matrix

Linear algebra functionalities are provided by numpy.linalg
For example the norm of a matrix or vector:

    np.linalg.norm(x)
    # 16.3707055437449
    np.linalg.norm(np.array([3,4]))
    # 5.0
    np.linalg.norm(a)
    # 3.7416573867739413

Norm of the matrix for the regularization parameter in a machine learning model

    bli = np.array([[1, 1, 0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0, 0.0, 0.0],
                    [0.0, 1, 0.0, 0.0, 0.0]])
    sum(np.linalg.norm(bli, axis=0)**2) 3.0000000000000004
    sum(np.linalg.norm(bli, axis=1)**2) 3.0000000000000004
    np.linalg.norm(bli)**2 2.9999999999999996

Append vs concatenate

    x = np.array([1,2])
    print(np.append(x,x))
    # [1 2 1 2]
    print(np.concatenate((x,x),axis=None))
    a = np.array([[1, 2], [3, 4]])
    b = np.array([[5, 6]])
    print(np.concatenate((a, b), axis=0))
    print(np.concatenate((a, b.T), axis=1))
    print(np.concatenate((a, b), axis=None))

## Random vector or matrices

    x = np.random.random([3,4])
    x
    # array([[0.64174957, 0.18540429, 0.7045183 , 0.44623567],
    #        [0.97558697, 0.69314058, 0.32469324, 0.82612627],
    #        [0.51646795, 0.71055115, 0.74864751, 0.2142459 ]])


Random choice, with a given probability
Choose zero with probability 0.1 and one with probability 0.9.

    for i in range(10):
        print(np.random.choice(2, p=[0.1, 0.9]))
        
    print(np.random.choice(2, 10, p=[0.1, 0.9]))
    print(np.random.choice(2, (10,10), p=[0.1, 0.9]))

    [[1 1 1 1 1 1 1 1 1 0]
     [1 1 1 1 1 1 1 1 1 1]
     [1 1 1 1 1 1 1 1 1 0]
     [1 1 1 1 1 1 1 1 1 1]
     [1 1 1 1 1 1 1 0 1 1]
     [1 1 1 0 1 1 1 1 1 1]
     [1 1 0 1 1 1 1 1 0 1]
     [1 1 1 1 1 1 1 1 1 1]
     [1 1 1 1 1 1 0 1 1 0]
     [1 1 1 1 1 1 1 1 1 1]]

Error if probabilities do not sum up to one

    print(np.random.choice(2, p=[0.1, 0.8]))

    # ---------------------------------------------------------------------------
    # ValueError                                Traceback (most recent call last)
    # <ipython-input-31-8a8665287968> in <module>
    # ----> 1 print(np.random.choice(2, p=[0.1, 0.8]))

    # mtrand.pyx in numpy.random.mtrand.RandomState.choice()

    # ValueError: probabilities do not sum to 1


# Objects

## Inheritance and composition

Example 

    ###############################################################################
    class Vehicle(object):

        def __init__(self, color, speed_max, garage=None):
            # Default attributes #
            self.color = color
            self.speed_max = speed_max
            self.garage = garage

        def paint(self, new_color):
            # Default attributes #
            self.color = new_color

        def go_back_home(self, new_color):
            # Default attributes #
            self.position = self.go_to(self.parent.location)

    ###############################################################################
    class Car(Vehicle):

        def open_door(self):
            pass

    ###############################################################################
    class Boat(Vehicle):

        def open_balast(self):
            pass

    ###############################################################################
    class Garage(object):

        def __init__(self, all_vehicles):
            self.all_vehicles = all_vehicles

        def mass_paint(self, new_color):
            # Default attributes #
            for v in self.all_vehicles: v.paint(new_color)

        def build_car(self, color):
            new_car = Car(color, 90, self)
            self.all_vehicles.append(new_car)
            return new_car

        @property
        def location(self):
            return '10, 18'

    ###############################################################################
    ###############################################################################
    ###############################################################################
    honda = Car('bleu', 60)
    gorgeoote = Boat('rouge', 30)
    honda.paint('purple')

    mike = Garage([honda, gorgeoote])

    mike.mass_paint()

    sport_car = mike.build_car('rouge')


# Pandas data frames

All code below assumes you have imported pandas
    
    import pandas 


## Creating a data frame

You can create a data frame by passing a dictionary with column names as keys

    pandas.DataFrame({'x':range(0,3), 'y':['a','b','c']})
           a  b
        0  0  3
        1  1  4
        2  2  5

Or by passing a list of tuples and defining the `columns` argument

    pandas.DataFrame(
        list(zip(range(0,3), ['a','b','c'])), 
        columns=["x", "y"]
    )


## Converting a data frame

Convert 2 columns to a dictionary

    df = pandas.DataFrame({'x':range(0,3), 
                           'y':['a','b','c'], 
                           'z':['m','n','o']})
    df.set_index('y').to_dict()['z']


## Columns

List columns as an index object

    df = pandas.DataFrame({'a':range(0,3),'b':range(3,6)})
    df.columns

List columns as a list

    df.columns.tolist()

Select only certain columns in a list

    df['bla'] = 0
    cols = df.columns.tolist()
    [name for name in cols if 'a' in name]


### Rename columns

Rename the 'a' column to 'new'

    df.rename(columns={'a':'new'})

Rename columns to snake case using a regular expression

    import re
    df.rename(columns=lambda x: re.sub(r" ", "_", str(x)).lower(), inplace=True)

Remove parenthesis and dots in column names

    df.rename(columns=lambda x: re.sub(r"[()\.]", "", x), inplace=True)


## Group by operations

Compute the sum for the sepal length grouped by species

    import seaborn
    iris = seaborn.load_dataset("iris")
    iris.groupby('species').agg(sepal_length = ('sepal_length', sum))

Compute the sum but repeated for every original row

    iris['sepal_sum'] = iris.groupby('species')['sepal_length'].transform('sum')
    iris

Compute the cumulative sum of the sepal length

    iris['cumsum'] = iris.groupby('species').sepal_length.cumsum()
    iris['cumsum'].plot()
    from matplotlib import pyplot
    pyplot.show()

Lag 

    iris['cumsum_lag'] = iris.groupby('species')['cumsum'].transform('shift', fill_value=0)
    iris[['cumsum', 'cumsum_lag']].plot()
    pyplot.show()


### Lag or shift a grouped variable

Load the flights dataset and for each month, display the passenger value in the
same month of the previous year. Compare the `passengers` and
`pass_year_minus_one` columns by displaying the tables for January and
December. 

    import seaborn
    flights = seaborn.load_dataset("flights")
    flights['pass_year_minus_one'] = flights.groupby(['month']).passengers.shift()
    flights.query("month=='January'")
    flights.query("month=='December'")


## Joining and merging

Stackoverflow [Pandas merging](https://stackoverflow.com/questions/53645882/pandas-merging-101/53645883#53645883)


## Read and write csv


### From an API

Pandas data frames can be used to read CSV files from the [Comtrade
data API](https://comtrade.un.org/Data/). For example, using the default API
URL for all countries: 

    import pandas
    df1 = pandas.read_csv('http://comtrade.un.org/api/get?max=500&type=C&freq=A&px=HS&ps=2020&r=all&p=0&rg=all&cc=TOTAL&fmt=csv')

    df2 = pandas.read_csv('http://comtrade.un.org/api/get?max=500&type=C&freq=A&px=HS&ps=2020&r=all&p=0&rg=all&cc=01&fmt=csv',
                           # Force the id column to remain a character column,
                           # otherwise str "01" becomes an int 1.
                           dtype={'Commodity Code': str, 'bli': str})


Then use df.to_csv to write the data frame to a csv file

     df1.to_csv("/tmp/comtrade.csv")


## Reshaping

[The Pandas user guide on reshaping](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html#reshaping-by-melt) 
gives several example using `melt` (easier to rename the  “variable” and “value” columns) 
or `stack` (designed to work together with MultiIndex objects).

Reshape using `melt`

    cheese = pandas.DataFrame(
          {
              "first": ["John", "Mary"],
              "last": ["Doe", "Bo"],
              "height": [5.5, 6.0],
              "weight": [130, 150],
          }
    )
    cheese
    cheese.melt(id_vars=["first", "last"], var_name="quantity")

Reshape using the `wide_to_long` convenience function

    import numpy as np
    dft = pandas.DataFrame(
        {
            "A1970": {0: "a", 1: "b", 2: "c"},
            "A1980": {0: "d", 1: "e", 2: "f"},
            "B1970": {0: 2.5, 1: 1.2, 2: 0.7},
            "B1980": {0: 3.2, 1: 1.3, 2: 0.1},
            "X": dict(zip(range(3), np.random.randn(3))),
        }
    )
    dft["id"] = dft.index
    dft
    pandas.wide_to_long(dft, stubnames=["A", "B"], i="id", j="year")



## Replacement

[Pyton pandas equivalent for replace](https://stackoverflow.com/questions/12152716/python-pandas-equivalent-for-replace)

    import pandas
    s = pandas.Series(["ape", "monkey", "seagull"])
    s.replace(["ape", "monkey"], ["lion", "panda"])
    s.replace("a", "x", regex=True)
    `s.replace({"ape": "lion", "monkey": "panda"})`
    pandas.Series(["bla", "bla"]).replace("a","i",regex=True)


## Selection with loc, iloc, query and isin

 * blog [Select pandas data frame rows and columns using iloc and loc](https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/)


### loc

`.loc` is primarily label based, but may also be used with a boolean array.

I copied the examples below from the pandas **loc** documentation at:
[pandas.DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html)

Create an example data frame

    import pandas
    df = pandas.DataFrame([[1, 2], [4, 5], [7, 8]],
                          index=['cobra', 'viper', 'sidewinder'],
                          columns=['max_speed', 'shield'])

List of index labels

    In :  df.loc[['viper', 'sidewinder']]
    Out:
                        max_speed  shield
            viper               4       5
            sidewinder          7       8

Conditional that returns a boolean Series

    In :  df.loc[df['shield'] > 6]
    Out:
                         max_speed  shield
             sidewinder          7       8

Select rows that are not in ['cobra','viper'] 
Based on a [SO answer use isin on the index](https://stackoverflow.com/a/29140194/2641825):

    In : df.index.isin(['cobra','viper'])
    Out: array([ True,  True, False])

    In : df.loc[~df.index.isin(['cobra','viper'])]
    Out: 
                max_speed  shield
    sidewinder          7       8

Slice with labels for row and labels for columns.

    In :  df.loc['cobra':'viper', 'max_speed':'shield']
    Out:
                   max_speed  shield
            cobra          1       2
            viper          4       5

Set value for all items matching the list of labels

    In : df.loc[['viper', 'sidewinder'], ['shield']] = 50

    In : df
    Out:
                        max_speed  shield
            cobra               1       2
            viper               4      50
            sidewinder          7      50

Another example using integers for the index

    In : df = pandas.DataFrame([[1, 2], [4, 5], [7, 8]],
                               index=[7, 8, 9],
                               columns=['max_speed', 'shield'])

Slice with integer labels for rows. Note that both the start and stop of the slice are included.

    In :  df.loc[8:9]
    Out:
          max_speed  shield
       8          4       5
       9          7       8

### iloc 

[.iloc](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-integer)
is primarily integer position based (from 0 to length -1 of the axis), but may also be used with a boolean array.

Create a sample data frame:

    In : example = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},
                    {'a': 100, 'b': 200, 'c': 300, 'd': 400},
                    {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]
          df = pandas.DataFrame(example)

    In : df
    Out: 
                a     b     c     d
          0     1     2     3     4
          1   100   200   300   400
          2  1000  2000  3000  4000

Index with a slice object. Note that it doesn't include the upper bound.

    In :  df.iloc[0:2]
    Out: 
              a    b    c    d
         0    1    2    3    4
         1  100  200  300  400

With lists of integers.

    In : df.iloc[[0, 2], [1, 3]]
    Out: 
                b     d
          0     2     4
          2  2000  4000

With slice objects.

    In : df.iloc[1:3, 0:3]
    Out: 
                a     b     c
          1   100   200   300
          2  1000  2000  3000

With a boolean array whose length matches the columns.

    In : df.iloc[:, [True, False, True, False]]
    Out: 
                a     c
          0     1     3
          1   100   300
          2  1000  3000
### Query

Query the columns of a Data Frame with a boolean expression.

    df = pandas.DataFrame({'A': range(1, 6),
                           'B': range(10, 0, -2),
                           'C': range(10, 5, -1)})
    df.query("A > B")

    A  B  C
    5  2  6

Query using a variable

    limit = 3
    df.query("A > @limit")

    A  B  C
    4  4  7
    5  2  6


### isin

[Use alist of values to select rows](https://stackoverflow.com/questions/12096252/use-a-list-of-values-to-select-rows-from-a-pandas-dataframe)

    df = pandas.DataFrame({'A': [5,6,3,4], 'B': [1,2,3,5]})
    df[df['A'].isin([3, 6])]
    df.loc[df['A'].isin([3, 6])]
    df.query("A in [3,6]")


### Determine whether a column contains a particular value

[How to determine whether a pandas column contains a particular
varlue](https://stackoverflow.com/questions/21319929/how-to-determine-whether-a-pandas-column-contains-a-particular-value) 

> In of a Series checks whether the value is in the index:

    In : s = pd.Series(list('abc'))
    In : 1 in s
    Out: True
    In : 'a' in s
    Out: False

> One option is to see if it's in unique values:

    In : 'a' in s.unique()
    Out: True


## Sort or arrange values

Sort iris by descending order of species and ascending order of petal width

    iris.sort_values(by=["species", "petal_width"], ascending=[False,True])


## String operations in pandas

See also string operations in python in another section.

Concatenate all values in a character vector:

    df['A'].str.cat()

Replace elements in a character vector:

    df['A'].replace('a','b',regex=True)

## Difference between 2 data frames

* [Find difference between two data frames](https://stackoverflow.com/questions/48647534/python-pandas-find-difference-between-two-data-frames)
* [Diff of 2 data frames](https://stackoverflow.com/questions/36891977/pandas-diff-of-two-dataframes/36893773)

Two methods
Using `merge`:

    merged = df1.merge(df2, indicator=True, how='outer')
    merged[merged['_merge'] == 'right_only']

Using `drop_duplicates`

    newdf=pd.concat[df1,df2].drop_duplicates(keep=False)


## Where and mask

`where` replaces values that do not fit the condition and `mak` replaces
values that fit the condition.

    s = pandas.Series(range(5))
    s.where(s > 1, 10)
    s.mask(s > 1, 10)


# Paths

## Pathlib

Pathlib is an object oriented path API for python as explained in [PEP
428](https://www.python.org/dev/peps/pep-0428/#why-an-object-oriented-api)

Instead of 

    import os
    os.path.join('~','downloads')

You can use:

    from pathlib import Path
    Path('~') / 'downloads'

Data located in the home folder

     data_dir = Path.home() / "repos/data/"


# Plot

[Python plotting for exploratory analysis](https://pythonplot.com/) is a great
gallery of plot examples, each example is written in 5 different plotting
libraries: pandas, plotnine, plotly, altair and R ggplot2. There is also one
seaborn example.


## Matplotlib

All matplotlib examples require the following imports:

    from matplotlib import pyplot as plt
    plt.style.use('seaborn-whitegrid')
    import numpy as np

Simple line plot changing the figure size and the axes limit with pyplot

    plt.rcParams['figure.figsize'] = [10, 10]
    fig = plt.figure()
    ax = plt.axes()
    x = np.linspace(-1.5, 1.5, 1000)
    ax.plot(x, 1-3*x)
    ax.set_xlim(-6, 6)
    ax.set_ylim(-6, 6)

Scatter plot, using a colour variable and the 'jet' colour map.

    Y = np.array([1,-1,-1, 1])
    X = np.array([
            [-1, -1],
            [ 1, -1],
            [-1,  1],
            [ 1,  1]])
    fig = plt.figure()
    ax = plt.axes()
    ax.scatter(X[:,0], X[:,1],c=Y, cmap='jet')

Use another [colour map](https://matplotlib.org/examples/color/colormaps_reference.html)

    ax.scatter(X[:,0], X[:,1],c=Y, cmap='Spectral')

Plot the probability density function of the 
[normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). 

$$f(x)=\frac{1}{\sigma{\sqrt {2\pi }}}e^{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}}$$

With various sigma and mu values displayed in the legend.

    fig = plt.figure()
    ax = plt.axes()
    x = np.linspace(-5, 5, 1000)
    def pdensitynormal(x,sigma_squared,mu):
        sigma = np.sqrt(sigma_squared)
        return 1/(sigma*np.sqrt(2*np.math.pi))*np.exp(-1/2*((x-mu)/sigma)**2)
    ax.plot(x, pdensitynormal(x,0.2,0), label="$\sigma^2=0.2, \mu=0$")
    ax.plot(x, pdensitynormal(x,1,0), label="$\sigma^2=1, \mu=0$")
    ax.plot(x, pdensitynormal(x,5,0), label="$\sigma^2=5, \mu=0$")
    ax.plot(x, pdensitynormal(x,0.5,-2), label="$\sigma^2=0.5, \mu=-2$")
    ax.legend(loc="upper right")
    plt.show()

3D line, contour plot and scatter plot

    from mpl_toolkits import mplot3d # Required for 3d plots
    fig = plt.figure()
    ax = plt.axes(projection='3d')
    # Data for a three-dimensional line
    xline = np.linspace(-10, 10, 1000)
    yline = np.linspace(-10, 10, 1000)
    # Just a line
    zline = xline**2 + yline**2
    ax.plot3D(xline, yline, zline, 'gray')
    # A mesh grid
    X, Y = np.meshgrid(xline, yline)
    Z = X**2 + Y**2
    ax.contour3D(X, Y, Z, 50, cmap='binary')
    # Scatter points
    ax.scatter(1,2,3)

See how the `np.meshgridi` objects interact with each other.
Note this nested loop is not the optimal way to compute.
Better to use X**2 + Y**2 directly as above.

    for i in range(Z.shape[0]):
        for j in range(Z.shape[1]):
            vector = np.array([X[i,j],Y[i,j]])
                Z[i,j] = np.linalg.norm(vector)**2
    fig = plt.figure()
    ax = plt.axes(projection='3d')
    ax.contour3D(X, Y, Z, 50, cmap='binary')

## Plotly 

The advantage of plotly is that it provides dynamic visualisation inside web
pages, such as the possibility to zoom in a graph. It's the open source
component of a commercial project called Dash entreprise.

For example [this
notebook](https://colab.research.google.com/github/bytehub-ai/blog-examples/blob/master/temperature_forecast_example.ipynb#scrollTo=7yJZA1CZSSx4)
on machine learning used to enhance the localisation of weather forecasts. Seen
on this blog post [What does machine learning have to do with
weather](https://medium.com/bytehub-ai/what-does-machine-learning-have-to-do-with-weather-94f3ac625ad3).

## Seaborn

All seaborn examples below require the following imports and datasets:

    import seaborn
    iris = seaborn.load_dataset("iris")
    tips = seaborn.load_dataset("tips") 
    from matplotlib import pyplot as plt

### Axes limit

Set limits on the one axis in a Seaborn plot:

    p = seaborn.scatterplot('petal_length','petal_width','species',data=iris)
    p.set(ylim=(-2,None))


In Seaborn facet grid.
[How to set xlim and ylim in seaborn facet grid](https://stackoverflow.com/a/25213614/2641825)

    g = seaborn.FacetGrid(tips, col="time", row="smoker")
    g = g.map(plt.hist, "total_bill")
    g.set(ylim=(0, None)) 

### Grid plot


#### Grid scatter plot

Draw a scatter plot for each iris species

    g = seaborn.FacetGrid(iris, col="species", height=6, width=2)
    iris['species'] = iris['species'].astype('category')
    g.map(seaborn.scatterplot,'petal_length','petal_width','species')
    plt.show()

Notice that if you don't change the color to a categorical variable, it will not vary across the species. 
I reported [this issue](https://github.com/mwaskom/seaborn/issues/2028).


#### Grid Bar plot 

Draw a facet bar plot [from SO](https://stackoverflow.com/a/62225095/2641825)
for each combination of size and smoker/non smoker 

    import seaborn as sns
    import matplotlib.pyplot as plt
    sns.set()
    tips=sns.load_dataset("tips")
    g = sns.FacetGrid(tips, col = 'size',  row = 'smoker', hue = 'day')
    g = (g.map(sns.barplot, 'time', 'total_bill', ci = None).add_legend())
    plt.show()


### Title

Use `set_title` to add a title:

    (seaborn
     .scatterplot(x="total_bill", y="tip", data=tips)
     .set_title('Progression of tips along the bill amount')
    )

Set a common title for grid plots

    g = seaborn.FacetGrid(tips, col="time", row="smoker")
    g = g.map(plt.hist, "total_bill")
    g.fig.suptitle('I don't smoke and I don't tip.')

In case the title is overwritten on the subplots, you might need to use 
[fig.subplot_adjust()](https://stackoverflow.com/a/28650623/2641825) 
as such:

    g.fig.subplots_adjust(top=.95)

### Figure size

`set_figwidth` and `set_figheight` work well for grid objects. 

    g = seaborn.FacetGrid(tips, col="time", row="smoker")
    g = g.map(plt.hist, "total_bill")
    g.fig.set_figwidth(10)
    g.fig.set_figheight(10) 

Mentioned as a comment under [this answer](https://stackoverflow.com/a/56970556/2641825)

#### Larger grid plots

To change the height and aspect ration of individual grid cells, you can use
the `height` and `aspect` arguments of the FacetGrid call as such:

    import seaborn 
    from matplotlib import pyplot
    seaborn.set()
    iris = seaborn.load_dataset("iris")
    # Change height and aspect ratio
    g = seaborn.FacetGrid(iris, col="species", height=8, aspect=0.3)
    iris['species'] = iris['species'].astype('category')
    g.map(seaborn.scatterplot,'petal_length','petal_width','species')
    pyplot.show()

`help(seaborn.FacetGrid)`

> ``aspect * height`` gives the width of each facet in inches.


### Scatter Plot

Create a scatter plot

    import seaborn
    import matplotlib.pyplot as plt
    tips = seaborn.load_dataset("tips")
    seaborn.scatterplot(x="total_bill", y="tip", data=tips)
    plt.show()

Group by another variable and show the groups with different colors:
    
    searborn(x="total_bill", y="tip", hue="time", data=tips)


# R and python

See also the [R page](R.html) for more details on R.

Reddit [python vs
R](https://old.reddit.com/r/datascience/comments/67p72w/python_vs_r/)

> "R is for analysis. Python is for production. If you want to do analysis
> only, use R. If you want to do production only, use Python. If you want to do
> analysis then production, use Python for both. If you aren't planning to do
> production then it's not worth doing, (unless you're an academic).
> Conclusion: Use python."


## History

The central objects in R are vectors, matrices and data frames, that is why I
mostly compare R to the python packages numpy and pandas. R was created almost
20 years before numpy and more than 40 years before pandas.

[R_(programming_language)](https://en.wikipedia.org/wiki/R_(programming_language))

> "R is an implementation of the S programming language combined with lexical
> scoping semantics, inspired by Scheme. S was created by John Chambers in
> 1976 while at Bell Labs. A commercial version of S was offered as S-PLUS
> starting in 1988. " 

[NumPy history](https://en.wikipedia.org/wiki/NumPy#History)

> "In 1995 the special interest group (SIG) matrix-sig was founded with the aim
> of defining an array computing package; among its members was Python designer
> and maintainer Guido van Rossum, who extended Python's syntax (in particular
> the indexing syntax) to make array computing easier. [...] An implementation
> of a matrix package was completed by Jim Fulton, then generalized by Jim
> Hugunin and called Numeric. [...] new package called Numarray was written as
> a more flexible replacement for Numeric. Like Numeric, it too is now
> deprecated. [...] In early 2005, NumPy developer Travis Oliphant wanted to
> unify the community around a single array package and ported Numarray's
> features to Numeric, releasing the result as NumPy 1.0 in 2006."

[Pandas_(software)](https://en.wikipedia.org/wiki/Pandas_(software)#History)

> "Developer Wes McKinney started working on pandas in 2008 while at AQR
> Capital Management out of the need for a high performance, flexible tool to
> perform quantitative analysis on financial data. Before leaving AQR he was
> able to convince management to allow him to open source the library." 

[Migrating from R to python](https://jrvcomputing.wordpress.com/2016/11/14/migrating-from-r-to-python/)

> "Python is a full fledge programming language but it is missing statistical
> and plotting libraries. Vectors are an after thought in python most
> functionality can be reproduced using operator overloading, but some
> functionality looks clumsy."


## Numpy and R

R session showing a division by zero returning an infinite value.

    > 1/0
    [1] Inf

Python session showing a division by zero error for normal integer division and
the same operation on a numpy array returning an infinite value with a warning.


    In [1]: 1/0
    ---------------------------------------------------------------------------
    ZeroDivisionError                         Traceback (most recent call last)
    <ipython-input-1-9e1622b385b6> in <module>
    ----> 1 1/0

    ZeroDivisionError: division by zero

    In [2]: import numpy as np

    In [3]: np.array([1]) / 0
    /home/paul/.local/bin/ipython:1: RuntimeWarning: divide by zero encountered in true_divide
      #!/usr/bin/python3
    Out[3]: array([inf])


## Pandas comparison with R

R data frame to be used for examples:

    df = data.frame(x = 1:3, y = c('a','b','c'), stringsAsFactors = FALSE)

Pandas data frame to be used for examples:

    `df = pandas.DataFrame({'x' : [1,2,3], 'y' : ['a','b','c']})  `

| Base R                      | pandas                        | SO questions                                       |
| --------------------------- | ----------------------------- | -------------------------------------------------- |
| `df[df$y %in% c('a','b'),]` | `df[df['y'].isin(['a','b'])]` | [list of values to select a row]                   |
| `dput(df)`                  | `df.to_dict()`                | [Print pandas data frame for reproducible example] |
| `expand.grid(df$x,df$y)`    | `itertools.product`           | see section below                                  |
| `gsub`                      | `replace(regex=True)`         | [gsub in pandas]                                   |
| `length(df)` and `dim(df)`  | `df.shape`                    | [row count of a data frame]                        |
| `rbind`                     | `pandas.concat`               | [Pandas version of rbind]                          |
| `seq(1:5)`                  | `np.array(range(0,5))`        | [numpy function to generate sequences]             |
| `summary`                   | `describe`                    |                                                    |
| `str`                       | `df.info()`                   | [pandas equivalents for R functions like str summary and head]|


[gsub in pandas]: https://stackoverflow.com/questions/21834293/equivalent-of-gsub-for-pandas-series-dataframe/56547104?noredirect=1#comment99677847_56547104
[list of values to select a row]: https://stackoverflow.com/questions/12096252/use-a-list-of-values-to-select-rows-from-a-pandas-dataframe
[row count of a data frame]: https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe
[Pandas version of rbind]: https://stackoverflow.com/questions/14988480/pandas-version-of-rbind
[Print pandas data frame for reproducible example]: https://stackoverflow.com/questions/47450931/print-pandas-data-frame-for-reproducible-example-equivalent-to-dput-in-r
[numpy function to generate sequences]: https://stackoverflow.com/a/60753578/2641825
[pandas equivalents for R functions like str summary and head]: https://stackoverflow.com/questions/27637281/what-are-python-pandas-equivalents-for-r-functions-like-str-summary-and-he


The mapping of tidyverse to pandas is:

| tidyverse                 | pandas                         | SO questions                         |
| ------------------------- | -----------------------------  | -----------------------------        |
| arrange                   | sort_values                    |                                      |
| df %>% select(-a,-b)      | df.drop(columns=['a', 'b'])    |                                      |
| filter                    | query                          |                                      |
| group_by                  | groupby                        |                                      |
| lag                       | [shift]                        | [pandas lag function][so_lag_pandas] |
| mutate                    | [assign]                       |                                      |
| pivot_longer              | [melt] or [wide_to_long]       |                                      |
| pivot_wider               | [pivot]                        |                                      |
| rename                    | df.rename(columns={'a':'new'}) |                                      |
| select                    | filter                         |                                      |
| summarize                 | agg                            |                                      |
| unnest                    | [explode]                      | [unnest in pandas][so_unnest]        |

[assign]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html
[explode]: https://pandas.pydata.org/pandas-docs/version/0.25/reference/api/pandas.DataFrame.explode.html
[melt]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html
[pivot]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html
[shift]: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html
[so_lag_pandas]: https://stackoverflow.com/questions/23664877/pandas-equivalent-of-oracle-lead-lag-function
[so_unnest]: https://stackoverflow.com/a/53218939/2641825
[wide_to_long]: https://pandas.pydata.org/docs/reference/api/pandas.wide_to_long.html


Methods to use inside the .groupby().agg() method:

* `sum`
* `count`
* `mean`
* `', '.join` [to get a union of strings](https://stackoverflow.com/questions/17841149/pandas-groupby-how-to-get-a-union-of-strings)

### Expand grid in pandas

This [SO answer](https://stackoverflow.com/a/60371544/2641825) provides an
implementation of expand grid using itertools:

    import itertools
    import pandas
    countries = self.by_country_year['country'].drop_duplicates()
    years = range(1990, datetime.now().year)
    expand_grid = list(itertools.product(countries, years))
    df = pandas.DataFrame(expand_grid, columns=('country', 'year'))

### Blogs and quotes on Pandas and R

 * pandas.pydata.org [Comparison with R](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html)
 * [Tydiverse style pandas](https://stmorse.github.io/journal/tidyverse-style-pandas.html) 

> "Tidyverse allows a mix of quoted and unquoted references to variable names.
> In my (in)experience, the convenience this brings is accompanied by equal
> consternation. It seems to me a lot of the problems solved by tidyeval would
> not exist if all variables were quoted all the time, as in pandas, but there
> are likely deeper truths I’m missing here…" 

Help of the R function unite from the tidyr package:

> "col: The name of the new column, as a string or symbol. This argument is
> passed by expression and supports quasiquotation (you can unquote strings and
> symbols). The name is captured from the expression with ‘rlang::ensym()’
> (note that **this kind of interface where symbols do not represent actual
> objects is now discouraged** in the tidyverse; we support it here for backward
> compatibility)."

The use of symbols which do not represent actual objects was was frustrating at first when using pandas,
because we hat to use df["x"] to assign vectors to new column names whereas we
could use df.x to display them.


## Personal reflection

R is great for statistical analysis and plotting. You can also use it to
elaborate a pipeline to load data, prepare it and analyse it. But when things
start to get complicated, such as loading json data from APIs, or dealing with
http requests issues, or understanding lazy evaluation, or the consequences of
non standard evaluation, moving down the rabbit whole can get really
complicated with R. The rabbit hole slide is smoother with python. I have the
feeling that I keep a certain level of understanding at all steps. It's just a
matter of taste anyway.

I see R more like the bash command line. It's great for scripts, but you
wouldn't want to write large applications in bash.

Non standard evaluation doesn't exist in python. 
- An email thread
discussing [the idea of non standard evaluation in
python](https://www.mail-archive.com/python-ideas@python.org/msg15694.html).
- A [comparison of a python implementation and an R implementation using non
  standard evaluation](https://third-bit.com/2018/11/16/non-standard-evaluation/).

# String 

See also string operations in pandas vectors.

[SO answer ](https://stackoverflow.com/questions/25675943/how-can-i-concatenate-str-and-int-objects) 
providing various ways to concatenate python strings.


## String formatting

[How to print number with commas as thousands separators?](https://stackoverflow.com/a/10742904/2641825)

    f'{1e6:,}' 


## Regex substitution

[Documentation of the re package](https://docs.python.org/3/library/re.html).

Insert a suffix in a file name before the extension
[SO anwser](https://stackoverflow.com/a/2763772/2641825)

    import re
    re.sub(r'(?:_a)?(\.[^\.]*)$' , r'_suff\1',"long.file.name.jpg")



# Statistics

## Linear programming solvers

Real Python [What is linear
programing](https://realpython.com/linear-programming-python/#what-is-linear-programming) 

> Several free Python libraries are specialized to interact with linear or
> mixed-integer linear programming solvers:

[SciPy Optimization and Root
Finding](https://docs.scipy.org/doc/scipy/reference/optimize.html)

[PuLP](https://www.coin-or.org/PuLP/solvers.html)

[Pyomo](https://pyomo.readthedocs.io/en/stable/solving_pyomo_models.html#supported-solvers)

[CVXOPT](https://cvxopt.org/userguide/coneprog.html#optional-solvers)


## Scaling

[Feature scaling with scikit learn](http://benalexkeen.com/feature-scaling-with-scikit-learn/)

 * StandardScaler
 * MinMaxScaler
 * RobustScaler
 * Normalizer


# Style guide and linter

## Black

[Black](https://github.com/psf/black)
is "the uncompromising Python code formater"

Run `black` as a pre commit hook with `pre-commit`


## Flake 8

Flake 8 looks at more than just formatting.

[List of FLake8 warnings and error
codes](https://pypi.org/project/flake8/1.6.1/#warning-error-codes)


## Pre commit hooks

Blog:

- [pre commits using black and
  flake8](https://ljvmiranda921.github.io/notebook/2018/06/21/precommits-using-black-and-flake8/)

### Install pre commit hooks

Install `pre-commit`

    pip install pre-commit

Set up `pre-commit` in a repository

    cd path_to_repository
    # Add the "pre-commit" python module to a requirements file
    vim requirements.txt 
    # Create a configuration file
    vim .pre-commit-config.yaml 

Configuration options such as

```
repos:
-   repo: https://github.com/ambv/black
    rev: 21.6b0
    hooks:
    - id: black
      language_version: python3.7
-   repo: https://gitlab.com/pycqa/flake8
    rev: 3.7.9
    hooks:
    - id: flake8
```

Update [hook repositories to the latest
version](https://pre-commit.com/#using-the-latest-version-for-a-repository)

    pre-commit autoupdate

Install git hooks in your .git/ directory.

    pre-commit install


### Usage in Continuous integration

[Usage in Continuous integration](https://pre-commit.com/#usage-in-continuous-integration) has a gitlab example:

```
    my_job:
      variables:
        PRE_COMMIT_HOME: ${CI_PROJECT_DIR}/.cache/pre-commit
      cache:
        paths:
          - ${PRE_COMMIT_HOME}
```


### Un install pre-commit hooks

Uninstall

    pre-commit uninstall


## Pylint

Edit the configuration file

    vim ~/.pylintrc

List of good names that shouldn't give a "short name" warning

    [pylint]
    good-names=df

Generate a configuration file

    pylint --generate-rcfile

Blog

- Reddit [Any advantages of Flake8 over
  PyLint](https://www.reddit.com/r/Python/comments/82hgzm/any_advantages_of_flake8_over_pylint/)
  some answers suggest keeping both.



###  Dangerous default argument

- [SO
answer][https://stackoverflow.com/questions/101268/hidden-features-of-python#113198)
- [pylint issues](https://github.com/PyCQA/pylint/issues/3642)

> I understand the dangerous of using a mutable default value and I suggest
> switching the warning message for something like "Dangerous mutable default
> value as argument". However, this is dangerous for all sorts of scenarios? (I
> know that pylint isn't supposed to check the functionality of my code, just
> trying to clarify this anti-pattern)

    >>> def find(_filter={'_id': 0}):
    ...     print({**find.__defaults__[0], **_filter})
    ...
    >>> find()
    {'_id': 0}
    >>> find({'a': 1})
    {'_id': 0, 'a': 1}
    >>> find()
    {'_id': 0}
    >>> find({'a': 1, 'b': 2})
    {'_id': 0, 'a': 1, 'b': 2}

> One might argue that the following should be used and I tend to agree:

    >>> def find(_filter=None):
    ...     if _filter is None:
    ...             _filter = {'_id': 0}
    ...     else:
    ...             _filter['_id'] = 0
    ...     print(_filter)
    ...
    >>> find()
    {'_id': 0}
    >>> find({'a': 1})
    {'a': 1, '_id': 0}
    >>> find()
    {'_id': 0}
    >>> find({'a': 1, 'b': 2})
    {'a': 1, 'b': 2, '_id': 0}



### Using with for resource allocation

Pylint message

> Consider using 'with' for resource-allocating operations

Explained in a [SO answer](https://stackoverflow.com/a/67419279/2641825)

> suppose you are opening a file:

    file_handle = open("some_file.txt", "r")
    ...
    ...
    file_handle.close()

> You need to close that file manually after required task is done. If it's not
> closed, then resource (memory/buffer in this case) is wasted. If you use `with`
> in the above example:

    with open("some_file.txt", "r") as file_handle:
        ...
        ...

> there is no need to close that file. Resource de-allocation automatically
> happens when you use with. 



# System information

Platform type

    import sys
    sys.platform

or

    import os
    os.name

Sys and os return different results 'linux' or 'posix'.

More details are given by

    os.uname()


## Memory

Sometimes when a python process runs out of memory, it can get killed by the
Linux Kernel. In that case the error message is short "killed" and there is no
python trace back printed. You can check that it is indeed a memory error by
calling

    sudo dmesg

Here is a typical message:

    [85962.510533] Out of memory: Kill process 16035 (ipython3) score 320 or sacrifice child
    [85962.510554] Killed process 16035 (ipython3) total-vm:7081812kB, anon-rss:4536336kB, file-rss:0kB, shmem-rss:8kB
    [85962.687468] oom_reaper: reaped process 16035 (ipython3), now anon-rss:0kB, file-rss:0kB, shmem-rss:8kB

Various related Stack Overflow questions [what does "kill"
mean](https://stackoverflow.com/questions/19189522/what-does-killed-mean-when-a-processing-of-a-huge-csv-with-python-which-sudde), 
[How can I find the reason that python script is
killed?](https://stackoverflow.com/questions/47408608/how-can-i-find-the-reason-that-python-script-is-killed),
[Why does python script randomly gets
killed?](https://stackoverflow.com/questions/1811173/why-does-my-python-script-randomly-get-killed).



# Test driven development

A good post about TDD 
[Unit testing your doing it wrong](https://medium.com/@Cyrdup/unit-testing-youre-doing-it-wrong-407a07692989)

> "TDD is actually about every form of tests. For example, I often write
> performance tests as part of my TDD routine; end-to-end tests as well.
> Furthermore, this is about behaviours, not implementation: you write a new
> test when you need to fulfil a requirement. You do not write a test when you
> need to code a new class or a new method. Subtle, but important nuance. For
> example, you should not write a new test just because you refactored the
> code. If you have to, it means you were not really doing TDD."
> [...]
> "Good tests must test a behavior in isolation to other tests. Calling them unit, system or integration has no relevance to this.
> Kent Beck says this much better than I would ever do.
> '''From this perspective, the integration/unit test frontier is a frontier of design, not of tools or frameworks or how long tests run or how many lines of code we wrote get executed while running the test.'''
> Kent Beck"


## Unit tests

See [python markdown unit tests](https://github.com/okken/markdown.py/blob/master/test_markdown_unittest.py)


## pytest
Numpy moved from nose to pytest in their, as explained in their
[testing guidelines](https://docs.scipy.org/doc/numpy/reference/testing.html):

> "Until the 1.15 release, NumPy used the nose testing framework, it now uses the pytest framework. The older framework is still maintained in order to support downstream projects that use the old numpy framework, but all tests for NumPy should use pytest." 

Save this function in a file names test_numpy.py

    def test_numpy_closeness():
        assert [1,2] == [1,2]
        assert (np.array([1,2]) == np.array([1,2])).all()
        np.testing.assert_allclose(np.array([1,2]),np.array([1,3]))

Save the file to test_nn.py

    import neural_nets as nn
    import numpy as np

    def test_rectified_linear_unit():
        x = np.array([[1,0],
                      [-1,-3]])
        expected = np.array([[1,0],
                             [0,0]])
        provided = nn.rectified_linear_unit(x)
        assert np.allclose(expected, provided), "test failed"

Execute the test suite from bash with py.test as follows:

    cd ~/rp/course_machine_learning/projects/project_2_3_mnist/part2-nn
    py.test

Test pandas data frame with

    assert_frame_equal
    assert_series_equal # tricky to use


# Web Frameworks

[Flask vs. Django](asynchrosnus://www.codementor.io/garethdwyer/flask-vs-django-why-flask-might-be-better-4xs7mdf8v)

Note: [Flask Evolution into Quart](http://pgjones.gitlab.io/quart/flask_evolution.html#flask-evolution) 
to support 
[asyncio](http://pgjones.gitlab.io/quart/asyncio.html#asyncio) 
This last link contains a nice, simple example of 
how asyncio works with a simulated delay to fetch a web page.


# Media

Podcast [talk python to me](https://talkpython.fm/)
 * [Data science year in review](https://talkpython.fm/episodes/show/193/data-science-year-in-review-2018-edition)


## Blogs

* [Things I Learnt The Hard Way (in 30 Years of Software
  Development)](https://blog.juliobiason.me/thoughts/things-i-learnt-the-hard-way/) 

* [I do not use a debugger](https://lemire.me/blog/2016/06/21/i-do-not-use-a-debugger/)
    >"“Debuggers don’t remove bugs. They only show them in slow motion.”
    [Linus Toarvald doesn't use a debugger](https://lwn.net/2000/0914/a/lt-debugger.php3)

- Dotan Nahum [Functional Programming with Python for People Without Time](https://jondot.medium.com/functional-programming-with-python-for-people-without-time-1eebdbd9526c)

> "Cracks in the Ice - We ended the previous part with stating that with a good
> measure of abstraction, functional programming doesn’t offer a considerable
> advantage over the “traditional” way of design, object oriented. It’s a lie.
> [...] In our pipeline example above with our Executors — how do you feed in
> the output of one executor as the input for the next one? well, you have to
> build that infrastructure. With functional programming, those abstractions
> are not abstractions that you have to custom build. They’re part of the
> language, mindset, and ecosystem. Generically speaking — it’s all about
> impedence mismatch and leaky abstractions and when it comes to data and
> functions; there’s no mismatch because it’s built up from the core. The
> thesis is — that to build a functional programming approach over an
> object-oriented playground — is going to crash and burn at one point or
> another: be it bad modeling of abstractions, performance problems, bad
> developer ergonomics, and the worst — wrong mindset. Being able to model
> problems and solutions in a functional way, transcends above traditional
> abstraction; the object-oriented approach, in comparison, is crude,
> inefficient and prone to maintenance problems."


